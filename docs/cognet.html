<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>cognet.cognet API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#666666;background-color:black;//mix-blend-mode:difference;color:#bbbbbb;z-index:3}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#66bb66;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#66FF66}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:transparent;padding:1px 4px;color:#FFA500;overflow-wrap:break-word}h1 code{background:transparent}pre{overflow-wrap:break-word;background:#111111;word-wrap:break-word;border:0;border-top:1px solid #666;border-bottom:1px solid #666;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#333333;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#aaeeaa;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;color:#bbbbbb;background-color:black;overflow-wrap:break-word}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>cognet.cognet</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from quasinet.qnet import Qnet, qdistance, load_qnet, qdistance_matrix
from quasinet.qsampling import qsample, targeted_qsample
#from mpi4py.futures import MPIPoolExecutor
import sys
import subprocess
from scipy.stats import entropy
import multiprocessing as mp
import time
from cognet.util import embed_to_pca
import pkgutil
import os

import numpy as np
import pandas as pd
import random

class cognet:
    &#34;&#34;&#34;Aggregate related Qnet functions
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Init
        &#34;&#34;&#34;
        self.year = None
        self.n_jobs = 28
        self.qnet = None
        self.steps = 120
        self.num_qsamples = None
        self.all_samples = None
        self.samples = None
        self.samples_as_strings = None
        self.features = None
        self.cols = None
        self.immutable_vars = None
        self.mutable_vars = None
        self.poles = None
        self.polar_features = None
        self.polar_indices = None
        self.poles_dict = {}
        self.d0 = None
        self.s_null = None
        self.D_null = None
        self.mask_prob = 0.5
        self.variation_weight = None
        self.polar_matrix = None
        self.nsamples = None
        self.restricted = False
        self.MAX_PROCESSES = 0
    
    def load_from_model(self,
                        model,
                        data_obj,
                        key,
                        im_vars=None,
                        m_vars=None):
        &#34;&#34;&#34;load parameters from model object

        Args:
          model (Class): model obj for loading parameters
          data_obj (class): instance of dataformatter class
          key (str): &#39;all&#39;, &#39;train&#39;, or &#39;test&#39;, corresponding to sample type
          im_vars (list[str], optional): Not implemented yet. Defaults to None.
          m_vars (list[str], optional): Not implemented yet. Defaults to None.
        &#34;&#34;&#34;
        if model is not None:
            # inherit atrributes from model object
            self.qnet = model.myQnet
            featurenames, samples = data_obj.format_samples(key)
            samples = pd.DataFrame(samples)
            self.cols = np.array(featurenames)
            self.features = pd.DataFrame(columns=np.array(featurenames))
            
            # inherit mutable and immutable variables from model obj
            if any(x is not None for x in [model.immutable_vars, model.mutable_vars]):
                if model.immutable_vars is not None:
                    self.immutable_vars = model.immutable_vars
                    self.mutable_vars = [x for x in self.features if x not in self.immutable_vars]
                elif model.mutable_vars is not None:
                    self.mutable_vars = model.mutable_vars
                    self.immutable_vars = [x for x in self.features if x not in self.mutable_vars]
            else:
                self.mutable_vars = self.features
            
            # inherit and set class attributes.
            self.samples = pd.DataFrame(samples).replace(&#34;nan&#34;,&#34;&#34;).fillna(&#34;&#34;)
            self.samples.columns = np.array(featurenames)
            self.all_samples = self.samples
            self.samples_as_strings = self.samples.fillna(&#39;&#39;).values.astype(str)[:]
            self.s_null=[&#39;&#39;]*len(self.samples_as_strings[0])
            self.D_null=self.qnet.predict_distributions(self.s_null)
            variation_weight = []
            for d in self.D_null:
                v=[]
                for val in d.values():
                    v=np.append(v,val)
                variation_weight.append(entropy(v,base=len(v)))
            variation_weight = np.nan_to_num(variation_weight) # remove nans
            self.variation_weight = variation_weight
    
    def load_from_dataformatter(self, 
                                data_obj,
                                key):
        &#34;&#34;&#34;read in either train or test data, specified by key, from data obj,
        and inherit other attributes.

        Args:
          data_obj (class): instance of dataformatter class
          key (str): &#39;all&#39;, &#39;train&#39;, or &#39;test&#39;, corresponding to sample type
          
        Returns:
          featurenames, samples: formatted arrays
        &#34;&#34;&#34;
        # inherit attributes from dataformatter object
        featurenames, samples = data_obj.format_samples(key)
        if any(x is not None for x in [self.features, self.samples]):
            print(&#34;replacing original features/samples with dataformatter data&#34;)
        self.cols = featurenames
        self.features = pd.DataFrame(columns=self.cols)
        self.samples = pd.DataFrame(samples,columns=self.features)
        self.all_samples = self.samples
        self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
        self.s_null=[&#39;&#39;]*len(self.samples_as_strings[0])
        return featurenames, samples

    def load_data(self,
                  year,
                  features_by_year,
                  samples,
                  Qnet):
        &#39;&#39;&#39;load cols, features, samples, and qnet.

        Args:
          year (str): to identify cols/features.
          features_by_year (str): file containing all features by year of the dataset.
          samples (str): file of samples for that year.
          Qnet (str): Qnet file location.
        &#39;&#39;&#39;
        # set attributes from given files and data
        self.qnet = load_qnet(qnet)
        self.year = year
        self.cols = np.array((pd.read_csv(features_by_year,
                            keep_default_na=True, 
                            index_col=0).set_index(
                                &#39;year&#39;)).loc[int(year)].apply(
                                    eval).values[0])
        self.features = pd.DataFrame(columns=self.cols)
        self.mutable_vars = [x for x in self.cols]
        #[self.cols].fillna(&#39;&#39;).values.astype(str)[:]

        # read in samples and initialize related attributes
        self.samples=pd.read_csv(samples)
        self.samples = pd.concat([self.samples,self.features], axis=0)
        self.all_samples = self.samples
        self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
        self.s_null=[&#39;&#39;]*len(self.samples_as_strings[0])
        self.D_null=self.qnet.predict_distributions(self.s_null)
        variation_weight = []
        for d in self.D_null:
            v=[]
            for val in d.values():
                v=np.append(v,val)
            variation_weight.append(entropy(v,base=len(v)))
        self.variation_weight = variation_weight

    def set_immutable_vars(self,
                        IMMUTABLE_FILE):
        &#39;&#39;&#39;set vars to immutable and mutable, 
        can prob combine this with the load_data func: only set the immutable vars if necessary

        Args:
          IMMUTABLE_FILE (str): file containing the immutable features/vars
        &#39;&#39;&#39;
        # set mutable and immutable variable attributes 
        if self.cols is None:
            raise ValueError(&#34;load_data first!&#34;)
        self.immutable_vars = pd.read_csv(IMMUTABLE_FILE,index_col=0).transpose()
        self.mutable_vars = None
        self.mutable_vars = [x for x in self.cols
                            if x.upper() not in self.immutable_vars.columns]
    
    def set_nsamples(self,
                    num_samples,
                    random=False):
        &#39;&#39;&#39;select a subset of the samples

        Args:
          num_samples (int): Set num of samples to subset, default to None, resets to all samples
          random (bool): take random sample if true, ordered sample if false
        &#39;&#39;&#39;
        # each time function is called, reset samples to use_all_samples
        # this allows us to call nsamples numerous times 
        self.samples = self.all_samples
        if self.samples is not None:
            # if a greater number of sample is selected than available, raise error
            if all(x is not None for x in [num_samples, self.samples]):
                if num_samples &gt; len(self.samples.index):
                    string = &#39;The number of selected samples ({}) &#39; + \
                        &#39;is greater than the number of samples ({})!&#39;
                    string = string.format(num_samples, len(self.samples.index))
                    raise ValueError(string)

                # if the same number of samples is selected as available, print warning
                if num_samples == len(self.samples.index):
                    string = &#39;The number of selected samples ({}) &#39; + \
                        &#39;is equal to the number of samples ({})!&#39;
                    string = string.format(num_samples, len(self.samples.index))
                    print(string)
                    
                # if random is true, return random sample, otherwise return an ordered slice
                if random:
                    self.samples = self.samples.sample(num_samples)
                else:
                    self.samples = self.samples.iloc[:num_samples]
                self.nsamples = num_samples
                self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
                
            elif self.samples is None:
                raise ValueError(&#34;load_data first!&#34;)

    def __variation_weight(self,
                        index):
        &#34;&#34;&#34;
        &#34;&#34;&#34;
        d_=self.D_null[index]
        v=[]
        for val in d_.values():
            v=np.append(v,val)
        return entropy(v,base=len(v))
    
    def getBaseFrequency(self, 
                        sample):
        &#39;&#39;&#39;get frequency of the variables
        helper func for qsampling

        Args:
          sample (list[str]): vector of sample, must have the same num of features as the qnet
        &#39;&#39;&#39;
        # if variable is not mutable, set its base frequency to zero 
        MUTABLE=pd.DataFrame(np.zeros(len(self.cols)),index=self.cols).transpose()
             
        for m in self.mutable_vars:
            MUTABLE[m]=1.0
        mutable_x=MUTABLE.values[0]
        base_frequency=mutable_x/mutable_x.sum()
        
        # otherwise, set base frequency weighted by variation weight
        for i in range(len(base_frequency)):
            if base_frequency[i]&gt;0.0:
                base_frequency[i]= self.variation_weight[i]*base_frequency[i]

        return base_frequency/base_frequency.sum()
    
    def qsampling(self,
                sample,
                steps,
                immutable=False):
        &#39;&#39;&#39;perturb the sample based on thet qnet distributions and number of steps

        Args:
          sample (1d array-like): sample vector, must have the same num of features as the qnet
          steps (int): number of steps to qsample
          immutable (bool): are there variables that are immutable?
        &#39;&#39;&#39;
        # immutable, check that mutable variables have been initialized
        if immutable == True:
            if all(x is not None for x in [self.mutable_vars, sample]):
                return qsample(sample,self.qnet,steps,self.getBaseFrequency(self.samples))
            elif self.mutable_vars is None:
                raise ValueError(&#34;set mutable and immutable variables first!&#34;)
        else:
            return qsample(sample,self.qnet,steps)

    def random_sample(self,
                      df=None,
                      n=1):
        &#39;&#39;&#39;compute a random sample from the underlying distributions of the dataset, by column.
        
        
        Args:
          df (pandas.DataFrame): Desired data to take random sample of. Defaults to None, in which case qnet samples are used.
          n (int): number of random samples to take. Defaults to 1.
          
        Returns:
          return_df (pd.DataFrame): Random sample drawn from underlying distribution of each column.
        &#39;&#39;&#39;
        # check if a new dataset was inputted
        if df is None:
            samples_ = self.samples
        else:
            samples_ = df

        # take random sample from each of the columns based on their distribution
        return_df = pd.DataFrame()
        for col in samples_.columns:
            return_df[col] = samples_[col].sample(n=n, replace=True).values
            
        return return_df
    
    def set_poles(self,
                  POLEFILE,
                  pole_1,
                  pole_2,
                  steps=0,
                  mutable=False,
                  VERBOSE=False,
                  restrict=True,
                  nsamples = None,
                  random=False):
        &#39;&#39;&#39;set the poles and samples such that the samples contain features in poles

        Args:
          steps (int): number of steps to qsample
          POLEFILE (str): file containing poles samples and features
          pole_1 (str): column name for first pole
          pole_2 (str): column name for second pole
          mutable (bool): Whether or not to set poles as the only mutable_vars
          VERBOSE (bool): boolean flag prints number of pole features not found in sample features if True
          restrict (bool): boolean flag restricts the sample features to polar features if True
          random (bool): boolean flag takes random sample of all_samples
        &#39;&#39;&#39;
        invalid_count = 0
        if all(x is not None for x in [self.samples, self.qnet]):
            # read and set poles
            poles = pd.read_csv(POLEFILE, index_col=0)
            self.poles=poles.transpose()
            self.polar_features = pd.concat([self.features, self.poles], axis=0).fillna(&#39;&#39;)
            poles_dict = {}
            for column in poles:
                p_ = self.polar_features.loc[column][self.cols].fillna(&#39;&#39;).values.astype(str)[:]
                # qsample poles to qnet
                poles_dict[column] = self.qsampling(p_,steps)
            self.poles_dict = poles_dict
            self.pL = self.poles_dict[pole_1]
            self.pR = self.poles_dict[pole_2]
            self.d0 = qdistance(self.pL, self.pR, self.qnet, self.qnet)
            
            # restrict sample columns to polar columns
            if restrict:
                cols = [x for x in self.poles.columns if x in self.samples.columns]
                self.samples=self.samples[cols]
                self.restricted = True
                self.samples = pd.concat([self.features,self.samples], axis=0).replace(&#34;nan&#34;,&#34;&#34;).fillna(&#39;&#39;)
                self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
                
            # if restrict==False, unrestrict it and set original
            else:
                self.restricted = False
                self.samples = self.all_samples
                if self.nsamples is not None:
                    self.set_nsamples(nsamples, random)
            
            # identify pole features that were excluded due to sample features restriction
            if VERBOSE:
                for x in self.poles.columns:
                    if x not in self.samples.columns:
                        invalid_count += 1
                        #self.samples[x]=&#39;&#39;
            
            if mutable:
                self.mutable_vars=[x for x in self.cols if x in self.poles.columns]
        elif self.samples is None:
            raise ValueError(&#34;load_data first!&#34;)

        if VERBOSE:
            print(&#34;{} pole features not found in sample features&#34;.format(invalid_count))

    def mp_compute(self, 
                   processes,
                   func, 
                   cols,
                   outfile, 
                   args=[]):
        &#34;&#34;&#34;
        Compute desired function through multiprocessing and save result to csv.

        Args:
          processes (int): number of processes to use.
          func (func): function to compute using multiprocessing
          cols (list): column names of resulting csv
          outfile (str)): filepath + filename for resulting csv
          args (list): list containing arguments for desired function. Defaults to empty list.
        &#34;&#34;&#34;

        # init mp.Manager and result dict
        manager = mp.Manager()
        return_dict = manager.dict()

        # set processes as given, unless class parameter is set
        max_processes = processes
        if self.MAX_PROCESSES != 0:
            max_processes = self.MAX_PROCESSES
            print(&#34;Number of Processes {} has been set using class parameter&#34;.format(self.MAX_PROCESSES))
        num_processes = 0
        process_list = []
        
        # init mp.Processes for each individual sample
        # run once collected processes hit max
        for i in range(len(self.samples)):
            params = tuple([i, return_dict] + args)
            num_processes += 1
            p = mp.Process(target=func,
                        args=params)
            process_list.append(p)
            if num_processes == max_processes:
                [x.start() for x in process_list]
                [x.join() for x in process_list]
                process_list = []
                num_processes = 0
                
        # compute remaining processes
        if num_processes != 0:
            [x.start() for x in process_list]
            [x.join() for x in process_list]
            process_list = []
            num_processes = 0
        
        # format and save resulting dict
        result = pd.DataFrame(return_dict.values(), columns=cols, index=return_dict.keys()).sort_index()
        result.to_csv(outfile, index=None)
        return result
    
    def distance(self,
                sample1,
                sample2,
                nsteps1=0,
                nsteps2=0):
        &#34;&#34;&#34;qsamples each sample set num of steps, then takes qdistance

        Args:
          sample1 (list[str]): sample vector 1, must have the same num of features as the qnet
          sample2 (list[str]): sample vector 2, must have the same num of features as the qnet
          nsteps1 (int, optional): number of steps to qsample for sample1
          nsteps2 (int, optional): number of steps to qsample for sample2

        Returns:
          qdistance: float, distance between two samples
        &#34;&#34;&#34;
        if self.qnet is None:
            raise ValueError(&#34;load qnet first!&#34;)
        #bp1 = self.getBaseFrequency(sample1)
        #bp2 = self.getBaseFrequency(sample2)
        # qsample samples
        sample1 = qsample(sample1, self.qnet, nsteps1)#, baseline_prob=bp1)
        sample2 = qsample(sample2, self.qnet, nsteps2)#, baseline_prob=bp2)
        return qdistance(sample1, sample2, self.qnet, self.qnet)
    
    def __distfunc(self, 
                   x, 
                   y):
        &#39;&#39;&#39;Compute distance between two samples

        Args:
          x (list[str]): first sample
          y (list[str]): second sample
          
        Returns:
         d: qdistance
        &#39;&#39;&#39;
        d=qdistance(x,y,self.qnet,self.qnet)
        return d
    
    def distfunc_line(self,
                    i,
                    return_dict=None):
        &#39;&#39;&#39;compute the distance for a single sample from all other samples

        Args:
          i (int): row
          return_dict (dict): dictionary containing multiprocessing results
        
        Return:
          line: float, numpy.ndarray
        &#39;&#39;&#39;
        if all(x is not None for x in [self.samples, self.features]):
            w = self.samples.index.size
            line = np.zeros(w)
            y = self.samples_as_strings[i]
            for j in range(w):
                # only compute half of the distance matrix
                if j &gt; i:
                    x = self.samples_as_strings[j]
                    line[j] = self.__distfunc(x, y)
        else:
            raise ValueError(&#34;load_data first!&#34;)
        if return_dict is not None:
            return_dict[i] = line
        return line
    
    def distfunc_multiples(self,
                           outfile,
                           processes=6):
        &#34;&#34;&#34;compute distance matrix for all samples in the dataset

        Args:
          outfile (str): desired output filename and path
          
        Returns:
          result: pandas.DataFrame containing distance matrix
        &#34;&#34;&#34;
        if all(x is not None for x in [self.samples, self.features]):
            cols = [i for i in range(len(self.samples))]
            result = self.mp_compute(processes,
                                        self.distfunc_line,
                                        cols,
                                        outfile)
            # format and save resulting dict, and tranpose symmetrical distance matrix
            result = result.to_numpy()
            result = pd.DataFrame(np.maximum(result, result.transpose()))
            result.to_csv(outfile, index=None, header=None)
        else:
            raise ValueError(&#34;load data first!&#34;)
        
        return result
    
    def polarDistance(self,
                    i,
                    return_dict=None):
        &#34;&#34;&#34;return the distances from a single sample to the poles

        Args:
          i (int): index of sample to take
          return_dict (dict): dictionary containing multiprocessing results

        Returns:
          distances: float, distance from sample to each pole
        &#34;&#34;&#34;
        p = self.samples_as_strings[i]
        distances = []
        # calculate from each pole to the sample, and append to array
        for index, row in self.polar_features[self.cols].iterrows():
            row = row.fillna(&#39;&#39;).values.astype(str)[:]
            distances.append(self.distance(p, np.array(row)))
        if return_dict is not None:
            return_dict[i] = distances
        return distances
            
    def polarDistance_multiple(self,
                               outfile,
                               processes=6):
        &#34;&#34;&#34;return the distance from all samples to the poles

        Args:
          outfile (str): desired output filename and path
          
        Returns:
          result: pandas.DataFrame containing polar distance results
        &#34;&#34;&#34;
        if all(x is not None for x in [self.samples, self.cols,
                                    self.polar_features]):
            # get the column names
            pole_names = []
            for index, row in self.polar_features[self.cols].iterrows():
                pole_names.append(index)
            result = self.mp_compute(processes,
                                        self.polarDistance,
                                        pole_names,
                                        outfile)
        else:
            raise ValueError(&#34;load data first!&#34;)
        return result
    
    def polar_separation(self,
                        nsteps=0):
        &#34;&#34;&#34;calculates the distance between poles as a qdistance matrix

        Args:
          nsteps (int, optional): [description]. Defaults to 0.
          
        Returns:
          self.polar_matrix: dictionary containing multiprocessing results
        &#34;&#34;&#34;
        # vectorize and qsample poles
        polar_arraydata = self.polar_features[self.cols].values.astype(str)[:]
        samples_ = []
        for vector in polar_arraydata:
            bp = self.getBaseFrequency(vector)
            sample = qsample(vector, self.qnet, nsteps, baseline_prob=bp)
            samples_.append(sample)
        samples_ = np.array(samples_)
        # calculate distance matrix for poles
        self.polar_matrix = qdistance_matrix(samples_, samples_, self.qnet, self.qnet)
        return self.polar_matrix
        
    def embed(self,
            infile,
            name_pref,
            out_dir,
            pca_model=False,
            EMBED_BINARY=None):
        &#39;&#39;&#39;
        embed data

        Args:
          infile (str): input file to be embedded
          name_pref (str): preferred name for output file
          out_dir (str): output dir for results
          pca_model (bool): whether or not to generate PCA model
          EMBED_BINARY (os.path.abspath): path to embed binary
        &#39;&#39;&#39;
        if all(x is not None for x in [self.year]):
            # init file names 
            yr = self.year
            PREF = name_pref
            FILE = infile
            DATAFILE = out_dir + &#39;data_&#39; +yr
            EFILE = out_dir + PREF + &#39;_E_&#39; +yr
            DFILE = out_dir + PREF + &#39;_D_&#39; +yr
            
            # set embed binary directory
            if EMBED_BINARY is None:
                EMBED = pkgutil.get_data(&#34;cognet.bin&#34;, &#34;__embed__.so&#34;) 
            else:
                EMBED = EMBED_BINARY
            
            # embed data files
            pd.read_csv(FILE, header=None).to_csv(DATAFILE,sep=&#39; &#39;,header=None,index=None)
            STR=EMBED+&#39; -f &#39;+DATAFILE+&#39; -E &#39;+EFILE+&#39; -D &#39;+DFILE
            subprocess.call(STR,shell=True)
            if pca_model:
                embed_to_pca(EFILE, EFILE+&#39;_PCA&#39;)
        elif self.year is None:
            raise ValueError(&#34;load_data first!&#34;)
    
    def __calc_d0(self,
                pole_1,
                pole_2):
        &#34;&#34;&#34;calculate distance between two poles

        Args:
          pole_1 (list[str]): a polar vector, must have same number of features as qnet
          pole_2 (list[str]): a polar vector, must have same number of features as qnet
        &#34;&#34;&#34;
        self.pL = self.poles_dict[pole_1]
        self.pR = self.poles_dict[pole_2]
        self.d0 = qdistance(self.pL, self.pR, self.qnet, self.qnet)
        
    def ideology(self,
                i,
                return_dict=None,
                pole_1=None,
                pole_2=None):
        &#34;&#34;&#34;return ideology index (left-leaning or right-leaning) for a singular sample

        Args:
          i (int): index of sample
          pole_1 (int): index of Pole One to calc as base distance. Defaults to 0.
          pole_2 (int): index of Pole Two to calc as base distance. Defaults to 1.
          return_dict (dict, optional): dict containing results
          
        Returns:
          [ideology_index, dR, dL, self.d0]: which way the sample leans,
                                             distance from the right pole,
                                             distance from the left pole,
                                             and distance between poles, respectively
        &#34;&#34;&#34;
        # calculate base distance between two poles
        if pole_1 is not None or pole_2 is not None:
            self.__calc_d0(pole_1, pole_2)
        
        # calculate distances between sample and the two poles
        p = self.samples_as_strings[i]
        dR = qdistance(self.pR, p, self.qnet, self.qnet)
        dL = qdistance(self.pL, p, self.qnet, self.qnet)
        
        ideology_index = (dR-dL)/self.d0
        if return_dict is not None:
            return_dict[i] = [ideology_index, dR, dL, self.d0]
        return [ideology_index, dR, dL, self.d0]

    def dispersion(self,
                   i,
                   return_dict=None):
        &#34;&#34;&#34;qsamples a sample n times and takes distance matrix 
        to determine max and std of distances between qsamples

        Args:
          i (int): index of sample
          return_dict (dict): dictionary containing multiprocessing results

        Returns:
          list[float]: std and max of the distances btwn qsamples
        &#34;&#34;&#34;
        # qsample sample num_qsample times
        p = self.samples_as_strings[i]
        Qset = [qsample(p, self.qnet, self.steps) for j in np.arange(self.num_qsamples)]
        Qset = np.array(Qset)

        # calculate qdistance matrix for qsampled samples
        matrix = (qdistance_matrix(Qset, Qset, self.qnet, self.qnet))
        Q = matrix.max()
        Qsd = matrix.std()
        
        if return_dict is not None:
            return_dict[i] = [Qsd, Q]
        return [Qsd, Q]
    
    def compute_DLI_samples(self,
                        type,
                        outfile,
                        num_qsamples=40,
                        steps=120,
                        n_jobs=28,
                        pole_1=0,
                        pole_2=1,
                        processes=6):
        &#34;&#34;&#34;compute and save ideology index or dispersion for all samples

        Args:
          num_qsamples (int): number of qsamples to compute
          outfile (str): output file for results
          type (str): whether to calc dispersion or ideology
          steps (int): number of steps to qsample
          n_jobs (int, optional): sets the number of jobs for parallelization. Defaults to 28.
          pole_1 (int, optional): index of Pole One to calc as base distance. Defaults to 0.
          pole_2 (int, optional): index of Pole Two to calc as base distance. Defaults to 1.

        Raises:
          ValueError: set poles if poles are not set
          ValueError: load data if samples or features are not present
            
        Returns:
          result: pandas.DataFrame containing multiprocessing results
        &#34;&#34;&#34;
        if all(x is not None for x in [self.samples, self.features,
                                    self.pL, self.pR]):
            # init vars
            self.num_qsamples = num_qsamples
            self.steps = steps
            if pole_1 != 0 or pole_2 != 1:
                self.__calc_d0(pole_1, pole_2)
            
            if type == &#39;ideology&#39;:
                func_ = self.ideology
                cols=[&#39;ideology&#39;, &#39;dR&#39;, &#39;dL&#39;, &#39;d0&#39;]
            elif type == &#39;dispersion&#39;:
                func_ = self.dispersion
                cols=[&#39;Qsd&#39;, &#39;Qmax&#39;]
            else:
                raise ValueError(&#34;Type must be either dispersion or ideology!&#34;)
            
            result = self.mp_compute(processes,
                                     func_,
                                     cols,
                                     outfile)
        elif self.pL is None or self.pR is None:
            raise ValueError(&#34;set_poles first!&#34;)
        else:
            raise ValueError(&#34;load_data first!&#34;)
        return result

    def compute_polar_indices(self,
                              num_samples=None,
                              polar_comp=False,
                              POLEFILE=None,
                              steps=5):
        &#39;&#39;&#39;set up polar indices for dissonance func

        Args:
          num_samples (int): subset of samples to take
          polar_comp (bool): whether or not to set poles
          POLEFILE (None): file containing pole samples and features
          steps (int): number of steps to qsample
        &#39;&#39;&#39;
        if all(x is not None for x in [self.samples, self.features, self.poles]):
            if num_samples is not None:
                self.set_nsamples(num_samples)

            if polar_comp:
                self.set_poles(self.qnet, steps, POLEFILE)
            
            # calculate polar indices
            polar_features = pd.concat([self.features, self.poles], axis=0)
            self.polar_indices=np.where(polar_features[self.cols].fillna(&#39;XXXX&#39;).values[0]!=&#39;XXXX&#39;)[0]
        
        elif self.poles is None:
            raise ValueError(&#34;set_poles first!&#34;)
        else:
            raise ValueError(&#34;load_data first!&#34;)

    def dissonance(self,
                    sample_index=0,
                    return_dict=None,
                    MISSING_VAL=0.0,
                    sample=None):
        &#39;&#39;&#39;compute dissonance for a single sample, helper function for all_dissonance
        
        Args:
          sample_index (int): index of the sample to compute dissonance. Defaults to 0.
          return_dict (dict): dictionary containing multiprocessing results
          MISSING_VAL (float): default dissonance value
          sample (1D array): sample to compute dissonance of, instead of using sample index. Defaults to None.
          
        Returns: 
          diss[self.polar_indices]: ndarray containing dissonance for sample
        &#39;&#39;&#39;
        if all(x is not None for x in [self.samples, self.features]):
            if sample is None:
                s = self.samples_as_strings[sample_index]
            else:
                s = sample
            if self.polar_indices is None:
                self.polar_indices = range(len(s))

            # init vars and calculate dissonance for sample
            Ds=self.qnet.predict_distributions(s)
            diss=np.ones(len(Ds))*MISSING_VAL
            for i in self.polar_indices:
                if s[i] != &#39;&#39;:
                    if s[i] in Ds[i].keys():
                        diss[i]=1-Ds[i][s[i]]/np.max(
                            list(Ds[i].values())) 
                    else:
                        diss[i]=1.0
            if return_dict is not None:
                return_dict[sample_index] = diss[self.polar_indices]
            return diss[self.polar_indices]
        else:
            raise ValueError(&#34;load_data first!&#34;)
    
    def dissonance_matrix(self,
                        outfile=&#39;/example_results/DISSONANCE_matrix.csv&#39;,
                        processes=6):
        &#39;&#39;&#39;get the dissonance for all samples

        Args:
          output_file (str): directory and/or file for output
          processes (int): max number of processes. Defaults to 6.

        Returns:
          result: pandas.DataFrame containing dissonances for each sample
        &#39;&#39;&#39;
        # set columns
        if self.polar_indices is not None:
            polar_features = pd.concat([self.features, self.poles], axis=0)
            cols = polar_features[self.cols].dropna(axis=1).columns
        else:
            cols = self.cols
        
        result = self.mp_compute(processes,
                                    self.dissonance,
                                    cols,
                                    outfile)
        return result
    
    def __choose_one(self,
                X):
        &#39;&#39;&#39;returns a random element of X

        Args:
          X (1D array-like): vector from which random element is to be chosen
        
        Returns:
          X: random element of sample
          None: if X has len 0
        &#39;&#39;&#39;
        X=list(X)
        if len(X)&gt;0:
            return X[np.random.randint(len(X))]
        return None

    def getMaskedSample(self,
                        s,
                        mask_prob=0.5,
                        allow_all_mutable=False):
        &#39;&#39;&#39;inputs a sample and randomly mask elements of the sample

        Args:
          s (list[str]): vector of sample, must have the same num of features as the qnet.
          mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5
          allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.
          
        Returns:
          s1,
          base_frequency,
          MASKrand,
          np.where(base_frequency)[0],
          np.mean(rnd_match_prob),
          np.mean(max_match_prob),
          random_sample
        &#39;&#39;&#39;
        if self.samples is not None:
            # init random mutable variable masking
            s0=s.copy()
            s0=np.array(s0)   
            # double check, because code seems to imply that masking happens in order,
            # i.e. limited to the first 100 features, if there are only 100 mutable features
            MUTABLE=pd.DataFrame(np.zeros(len(self.cols)),index=self.cols).transpose()
            WITHVAL=[x for x in self.cols[np.where(s0)[0]] if x in self.mutable_vars ]
            MASKrand=[x for x in WITHVAL if random.random() &lt; mask_prob ]
            for m in MASKrand:
                MUTABLE[m]=1.0
            
            mutable_x=MUTABLE.values[0]
            base_frequency=mutable_x/mutable_x.sum()

            # if np.isnan(base_frequency).any():
            #     return np.nan,np.nan,np.nan
            #     return self.getMaskedSample(s)

            # mask sample according to masking (base_frequency)
            s1=s.copy()
            for i in range(len(base_frequency)):
                if base_frequency[i]&gt;0.0001:
                    s1[i]=&#39;&#39;
                
            # create a random sample to test reconstruction effectiveness
            random_sample=np.copy(s)
            rnd_match_prob=[]        
            max_match_prob=[]        
            D=self.qnet.predict_distributions(s)
            for i in MASKrand:
                random_sample[np.where(
                    self.cols==i)[0][0]]=self.__choose_one(
                        self.D_null[np.where(self.cols==i)[0][0]].keys())
                rnd_match_prob=np.append(rnd_match_prob,1/len(
                    self.D_null[np.where(self.cols==i)[0][0]].keys()))
                max_match_prob=np.append(
                    max_match_prob,np.max(
                        list(D[np.where(
                            self.cols==i)[0][0]].values())))
            
            # calculate base_frequency if all variables are mutable
            if allow_all_mutable:
                WITHVAL=[x for x in self.cols[np.where(s0)[0]]]
                MASKrand=[x for x in WITHVAL if random.random() &lt; mask_prob ]
                for m in MASKrand:
                    MUTABLE[m]=1.0
                mutable_x=MUTABLE.values[0]
                base_frequency=mutable_x/mutable_x.sum()
                s1=s.copy()
                for i in range(len(base_frequency)):
                    if base_frequency[i]&gt;0.0001:
                        s1[i]=&#39;&#39;

            return s1,base_frequency,MASKrand,np.where(
                base_frequency)[0],np.mean(rnd_match_prob),np.mean(max_match_prob),random_sample
        else:
            raise ValueError(&#34;load_data first!&#34;)

    def randomMaskReconstruction(self,
                                index=None,
                                return_dict=None,
                                sample=None,
                                index_colname=&#34;feature_names&#34;,
                                output_dir=&#34;recon_results/&#34;,
                                file_name=&#34;recon_tmp.csv&#34;,
                                mask_prob=0.5,
                                allow_all_mutable=False,
                                save_samples=False,
                                save_output=True):
        &#34;&#34;&#34;reconstruct the masked sample by qsampling and comparing to original
        set self.mask_prob and self.steps if needed

        Args:
          index (int): index of sample to take.
          return_dict (dict): dictionary containing multiprocessing results. Defaults to None.
          sample (list[str], optional): sample vector, must have the same num of features as the qnet. Defaults to None.
          index_colname (str): column name for index. Defaults to &#34;feature_names&#34;
          output_dir (str): directory name for output files. Defaults to &#34;recon_results/&#34;.
          file_name (str): base file name for output files Defaults to &#34;recon_tmp.csv&#34;.
          mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5
          allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.
          save_samples (bool): whether to include sample vectors in the savefile. Defaults to False.
          save_output (bool): whether or not to save output df to file. Defaults to True.

        Raises:
          ValueError: Neither sample or index were given
          ValueError: Both sample and index were given
          
        Returns:
          return_values:(1 - (dqestim/dactual))*100,
                            rmatch_u,
                            rmatch,
                            s,
                            qs,
                            random_sample,
                            mask_
        &#34;&#34;&#34;
        if all(x is None for x in [sample, index]):
            raise ValueError(&#34;Must input either sample or index!&#34;)
        elif all(x is not None for x in [sample, index]):
            raise ValueError(&#34;Must input either sample or index not both!&#34;)
        elif sample is not None:
            s=sample#np.array(pd.DataFrame(sample).fillna(&#39;&#39;).values.astype(str)[:])
        elif index is not None:
            s=self.samples_as_strings[index]
        
        # calculate masked sample and get variables
        s1,bp,mask_,maskindex,rmatch_u,rmatch,random_sample=self.getMaskedSample(s, 
                                                                        mask_prob=mask_prob,
                                                                        allow_all_mutable=allow_all_mutable)
        # if base_frequency is nan, set return_dict to nans
        if np.isnan(bp).any():
            return_dict[index] = np.nan,np.nan,np.nan
            return np.nan,np.nan,np.nan
        
        # make directories
        if not os.path.exists(output_dir):
            os.mkdir(output_dir)

        # qsample sample and calculate distances between original and qsampled 
        qs=qsample(s1,self.qnet,self.steps,bp)
        dqestim=qdistance(s,qs,self.qnet,self.qnet)
        dactual=qdistance(s,s1,self.qnet,self.qnet)
        
        # format and save sample, qsample statistics and values
        cmpf=pd.DataFrame([s,qs,random_sample],
                          columns=self.cols,
                          index=[&#39;sample&#39;,&#39;qsampled&#39;,&#39;random_sample&#39;])[mask_].transpose()
        cmpf.index.name= index_colname
        if save_output:
            file_name = file_name.replace(&#34;tmp&#34;, str(index))
            cmpf.to_csv(output_dir+file_name)
            
        if save_samples:
            return_values= (1 - (dqestim/dactual))*100,rmatch_u,rmatch,s,qs,random_sample,mask_
        else:
            return_values = (1 - (dqestim/dactual))*100,rmatch_u,rmatch,mask_
        
        if return_dict is not None:
            return_dict[index] = return_values
            return return_dict[index]
        return return_values

    def randomMaskReconstruction_multiple(self,
                                          outfile,
                                          processes=6,
                                          save_samples=False,
                                          index_colname=&#34;feature_names&#34;,
                                          output_dir=&#34;recon_results/&#34;,
                                          file_name=&#34;recon_tmp.csv&#34;,
                                          mask_prob=0.5,
                                          allow_all_mutable=False):
        &#39;&#39;&#39;runs and saves the results of the predicted masked sample

        Args:
          output_file (str): directory and/or file for output.
          processes (int): max number of processes. Defaults to 6.
          save_samples (boolean): whether or not to save the generated qsamples, random samples, etc. Defaults to False.
          index_colname=&#34;feature_names&#34;,
          output_dir=&#34;recon_results/&#34;,
          file_name=&#34;recon_tmp.csv&#34;,
          mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5
          allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.
          
        Returns:
          result: pandas.DataFrame containing masking and reconstruction results.
        &#39;&#39;&#39;
        # set columns for mp_compute
        if save_samples:
            cols = [&#39;rederr&#39;,&#39;r_prob&#39;,&#39;rand_err&#39;,&#39;sample&#39;,&#39;qsampled&#39;,&#39;random_sample&#39;,&#39;mask_&#39;]
        else:
            cols = [&#39;rederr&#39;,&#39;r_prob&#39;,&#39;rand_err&#39;,&#39;mask_&#39;]
        
        # 
        args=[None, index_colname, output_dir,
              file_name, mask_prob, allow_all_mutable]
        
        result = self.mp_compute(processes,
                                    self.randomMaskReconstruction,
                                    cols,
                                    outfile,
                                    args=args)
        return result
    
    def dmat_filewriter(self,
                        QNETPATH,
                        mpi_path=&#34;mpi_tmp/&#34;,
                        pyfile=&#34;cognet_qdistmatrix.py&#34;,
                        MPI_SETUP_FILE=&#34;mpi_setup.sh&#34;,
                        MPI_RUN_FILE=&#34;mpi_run.sh&#34;,
                        MPI_LAUNCHER_FILE=&#34;../launcher.sh&#34;,
                        YEARS=&#39;2016&#39;,
                        NODES=4,
                        T=12,
                        num_samples=None,
                        OUTFILE=&#39;tmp_distmatrix.csv&#39;,
                        tmp_samplesfile=&#34;tmp_samples_as_strings.csv&#34;):
        &#34;&#34;&#34;generate files to compute qdistance matrix using mpi parallelization

        Args:
          QNETPATH (str): Qnet filepath
          pyfile (str, optional): Name of generated python file. Defaults to &#34;cognet_qdistmatrix.py&#34;.
          MPI_SETUP_FILE (str, optional): Name of mpi setup script. Defaults to &#34;mpi_setup.sh&#34;.
          MPI_RUN_FILE (str, optional): Name of mpi run script. Defaults to &#34;mpi_run.sh&#34;.
          MPI_LAUNCHER_FILE (str, optional): Launcher script filepath. Defaults to &#34;launcher.sh&#34;.
          YEARS (str, optional): If looping by year, not currently implemented. Defaults to &#39;2016&#39;.
          NODES (int, optional): Number of nodes to use. Defaults to 4.
          T (int, optional): Number of hours to reserve nodes for. Defaults to 12.
          num_samples ([type], optional): How many samples to take. Defaults to None.
          OUTFILE (str, optional): CSV File to write computed qdist matrix. Defaults to &#39;tmp_distmatrix.csv&#39;.
          tmp_samplesfile (str, optional): CSV File to write samples as strings. Defaults to &#34;tmp_samples_as_strings.csv&#34;.

        Raises:
            ValueError: load data if qnet, features, or samples are not present]
        &#34;&#34;&#34;
        if all(x is not None for x in [self.samples,self.features,
                                       self.qnet, self.cols]):
            if num_samples is not None:
                self.set_nsamples(num_samples)
            
            # init and make tmp dir 
            tmp_path = mpi_path
            if not os.path.exists(tmp_path):
                os.makedirs(tmp_path)
            
            pd.DataFrame(self.samples_as_strings).to_csv(tmp_path+tmp_samplesfile, header=None, index=None)
            
            w = self.samples.index.size
            
            # writing python file
            with open(tmp_path+pyfile, &#39;w+&#39;) as f:
                f.writelines([&#34;from mpi4py.futures import MPIPoolExecutor\n&#34;,
                              &#34;import numpy as np\n&#34;,
                              &#34;import pandas as pd\n&#34;,
                              &#34;from quasinet.qnet import Qnet, qdistance, load_qnet, qdistance_matrix\n&#34;,
                              &#34;from quasinet.qsampling import qsample, targeted_qsample\n\n&#34;,
                              &#34;qnet=load_qnet(\&#39;{}\&#39;)\n&#34;.format(QNETPATH)])

                f.writelines([&#34;w = {}\n&#34;.format(w),
                              &#34;h = w\n&#34;,
                              &#34;p_all = pd.read_csv(\&#34;{}\&#34;, header=None).values.astype(str)[:]\n\n&#34;.format(tmp_samplesfile)])

                f.writelines([&#34;def distfunc(x,y):\n&#34;,
                              &#34;\td=qdistance(x,y,qnet,qnet)\n&#34;,
                              &#34;\treturn d\n\n&#34;])

                f.writelines([&#34;def dfunc_line(k):\n&#34;,
                              &#34;\tline = np.zeros(w)\n&#34;,
                              &#34;\ty = p_all[k]\n&#34;,
                              &#34;\tfor j in range(w):\n&#34;,
                              &#34;\t\tif j &gt; k:\n&#34;,
                              &#34;\t\t\tx = p_all[j]\n&#34;,
                              &#34;\t\t\tline[j] = distfunc(x, y)\n&#34;,
                              &#34;\treturn line\n\n&#34;])

                f.writelines([&#34;if __name__ == &#39;__main__&#39;:\n&#34;,
                              &#34;\twith MPIPoolExecutor() as executor:\n&#34;,
                              &#34;\t\tresult = executor.map(dfunc_line, range(h))\n&#34;,
                              &#34;\tresult = pd.DataFrame(result)\n&#34;,
                                  &#34;\tresult = result.to_numpy()\n&#34;,
                              &#34;\tresult = pd.DataFrame(np.maximum(result, result.transpose()))\n&#34;
                              &#34;\tresult.to_csv(\&#39;{}\&#39;,index=None,header=None)&#34;.format(OUTFILE)])
            
            # writing MPI setup file
            with open(tmp_path+MPI_SETUP_FILE, &#39;w+&#39;) as ms:
                ms.writelines([&#34;#!/bin/bash\n&#34;,
                               &#34;YEAR=$1\n\n&#34;,
                               &#34;if [ $# -gt 1 ] ; then\n&#34;,
                               &#34;\tNODES=$2\n&#34;,
                               &#34;else\n&#34;,
                               &#34;\tNODES=3\n&#34;,
                               &#34;fi\n&#34;,
                               &#34;if [ $# -gt 2 ] ; then\n&#34;,
                               &#34;\tNUM=$3\n&#34;,
                               &#34;else\n&#34;,
                               &#34;\tNUM=&#39;all&#39;\n&#34;,
                               &#34;fi\n&#34;,
                               &#34;if [ $# -gt 3 ] ; then\n&#34;,
                               &#34;\tPROG=$4\n&#34;,
                               &#34;else\n&#34;,
                               &#34;\tPROG=$(tty)\n&#34;,
                               &#34;fi\n\n&#34;,
                               &#34;NUMPROC=`expr 28 \* $NODES`\n&#34;,
                               &#34;echo \&#34;module load midway2\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;module unload python\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;module unload openmpi\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;module load python/anaconda-2020.02\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;module load mpi4py\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;date; mpiexec -n \&#34;$NUMPROC\&#34; python3 -m mpi4py.futures {}; date\&#34;  &gt;&gt; $PROG\n&#34;.format(pyfile),
                                ])

            # writing MPI run file
            with open(tmp_path+MPI_RUN_FILE, &#39;w+&#39;) as mr:
                mr.writelines([&#34;#!/bin/bash\n&#34;,
                               &#34;YEARS=\&#39;{}\&#39;\n&#34;.format(YEARS),
                               &#34;# nodes requested\n&#34;,
                               &#34;NODES={}\n&#34;.format(NODES),
                               &#34;# time requested\n&#34;,
                               &#34;T={}\n&#34;.format(T),
                               &#34;NUM=\&#39;all\&#39;\n&#34;,
                               &#34;LAUNCH=\&#39;{}\&#39;\n\n&#34;.format(MPI_LAUNCHER_FILE),
                               &#34;for yr in `echo $YEARS`\n&#34;,
                               &#34;do\n&#34;,
                               &#34;\techo $yr\n&#34;,
                               &#34;\t./{} $yr $NODES $NUM tmp_\&#34;$yr\&#34;\n&#34;.format(MPI_SETUP_FILE),
                               &#34;\t$LAUNCH -P tmp_\&#34;$yr\&#34; -F -T $T -N \&#34;$NODES\&#34; -C 28 -p broadwl -J MPI_TMP_\&#34;$yr\&#34; -M 56\n&#34;,
                               &#34;done\n&#34;,
                               &#34;rm tmp_\&#34;$yr\&#34;*\n&#34;])
            os.system(&#34;cp {} {}&#34;.format(MPI_LAUNCHER_FILE,tmp_path+&#39;mpi_launcher.sh&#39;))
        
        else:
            raise ValueError(&#34;load data first!&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="cognet.cognet.cognet"><code class="flex name class">
<span>class <span class="ident">cognet</span></span>
</code></dt>
<dd>
<div class="desc"><p>Aggregate related Qnet functions</p>
<p>Init</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class cognet:
    &#34;&#34;&#34;Aggregate related Qnet functions
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Init
        &#34;&#34;&#34;
        self.year = None
        self.n_jobs = 28
        self.qnet = None
        self.steps = 120
        self.num_qsamples = None
        self.all_samples = None
        self.samples = None
        self.samples_as_strings = None
        self.features = None
        self.cols = None
        self.immutable_vars = None
        self.mutable_vars = None
        self.poles = None
        self.polar_features = None
        self.polar_indices = None
        self.poles_dict = {}
        self.d0 = None
        self.s_null = None
        self.D_null = None
        self.mask_prob = 0.5
        self.variation_weight = None
        self.polar_matrix = None
        self.nsamples = None
        self.restricted = False
        self.MAX_PROCESSES = 0
    
    def load_from_model(self,
                        model,
                        data_obj,
                        key,
                        im_vars=None,
                        m_vars=None):
        &#34;&#34;&#34;load parameters from model object

        Args:
          model (Class): model obj for loading parameters
          data_obj (class): instance of dataformatter class
          key (str): &#39;all&#39;, &#39;train&#39;, or &#39;test&#39;, corresponding to sample type
          im_vars (list[str], optional): Not implemented yet. Defaults to None.
          m_vars (list[str], optional): Not implemented yet. Defaults to None.
        &#34;&#34;&#34;
        if model is not None:
            # inherit atrributes from model object
            self.qnet = model.myQnet
            featurenames, samples = data_obj.format_samples(key)
            samples = pd.DataFrame(samples)
            self.cols = np.array(featurenames)
            self.features = pd.DataFrame(columns=np.array(featurenames))
            
            # inherit mutable and immutable variables from model obj
            if any(x is not None for x in [model.immutable_vars, model.mutable_vars]):
                if model.immutable_vars is not None:
                    self.immutable_vars = model.immutable_vars
                    self.mutable_vars = [x for x in self.features if x not in self.immutable_vars]
                elif model.mutable_vars is not None:
                    self.mutable_vars = model.mutable_vars
                    self.immutable_vars = [x for x in self.features if x not in self.mutable_vars]
            else:
                self.mutable_vars = self.features
            
            # inherit and set class attributes.
            self.samples = pd.DataFrame(samples).replace(&#34;nan&#34;,&#34;&#34;).fillna(&#34;&#34;)
            self.samples.columns = np.array(featurenames)
            self.all_samples = self.samples
            self.samples_as_strings = self.samples.fillna(&#39;&#39;).values.astype(str)[:]
            self.s_null=[&#39;&#39;]*len(self.samples_as_strings[0])
            self.D_null=self.qnet.predict_distributions(self.s_null)
            variation_weight = []
            for d in self.D_null:
                v=[]
                for val in d.values():
                    v=np.append(v,val)
                variation_weight.append(entropy(v,base=len(v)))
            variation_weight = np.nan_to_num(variation_weight) # remove nans
            self.variation_weight = variation_weight
    
    def load_from_dataformatter(self, 
                                data_obj,
                                key):
        &#34;&#34;&#34;read in either train or test data, specified by key, from data obj,
        and inherit other attributes.

        Args:
          data_obj (class): instance of dataformatter class
          key (str): &#39;all&#39;, &#39;train&#39;, or &#39;test&#39;, corresponding to sample type
          
        Returns:
          featurenames, samples: formatted arrays
        &#34;&#34;&#34;
        # inherit attributes from dataformatter object
        featurenames, samples = data_obj.format_samples(key)
        if any(x is not None for x in [self.features, self.samples]):
            print(&#34;replacing original features/samples with dataformatter data&#34;)
        self.cols = featurenames
        self.features = pd.DataFrame(columns=self.cols)
        self.samples = pd.DataFrame(samples,columns=self.features)
        self.all_samples = self.samples
        self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
        self.s_null=[&#39;&#39;]*len(self.samples_as_strings[0])
        return featurenames, samples

    def load_data(self,
                  year,
                  features_by_year,
                  samples,
                  Qnet):
        &#39;&#39;&#39;load cols, features, samples, and qnet.

        Args:
          year (str): to identify cols/features.
          features_by_year (str): file containing all features by year of the dataset.
          samples (str): file of samples for that year.
          Qnet (str): Qnet file location.
        &#39;&#39;&#39;
        # set attributes from given files and data
        self.qnet = load_qnet(qnet)
        self.year = year
        self.cols = np.array((pd.read_csv(features_by_year,
                            keep_default_na=True, 
                            index_col=0).set_index(
                                &#39;year&#39;)).loc[int(year)].apply(
                                    eval).values[0])
        self.features = pd.DataFrame(columns=self.cols)
        self.mutable_vars = [x for x in self.cols]
        #[self.cols].fillna(&#39;&#39;).values.astype(str)[:]

        # read in samples and initialize related attributes
        self.samples=pd.read_csv(samples)
        self.samples = pd.concat([self.samples,self.features], axis=0)
        self.all_samples = self.samples
        self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
        self.s_null=[&#39;&#39;]*len(self.samples_as_strings[0])
        self.D_null=self.qnet.predict_distributions(self.s_null)
        variation_weight = []
        for d in self.D_null:
            v=[]
            for val in d.values():
                v=np.append(v,val)
            variation_weight.append(entropy(v,base=len(v)))
        self.variation_weight = variation_weight

    def set_immutable_vars(self,
                        IMMUTABLE_FILE):
        &#39;&#39;&#39;set vars to immutable and mutable, 
        can prob combine this with the load_data func: only set the immutable vars if necessary

        Args:
          IMMUTABLE_FILE (str): file containing the immutable features/vars
        &#39;&#39;&#39;
        # set mutable and immutable variable attributes 
        if self.cols is None:
            raise ValueError(&#34;load_data first!&#34;)
        self.immutable_vars = pd.read_csv(IMMUTABLE_FILE,index_col=0).transpose()
        self.mutable_vars = None
        self.mutable_vars = [x for x in self.cols
                            if x.upper() not in self.immutable_vars.columns]
    
    def set_nsamples(self,
                    num_samples,
                    random=False):
        &#39;&#39;&#39;select a subset of the samples

        Args:
          num_samples (int): Set num of samples to subset, default to None, resets to all samples
          random (bool): take random sample if true, ordered sample if false
        &#39;&#39;&#39;
        # each time function is called, reset samples to use_all_samples
        # this allows us to call nsamples numerous times 
        self.samples = self.all_samples
        if self.samples is not None:
            # if a greater number of sample is selected than available, raise error
            if all(x is not None for x in [num_samples, self.samples]):
                if num_samples &gt; len(self.samples.index):
                    string = &#39;The number of selected samples ({}) &#39; + \
                        &#39;is greater than the number of samples ({})!&#39;
                    string = string.format(num_samples, len(self.samples.index))
                    raise ValueError(string)

                # if the same number of samples is selected as available, print warning
                if num_samples == len(self.samples.index):
                    string = &#39;The number of selected samples ({}) &#39; + \
                        &#39;is equal to the number of samples ({})!&#39;
                    string = string.format(num_samples, len(self.samples.index))
                    print(string)
                    
                # if random is true, return random sample, otherwise return an ordered slice
                if random:
                    self.samples = self.samples.sample(num_samples)
                else:
                    self.samples = self.samples.iloc[:num_samples]
                self.nsamples = num_samples
                self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
                
            elif self.samples is None:
                raise ValueError(&#34;load_data first!&#34;)

    def __variation_weight(self,
                        index):
        &#34;&#34;&#34;
        &#34;&#34;&#34;
        d_=self.D_null[index]
        v=[]
        for val in d_.values():
            v=np.append(v,val)
        return entropy(v,base=len(v))
    
    def getBaseFrequency(self, 
                        sample):
        &#39;&#39;&#39;get frequency of the variables
        helper func for qsampling

        Args:
          sample (list[str]): vector of sample, must have the same num of features as the qnet
        &#39;&#39;&#39;
        # if variable is not mutable, set its base frequency to zero 
        MUTABLE=pd.DataFrame(np.zeros(len(self.cols)),index=self.cols).transpose()
             
        for m in self.mutable_vars:
            MUTABLE[m]=1.0
        mutable_x=MUTABLE.values[0]
        base_frequency=mutable_x/mutable_x.sum()
        
        # otherwise, set base frequency weighted by variation weight
        for i in range(len(base_frequency)):
            if base_frequency[i]&gt;0.0:
                base_frequency[i]= self.variation_weight[i]*base_frequency[i]

        return base_frequency/base_frequency.sum()
    
    def qsampling(self,
                sample,
                steps,
                immutable=False):
        &#39;&#39;&#39;perturb the sample based on thet qnet distributions and number of steps

        Args:
          sample (1d array-like): sample vector, must have the same num of features as the qnet
          steps (int): number of steps to qsample
          immutable (bool): are there variables that are immutable?
        &#39;&#39;&#39;
        # immutable, check that mutable variables have been initialized
        if immutable == True:
            if all(x is not None for x in [self.mutable_vars, sample]):
                return qsample(sample,self.qnet,steps,self.getBaseFrequency(self.samples))
            elif self.mutable_vars is None:
                raise ValueError(&#34;set mutable and immutable variables first!&#34;)
        else:
            return qsample(sample,self.qnet,steps)

    def random_sample(self,
                      df=None,
                      n=1):
        &#39;&#39;&#39;compute a random sample from the underlying distributions of the dataset, by column.
        
        
        Args:
          df (pandas.DataFrame): Desired data to take random sample of. Defaults to None, in which case qnet samples are used.
          n (int): number of random samples to take. Defaults to 1.
          
        Returns:
          return_df (pd.DataFrame): Random sample drawn from underlying distribution of each column.
        &#39;&#39;&#39;
        # check if a new dataset was inputted
        if df is None:
            samples_ = self.samples
        else:
            samples_ = df

        # take random sample from each of the columns based on their distribution
        return_df = pd.DataFrame()
        for col in samples_.columns:
            return_df[col] = samples_[col].sample(n=n, replace=True).values
            
        return return_df
    
    def set_poles(self,
                  POLEFILE,
                  pole_1,
                  pole_2,
                  steps=0,
                  mutable=False,
                  VERBOSE=False,
                  restrict=True,
                  nsamples = None,
                  random=False):
        &#39;&#39;&#39;set the poles and samples such that the samples contain features in poles

        Args:
          steps (int): number of steps to qsample
          POLEFILE (str): file containing poles samples and features
          pole_1 (str): column name for first pole
          pole_2 (str): column name for second pole
          mutable (bool): Whether or not to set poles as the only mutable_vars
          VERBOSE (bool): boolean flag prints number of pole features not found in sample features if True
          restrict (bool): boolean flag restricts the sample features to polar features if True
          random (bool): boolean flag takes random sample of all_samples
        &#39;&#39;&#39;
        invalid_count = 0
        if all(x is not None for x in [self.samples, self.qnet]):
            # read and set poles
            poles = pd.read_csv(POLEFILE, index_col=0)
            self.poles=poles.transpose()
            self.polar_features = pd.concat([self.features, self.poles], axis=0).fillna(&#39;&#39;)
            poles_dict = {}
            for column in poles:
                p_ = self.polar_features.loc[column][self.cols].fillna(&#39;&#39;).values.astype(str)[:]
                # qsample poles to qnet
                poles_dict[column] = self.qsampling(p_,steps)
            self.poles_dict = poles_dict
            self.pL = self.poles_dict[pole_1]
            self.pR = self.poles_dict[pole_2]
            self.d0 = qdistance(self.pL, self.pR, self.qnet, self.qnet)
            
            # restrict sample columns to polar columns
            if restrict:
                cols = [x for x in self.poles.columns if x in self.samples.columns]
                self.samples=self.samples[cols]
                self.restricted = True
                self.samples = pd.concat([self.features,self.samples], axis=0).replace(&#34;nan&#34;,&#34;&#34;).fillna(&#39;&#39;)
                self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
                
            # if restrict==False, unrestrict it and set original
            else:
                self.restricted = False
                self.samples = self.all_samples
                if self.nsamples is not None:
                    self.set_nsamples(nsamples, random)
            
            # identify pole features that were excluded due to sample features restriction
            if VERBOSE:
                for x in self.poles.columns:
                    if x not in self.samples.columns:
                        invalid_count += 1
                        #self.samples[x]=&#39;&#39;
            
            if mutable:
                self.mutable_vars=[x for x in self.cols if x in self.poles.columns]
        elif self.samples is None:
            raise ValueError(&#34;load_data first!&#34;)

        if VERBOSE:
            print(&#34;{} pole features not found in sample features&#34;.format(invalid_count))

    def mp_compute(self, 
                   processes,
                   func, 
                   cols,
                   outfile, 
                   args=[]):
        &#34;&#34;&#34;
        Compute desired function through multiprocessing and save result to csv.

        Args:
          processes (int): number of processes to use.
          func (func): function to compute using multiprocessing
          cols (list): column names of resulting csv
          outfile (str)): filepath + filename for resulting csv
          args (list): list containing arguments for desired function. Defaults to empty list.
        &#34;&#34;&#34;

        # init mp.Manager and result dict
        manager = mp.Manager()
        return_dict = manager.dict()

        # set processes as given, unless class parameter is set
        max_processes = processes
        if self.MAX_PROCESSES != 0:
            max_processes = self.MAX_PROCESSES
            print(&#34;Number of Processes {} has been set using class parameter&#34;.format(self.MAX_PROCESSES))
        num_processes = 0
        process_list = []
        
        # init mp.Processes for each individual sample
        # run once collected processes hit max
        for i in range(len(self.samples)):
            params = tuple([i, return_dict] + args)
            num_processes += 1
            p = mp.Process(target=func,
                        args=params)
            process_list.append(p)
            if num_processes == max_processes:
                [x.start() for x in process_list]
                [x.join() for x in process_list]
                process_list = []
                num_processes = 0
                
        # compute remaining processes
        if num_processes != 0:
            [x.start() for x in process_list]
            [x.join() for x in process_list]
            process_list = []
            num_processes = 0
        
        # format and save resulting dict
        result = pd.DataFrame(return_dict.values(), columns=cols, index=return_dict.keys()).sort_index()
        result.to_csv(outfile, index=None)
        return result
    
    def distance(self,
                sample1,
                sample2,
                nsteps1=0,
                nsteps2=0):
        &#34;&#34;&#34;qsamples each sample set num of steps, then takes qdistance

        Args:
          sample1 (list[str]): sample vector 1, must have the same num of features as the qnet
          sample2 (list[str]): sample vector 2, must have the same num of features as the qnet
          nsteps1 (int, optional): number of steps to qsample for sample1
          nsteps2 (int, optional): number of steps to qsample for sample2

        Returns:
          qdistance: float, distance between two samples
        &#34;&#34;&#34;
        if self.qnet is None:
            raise ValueError(&#34;load qnet first!&#34;)
        #bp1 = self.getBaseFrequency(sample1)
        #bp2 = self.getBaseFrequency(sample2)
        # qsample samples
        sample1 = qsample(sample1, self.qnet, nsteps1)#, baseline_prob=bp1)
        sample2 = qsample(sample2, self.qnet, nsteps2)#, baseline_prob=bp2)
        return qdistance(sample1, sample2, self.qnet, self.qnet)
    
    def __distfunc(self, 
                   x, 
                   y):
        &#39;&#39;&#39;Compute distance between two samples

        Args:
          x (list[str]): first sample
          y (list[str]): second sample
          
        Returns:
         d: qdistance
        &#39;&#39;&#39;
        d=qdistance(x,y,self.qnet,self.qnet)
        return d
    
    def distfunc_line(self,
                    i,
                    return_dict=None):
        &#39;&#39;&#39;compute the distance for a single sample from all other samples

        Args:
          i (int): row
          return_dict (dict): dictionary containing multiprocessing results
        
        Return:
          line: float, numpy.ndarray
        &#39;&#39;&#39;
        if all(x is not None for x in [self.samples, self.features]):
            w = self.samples.index.size
            line = np.zeros(w)
            y = self.samples_as_strings[i]
            for j in range(w):
                # only compute half of the distance matrix
                if j &gt; i:
                    x = self.samples_as_strings[j]
                    line[j] = self.__distfunc(x, y)
        else:
            raise ValueError(&#34;load_data first!&#34;)
        if return_dict is not None:
            return_dict[i] = line
        return line
    
    def distfunc_multiples(self,
                           outfile,
                           processes=6):
        &#34;&#34;&#34;compute distance matrix for all samples in the dataset

        Args:
          outfile (str): desired output filename and path
          
        Returns:
          result: pandas.DataFrame containing distance matrix
        &#34;&#34;&#34;
        if all(x is not None for x in [self.samples, self.features]):
            cols = [i for i in range(len(self.samples))]
            result = self.mp_compute(processes,
                                        self.distfunc_line,
                                        cols,
                                        outfile)
            # format and save resulting dict, and tranpose symmetrical distance matrix
            result = result.to_numpy()
            result = pd.DataFrame(np.maximum(result, result.transpose()))
            result.to_csv(outfile, index=None, header=None)
        else:
            raise ValueError(&#34;load data first!&#34;)
        
        return result
    
    def polarDistance(self,
                    i,
                    return_dict=None):
        &#34;&#34;&#34;return the distances from a single sample to the poles

        Args:
          i (int): index of sample to take
          return_dict (dict): dictionary containing multiprocessing results

        Returns:
          distances: float, distance from sample to each pole
        &#34;&#34;&#34;
        p = self.samples_as_strings[i]
        distances = []
        # calculate from each pole to the sample, and append to array
        for index, row in self.polar_features[self.cols].iterrows():
            row = row.fillna(&#39;&#39;).values.astype(str)[:]
            distances.append(self.distance(p, np.array(row)))
        if return_dict is not None:
            return_dict[i] = distances
        return distances
            
    def polarDistance_multiple(self,
                               outfile,
                               processes=6):
        &#34;&#34;&#34;return the distance from all samples to the poles

        Args:
          outfile (str): desired output filename and path
          
        Returns:
          result: pandas.DataFrame containing polar distance results
        &#34;&#34;&#34;
        if all(x is not None for x in [self.samples, self.cols,
                                    self.polar_features]):
            # get the column names
            pole_names = []
            for index, row in self.polar_features[self.cols].iterrows():
                pole_names.append(index)
            result = self.mp_compute(processes,
                                        self.polarDistance,
                                        pole_names,
                                        outfile)
        else:
            raise ValueError(&#34;load data first!&#34;)
        return result
    
    def polar_separation(self,
                        nsteps=0):
        &#34;&#34;&#34;calculates the distance between poles as a qdistance matrix

        Args:
          nsteps (int, optional): [description]. Defaults to 0.
          
        Returns:
          self.polar_matrix: dictionary containing multiprocessing results
        &#34;&#34;&#34;
        # vectorize and qsample poles
        polar_arraydata = self.polar_features[self.cols].values.astype(str)[:]
        samples_ = []
        for vector in polar_arraydata:
            bp = self.getBaseFrequency(vector)
            sample = qsample(vector, self.qnet, nsteps, baseline_prob=bp)
            samples_.append(sample)
        samples_ = np.array(samples_)
        # calculate distance matrix for poles
        self.polar_matrix = qdistance_matrix(samples_, samples_, self.qnet, self.qnet)
        return self.polar_matrix
        
    def embed(self,
            infile,
            name_pref,
            out_dir,
            pca_model=False,
            EMBED_BINARY=None):
        &#39;&#39;&#39;
        embed data

        Args:
          infile (str): input file to be embedded
          name_pref (str): preferred name for output file
          out_dir (str): output dir for results
          pca_model (bool): whether or not to generate PCA model
          EMBED_BINARY (os.path.abspath): path to embed binary
        &#39;&#39;&#39;
        if all(x is not None for x in [self.year]):
            # init file names 
            yr = self.year
            PREF = name_pref
            FILE = infile
            DATAFILE = out_dir + &#39;data_&#39; +yr
            EFILE = out_dir + PREF + &#39;_E_&#39; +yr
            DFILE = out_dir + PREF + &#39;_D_&#39; +yr
            
            # set embed binary directory
            if EMBED_BINARY is None:
                EMBED = pkgutil.get_data(&#34;cognet.bin&#34;, &#34;__embed__.so&#34;) 
            else:
                EMBED = EMBED_BINARY
            
            # embed data files
            pd.read_csv(FILE, header=None).to_csv(DATAFILE,sep=&#39; &#39;,header=None,index=None)
            STR=EMBED+&#39; -f &#39;+DATAFILE+&#39; -E &#39;+EFILE+&#39; -D &#39;+DFILE
            subprocess.call(STR,shell=True)
            if pca_model:
                embed_to_pca(EFILE, EFILE+&#39;_PCA&#39;)
        elif self.year is None:
            raise ValueError(&#34;load_data first!&#34;)
    
    def __calc_d0(self,
                pole_1,
                pole_2):
        &#34;&#34;&#34;calculate distance between two poles

        Args:
          pole_1 (list[str]): a polar vector, must have same number of features as qnet
          pole_2 (list[str]): a polar vector, must have same number of features as qnet
        &#34;&#34;&#34;
        self.pL = self.poles_dict[pole_1]
        self.pR = self.poles_dict[pole_2]
        self.d0 = qdistance(self.pL, self.pR, self.qnet, self.qnet)
        
    def ideology(self,
                i,
                return_dict=None,
                pole_1=None,
                pole_2=None):
        &#34;&#34;&#34;return ideology index (left-leaning or right-leaning) for a singular sample

        Args:
          i (int): index of sample
          pole_1 (int): index of Pole One to calc as base distance. Defaults to 0.
          pole_2 (int): index of Pole Two to calc as base distance. Defaults to 1.
          return_dict (dict, optional): dict containing results
          
        Returns:
          [ideology_index, dR, dL, self.d0]: which way the sample leans,
                                             distance from the right pole,
                                             distance from the left pole,
                                             and distance between poles, respectively
        &#34;&#34;&#34;
        # calculate base distance between two poles
        if pole_1 is not None or pole_2 is not None:
            self.__calc_d0(pole_1, pole_2)
        
        # calculate distances between sample and the two poles
        p = self.samples_as_strings[i]
        dR = qdistance(self.pR, p, self.qnet, self.qnet)
        dL = qdistance(self.pL, p, self.qnet, self.qnet)
        
        ideology_index = (dR-dL)/self.d0
        if return_dict is not None:
            return_dict[i] = [ideology_index, dR, dL, self.d0]
        return [ideology_index, dR, dL, self.d0]

    def dispersion(self,
                   i,
                   return_dict=None):
        &#34;&#34;&#34;qsamples a sample n times and takes distance matrix 
        to determine max and std of distances between qsamples

        Args:
          i (int): index of sample
          return_dict (dict): dictionary containing multiprocessing results

        Returns:
          list[float]: std and max of the distances btwn qsamples
        &#34;&#34;&#34;
        # qsample sample num_qsample times
        p = self.samples_as_strings[i]
        Qset = [qsample(p, self.qnet, self.steps) for j in np.arange(self.num_qsamples)]
        Qset = np.array(Qset)

        # calculate qdistance matrix for qsampled samples
        matrix = (qdistance_matrix(Qset, Qset, self.qnet, self.qnet))
        Q = matrix.max()
        Qsd = matrix.std()
        
        if return_dict is not None:
            return_dict[i] = [Qsd, Q]
        return [Qsd, Q]
    
    def compute_DLI_samples(self,
                        type,
                        outfile,
                        num_qsamples=40,
                        steps=120,
                        n_jobs=28,
                        pole_1=0,
                        pole_2=1,
                        processes=6):
        &#34;&#34;&#34;compute and save ideology index or dispersion for all samples

        Args:
          num_qsamples (int): number of qsamples to compute
          outfile (str): output file for results
          type (str): whether to calc dispersion or ideology
          steps (int): number of steps to qsample
          n_jobs (int, optional): sets the number of jobs for parallelization. Defaults to 28.
          pole_1 (int, optional): index of Pole One to calc as base distance. Defaults to 0.
          pole_2 (int, optional): index of Pole Two to calc as base distance. Defaults to 1.

        Raises:
          ValueError: set poles if poles are not set
          ValueError: load data if samples or features are not present
            
        Returns:
          result: pandas.DataFrame containing multiprocessing results
        &#34;&#34;&#34;
        if all(x is not None for x in [self.samples, self.features,
                                    self.pL, self.pR]):
            # init vars
            self.num_qsamples = num_qsamples
            self.steps = steps
            if pole_1 != 0 or pole_2 != 1:
                self.__calc_d0(pole_1, pole_2)
            
            if type == &#39;ideology&#39;:
                func_ = self.ideology
                cols=[&#39;ideology&#39;, &#39;dR&#39;, &#39;dL&#39;, &#39;d0&#39;]
            elif type == &#39;dispersion&#39;:
                func_ = self.dispersion
                cols=[&#39;Qsd&#39;, &#39;Qmax&#39;]
            else:
                raise ValueError(&#34;Type must be either dispersion or ideology!&#34;)
            
            result = self.mp_compute(processes,
                                     func_,
                                     cols,
                                     outfile)
        elif self.pL is None or self.pR is None:
            raise ValueError(&#34;set_poles first!&#34;)
        else:
            raise ValueError(&#34;load_data first!&#34;)
        return result

    def compute_polar_indices(self,
                              num_samples=None,
                              polar_comp=False,
                              POLEFILE=None,
                              steps=5):
        &#39;&#39;&#39;set up polar indices for dissonance func

        Args:
          num_samples (int): subset of samples to take
          polar_comp (bool): whether or not to set poles
          POLEFILE (None): file containing pole samples and features
          steps (int): number of steps to qsample
        &#39;&#39;&#39;
        if all(x is not None for x in [self.samples, self.features, self.poles]):
            if num_samples is not None:
                self.set_nsamples(num_samples)

            if polar_comp:
                self.set_poles(self.qnet, steps, POLEFILE)
            
            # calculate polar indices
            polar_features = pd.concat([self.features, self.poles], axis=0)
            self.polar_indices=np.where(polar_features[self.cols].fillna(&#39;XXXX&#39;).values[0]!=&#39;XXXX&#39;)[0]
        
        elif self.poles is None:
            raise ValueError(&#34;set_poles first!&#34;)
        else:
            raise ValueError(&#34;load_data first!&#34;)

    def dissonance(self,
                    sample_index=0,
                    return_dict=None,
                    MISSING_VAL=0.0,
                    sample=None):
        &#39;&#39;&#39;compute dissonance for a single sample, helper function for all_dissonance
        
        Args:
          sample_index (int): index of the sample to compute dissonance. Defaults to 0.
          return_dict (dict): dictionary containing multiprocessing results
          MISSING_VAL (float): default dissonance value
          sample (1D array): sample to compute dissonance of, instead of using sample index. Defaults to None.
          
        Returns: 
          diss[self.polar_indices]: ndarray containing dissonance for sample
        &#39;&#39;&#39;
        if all(x is not None for x in [self.samples, self.features]):
            if sample is None:
                s = self.samples_as_strings[sample_index]
            else:
                s = sample
            if self.polar_indices is None:
                self.polar_indices = range(len(s))

            # init vars and calculate dissonance for sample
            Ds=self.qnet.predict_distributions(s)
            diss=np.ones(len(Ds))*MISSING_VAL
            for i in self.polar_indices:
                if s[i] != &#39;&#39;:
                    if s[i] in Ds[i].keys():
                        diss[i]=1-Ds[i][s[i]]/np.max(
                            list(Ds[i].values())) 
                    else:
                        diss[i]=1.0
            if return_dict is not None:
                return_dict[sample_index] = diss[self.polar_indices]
            return diss[self.polar_indices]
        else:
            raise ValueError(&#34;load_data first!&#34;)
    
    def dissonance_matrix(self,
                        outfile=&#39;/example_results/DISSONANCE_matrix.csv&#39;,
                        processes=6):
        &#39;&#39;&#39;get the dissonance for all samples

        Args:
          output_file (str): directory and/or file for output
          processes (int): max number of processes. Defaults to 6.

        Returns:
          result: pandas.DataFrame containing dissonances for each sample
        &#39;&#39;&#39;
        # set columns
        if self.polar_indices is not None:
            polar_features = pd.concat([self.features, self.poles], axis=0)
            cols = polar_features[self.cols].dropna(axis=1).columns
        else:
            cols = self.cols
        
        result = self.mp_compute(processes,
                                    self.dissonance,
                                    cols,
                                    outfile)
        return result
    
    def __choose_one(self,
                X):
        &#39;&#39;&#39;returns a random element of X

        Args:
          X (1D array-like): vector from which random element is to be chosen
        
        Returns:
          X: random element of sample
          None: if X has len 0
        &#39;&#39;&#39;
        X=list(X)
        if len(X)&gt;0:
            return X[np.random.randint(len(X))]
        return None

    def getMaskedSample(self,
                        s,
                        mask_prob=0.5,
                        allow_all_mutable=False):
        &#39;&#39;&#39;inputs a sample and randomly mask elements of the sample

        Args:
          s (list[str]): vector of sample, must have the same num of features as the qnet.
          mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5
          allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.
          
        Returns:
          s1,
          base_frequency,
          MASKrand,
          np.where(base_frequency)[0],
          np.mean(rnd_match_prob),
          np.mean(max_match_prob),
          random_sample
        &#39;&#39;&#39;
        if self.samples is not None:
            # init random mutable variable masking
            s0=s.copy()
            s0=np.array(s0)   
            # double check, because code seems to imply that masking happens in order,
            # i.e. limited to the first 100 features, if there are only 100 mutable features
            MUTABLE=pd.DataFrame(np.zeros(len(self.cols)),index=self.cols).transpose()
            WITHVAL=[x for x in self.cols[np.where(s0)[0]] if x in self.mutable_vars ]
            MASKrand=[x for x in WITHVAL if random.random() &lt; mask_prob ]
            for m in MASKrand:
                MUTABLE[m]=1.0
            
            mutable_x=MUTABLE.values[0]
            base_frequency=mutable_x/mutable_x.sum()

            # if np.isnan(base_frequency).any():
            #     return np.nan,np.nan,np.nan
            #     return self.getMaskedSample(s)

            # mask sample according to masking (base_frequency)
            s1=s.copy()
            for i in range(len(base_frequency)):
                if base_frequency[i]&gt;0.0001:
                    s1[i]=&#39;&#39;
                
            # create a random sample to test reconstruction effectiveness
            random_sample=np.copy(s)
            rnd_match_prob=[]        
            max_match_prob=[]        
            D=self.qnet.predict_distributions(s)
            for i in MASKrand:
                random_sample[np.where(
                    self.cols==i)[0][0]]=self.__choose_one(
                        self.D_null[np.where(self.cols==i)[0][0]].keys())
                rnd_match_prob=np.append(rnd_match_prob,1/len(
                    self.D_null[np.where(self.cols==i)[0][0]].keys()))
                max_match_prob=np.append(
                    max_match_prob,np.max(
                        list(D[np.where(
                            self.cols==i)[0][0]].values())))
            
            # calculate base_frequency if all variables are mutable
            if allow_all_mutable:
                WITHVAL=[x for x in self.cols[np.where(s0)[0]]]
                MASKrand=[x for x in WITHVAL if random.random() &lt; mask_prob ]
                for m in MASKrand:
                    MUTABLE[m]=1.0
                mutable_x=MUTABLE.values[0]
                base_frequency=mutable_x/mutable_x.sum()
                s1=s.copy()
                for i in range(len(base_frequency)):
                    if base_frequency[i]&gt;0.0001:
                        s1[i]=&#39;&#39;

            return s1,base_frequency,MASKrand,np.where(
                base_frequency)[0],np.mean(rnd_match_prob),np.mean(max_match_prob),random_sample
        else:
            raise ValueError(&#34;load_data first!&#34;)

    def randomMaskReconstruction(self,
                                index=None,
                                return_dict=None,
                                sample=None,
                                index_colname=&#34;feature_names&#34;,
                                output_dir=&#34;recon_results/&#34;,
                                file_name=&#34;recon_tmp.csv&#34;,
                                mask_prob=0.5,
                                allow_all_mutable=False,
                                save_samples=False,
                                save_output=True):
        &#34;&#34;&#34;reconstruct the masked sample by qsampling and comparing to original
        set self.mask_prob and self.steps if needed

        Args:
          index (int): index of sample to take.
          return_dict (dict): dictionary containing multiprocessing results. Defaults to None.
          sample (list[str], optional): sample vector, must have the same num of features as the qnet. Defaults to None.
          index_colname (str): column name for index. Defaults to &#34;feature_names&#34;
          output_dir (str): directory name for output files. Defaults to &#34;recon_results/&#34;.
          file_name (str): base file name for output files Defaults to &#34;recon_tmp.csv&#34;.
          mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5
          allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.
          save_samples (bool): whether to include sample vectors in the savefile. Defaults to False.
          save_output (bool): whether or not to save output df to file. Defaults to True.

        Raises:
          ValueError: Neither sample or index were given
          ValueError: Both sample and index were given
          
        Returns:
          return_values:(1 - (dqestim/dactual))*100,
                            rmatch_u,
                            rmatch,
                            s,
                            qs,
                            random_sample,
                            mask_
        &#34;&#34;&#34;
        if all(x is None for x in [sample, index]):
            raise ValueError(&#34;Must input either sample or index!&#34;)
        elif all(x is not None for x in [sample, index]):
            raise ValueError(&#34;Must input either sample or index not both!&#34;)
        elif sample is not None:
            s=sample#np.array(pd.DataFrame(sample).fillna(&#39;&#39;).values.astype(str)[:])
        elif index is not None:
            s=self.samples_as_strings[index]
        
        # calculate masked sample and get variables
        s1,bp,mask_,maskindex,rmatch_u,rmatch,random_sample=self.getMaskedSample(s, 
                                                                        mask_prob=mask_prob,
                                                                        allow_all_mutable=allow_all_mutable)
        # if base_frequency is nan, set return_dict to nans
        if np.isnan(bp).any():
            return_dict[index] = np.nan,np.nan,np.nan
            return np.nan,np.nan,np.nan
        
        # make directories
        if not os.path.exists(output_dir):
            os.mkdir(output_dir)

        # qsample sample and calculate distances between original and qsampled 
        qs=qsample(s1,self.qnet,self.steps,bp)
        dqestim=qdistance(s,qs,self.qnet,self.qnet)
        dactual=qdistance(s,s1,self.qnet,self.qnet)
        
        # format and save sample, qsample statistics and values
        cmpf=pd.DataFrame([s,qs,random_sample],
                          columns=self.cols,
                          index=[&#39;sample&#39;,&#39;qsampled&#39;,&#39;random_sample&#39;])[mask_].transpose()
        cmpf.index.name= index_colname
        if save_output:
            file_name = file_name.replace(&#34;tmp&#34;, str(index))
            cmpf.to_csv(output_dir+file_name)
            
        if save_samples:
            return_values= (1 - (dqestim/dactual))*100,rmatch_u,rmatch,s,qs,random_sample,mask_
        else:
            return_values = (1 - (dqestim/dactual))*100,rmatch_u,rmatch,mask_
        
        if return_dict is not None:
            return_dict[index] = return_values
            return return_dict[index]
        return return_values

    def randomMaskReconstruction_multiple(self,
                                          outfile,
                                          processes=6,
                                          save_samples=False,
                                          index_colname=&#34;feature_names&#34;,
                                          output_dir=&#34;recon_results/&#34;,
                                          file_name=&#34;recon_tmp.csv&#34;,
                                          mask_prob=0.5,
                                          allow_all_mutable=False):
        &#39;&#39;&#39;runs and saves the results of the predicted masked sample

        Args:
          output_file (str): directory and/or file for output.
          processes (int): max number of processes. Defaults to 6.
          save_samples (boolean): whether or not to save the generated qsamples, random samples, etc. Defaults to False.
          index_colname=&#34;feature_names&#34;,
          output_dir=&#34;recon_results/&#34;,
          file_name=&#34;recon_tmp.csv&#34;,
          mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5
          allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.
          
        Returns:
          result: pandas.DataFrame containing masking and reconstruction results.
        &#39;&#39;&#39;
        # set columns for mp_compute
        if save_samples:
            cols = [&#39;rederr&#39;,&#39;r_prob&#39;,&#39;rand_err&#39;,&#39;sample&#39;,&#39;qsampled&#39;,&#39;random_sample&#39;,&#39;mask_&#39;]
        else:
            cols = [&#39;rederr&#39;,&#39;r_prob&#39;,&#39;rand_err&#39;,&#39;mask_&#39;]
        
        # 
        args=[None, index_colname, output_dir,
              file_name, mask_prob, allow_all_mutable]
        
        result = self.mp_compute(processes,
                                    self.randomMaskReconstruction,
                                    cols,
                                    outfile,
                                    args=args)
        return result
    
    def dmat_filewriter(self,
                        QNETPATH,
                        mpi_path=&#34;mpi_tmp/&#34;,
                        pyfile=&#34;cognet_qdistmatrix.py&#34;,
                        MPI_SETUP_FILE=&#34;mpi_setup.sh&#34;,
                        MPI_RUN_FILE=&#34;mpi_run.sh&#34;,
                        MPI_LAUNCHER_FILE=&#34;../launcher.sh&#34;,
                        YEARS=&#39;2016&#39;,
                        NODES=4,
                        T=12,
                        num_samples=None,
                        OUTFILE=&#39;tmp_distmatrix.csv&#39;,
                        tmp_samplesfile=&#34;tmp_samples_as_strings.csv&#34;):
        &#34;&#34;&#34;generate files to compute qdistance matrix using mpi parallelization

        Args:
          QNETPATH (str): Qnet filepath
          pyfile (str, optional): Name of generated python file. Defaults to &#34;cognet_qdistmatrix.py&#34;.
          MPI_SETUP_FILE (str, optional): Name of mpi setup script. Defaults to &#34;mpi_setup.sh&#34;.
          MPI_RUN_FILE (str, optional): Name of mpi run script. Defaults to &#34;mpi_run.sh&#34;.
          MPI_LAUNCHER_FILE (str, optional): Launcher script filepath. Defaults to &#34;launcher.sh&#34;.
          YEARS (str, optional): If looping by year, not currently implemented. Defaults to &#39;2016&#39;.
          NODES (int, optional): Number of nodes to use. Defaults to 4.
          T (int, optional): Number of hours to reserve nodes for. Defaults to 12.
          num_samples ([type], optional): How many samples to take. Defaults to None.
          OUTFILE (str, optional): CSV File to write computed qdist matrix. Defaults to &#39;tmp_distmatrix.csv&#39;.
          tmp_samplesfile (str, optional): CSV File to write samples as strings. Defaults to &#34;tmp_samples_as_strings.csv&#34;.

        Raises:
            ValueError: load data if qnet, features, or samples are not present]
        &#34;&#34;&#34;
        if all(x is not None for x in [self.samples,self.features,
                                       self.qnet, self.cols]):
            if num_samples is not None:
                self.set_nsamples(num_samples)
            
            # init and make tmp dir 
            tmp_path = mpi_path
            if not os.path.exists(tmp_path):
                os.makedirs(tmp_path)
            
            pd.DataFrame(self.samples_as_strings).to_csv(tmp_path+tmp_samplesfile, header=None, index=None)
            
            w = self.samples.index.size
            
            # writing python file
            with open(tmp_path+pyfile, &#39;w+&#39;) as f:
                f.writelines([&#34;from mpi4py.futures import MPIPoolExecutor\n&#34;,
                              &#34;import numpy as np\n&#34;,
                              &#34;import pandas as pd\n&#34;,
                              &#34;from quasinet.qnet import Qnet, qdistance, load_qnet, qdistance_matrix\n&#34;,
                              &#34;from quasinet.qsampling import qsample, targeted_qsample\n\n&#34;,
                              &#34;qnet=load_qnet(\&#39;{}\&#39;)\n&#34;.format(QNETPATH)])

                f.writelines([&#34;w = {}\n&#34;.format(w),
                              &#34;h = w\n&#34;,
                              &#34;p_all = pd.read_csv(\&#34;{}\&#34;, header=None).values.astype(str)[:]\n\n&#34;.format(tmp_samplesfile)])

                f.writelines([&#34;def distfunc(x,y):\n&#34;,
                              &#34;\td=qdistance(x,y,qnet,qnet)\n&#34;,
                              &#34;\treturn d\n\n&#34;])

                f.writelines([&#34;def dfunc_line(k):\n&#34;,
                              &#34;\tline = np.zeros(w)\n&#34;,
                              &#34;\ty = p_all[k]\n&#34;,
                              &#34;\tfor j in range(w):\n&#34;,
                              &#34;\t\tif j &gt; k:\n&#34;,
                              &#34;\t\t\tx = p_all[j]\n&#34;,
                              &#34;\t\t\tline[j] = distfunc(x, y)\n&#34;,
                              &#34;\treturn line\n\n&#34;])

                f.writelines([&#34;if __name__ == &#39;__main__&#39;:\n&#34;,
                              &#34;\twith MPIPoolExecutor() as executor:\n&#34;,
                              &#34;\t\tresult = executor.map(dfunc_line, range(h))\n&#34;,
                              &#34;\tresult = pd.DataFrame(result)\n&#34;,
                                  &#34;\tresult = result.to_numpy()\n&#34;,
                              &#34;\tresult = pd.DataFrame(np.maximum(result, result.transpose()))\n&#34;
                              &#34;\tresult.to_csv(\&#39;{}\&#39;,index=None,header=None)&#34;.format(OUTFILE)])
            
            # writing MPI setup file
            with open(tmp_path+MPI_SETUP_FILE, &#39;w+&#39;) as ms:
                ms.writelines([&#34;#!/bin/bash\n&#34;,
                               &#34;YEAR=$1\n\n&#34;,
                               &#34;if [ $# -gt 1 ] ; then\n&#34;,
                               &#34;\tNODES=$2\n&#34;,
                               &#34;else\n&#34;,
                               &#34;\tNODES=3\n&#34;,
                               &#34;fi\n&#34;,
                               &#34;if [ $# -gt 2 ] ; then\n&#34;,
                               &#34;\tNUM=$3\n&#34;,
                               &#34;else\n&#34;,
                               &#34;\tNUM=&#39;all&#39;\n&#34;,
                               &#34;fi\n&#34;,
                               &#34;if [ $# -gt 3 ] ; then\n&#34;,
                               &#34;\tPROG=$4\n&#34;,
                               &#34;else\n&#34;,
                               &#34;\tPROG=$(tty)\n&#34;,
                               &#34;fi\n\n&#34;,
                               &#34;NUMPROC=`expr 28 \* $NODES`\n&#34;,
                               &#34;echo \&#34;module load midway2\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;module unload python\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;module unload openmpi\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;module load python/anaconda-2020.02\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;module load mpi4py\&#34; &gt;&gt; $PROG\n&#34;,
                               &#34;echo \&#34;date; mpiexec -n \&#34;$NUMPROC\&#34; python3 -m mpi4py.futures {}; date\&#34;  &gt;&gt; $PROG\n&#34;.format(pyfile),
                                ])

            # writing MPI run file
            with open(tmp_path+MPI_RUN_FILE, &#39;w+&#39;) as mr:
                mr.writelines([&#34;#!/bin/bash\n&#34;,
                               &#34;YEARS=\&#39;{}\&#39;\n&#34;.format(YEARS),
                               &#34;# nodes requested\n&#34;,
                               &#34;NODES={}\n&#34;.format(NODES),
                               &#34;# time requested\n&#34;,
                               &#34;T={}\n&#34;.format(T),
                               &#34;NUM=\&#39;all\&#39;\n&#34;,
                               &#34;LAUNCH=\&#39;{}\&#39;\n\n&#34;.format(MPI_LAUNCHER_FILE),
                               &#34;for yr in `echo $YEARS`\n&#34;,
                               &#34;do\n&#34;,
                               &#34;\techo $yr\n&#34;,
                               &#34;\t./{} $yr $NODES $NUM tmp_\&#34;$yr\&#34;\n&#34;.format(MPI_SETUP_FILE),
                               &#34;\t$LAUNCH -P tmp_\&#34;$yr\&#34; -F -T $T -N \&#34;$NODES\&#34; -C 28 -p broadwl -J MPI_TMP_\&#34;$yr\&#34; -M 56\n&#34;,
                               &#34;done\n&#34;,
                               &#34;rm tmp_\&#34;$yr\&#34;*\n&#34;])
            os.system(&#34;cp {} {}&#34;.format(MPI_LAUNCHER_FILE,tmp_path+&#39;mpi_launcher.sh&#39;))
        
        else:
            raise ValueError(&#34;load data first!&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="cognet.cognet.cognet.compute_DLI_samples"><code class="name flex">
<span>def <span class="ident">compute_DLI_samples</span></span>(<span>self, type, outfile, num_qsamples=40, steps=120, n_jobs=28, pole_1=0, pole_2=1, processes=6)</span>
</code></dt>
<dd>
<div class="desc"><p>compute and save ideology index or dispersion for all samples</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_qsamples</code></strong> :&ensp;<code>int</code></dt>
<dd>number of qsamples to compute</dd>
<dt><strong><code>outfile</code></strong> :&ensp;<code>str</code></dt>
<dd>output file for results</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>str</code></dt>
<dd>whether to calc dispersion or ideology</dd>
<dt><strong><code>steps</code></strong> :&ensp;<code>int</code></dt>
<dd>number of steps to qsample</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>sets the number of jobs for parallelization. Defaults to 28.</dd>
<dt><strong><code>pole_1</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>index of Pole One to calc as base distance. Defaults to 0.</dd>
<dt><strong><code>pole_2</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>index of Pole Two to calc as base distance. Defaults to 1.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>set poles if poles are not set</dd>
<dt><code>ValueError</code></dt>
<dd>load data if samples or features are not present</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>result</code></dt>
<dd>pandas.DataFrame containing multiprocessing results</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_DLI_samples(self,
                    type,
                    outfile,
                    num_qsamples=40,
                    steps=120,
                    n_jobs=28,
                    pole_1=0,
                    pole_2=1,
                    processes=6):
    &#34;&#34;&#34;compute and save ideology index or dispersion for all samples

    Args:
      num_qsamples (int): number of qsamples to compute
      outfile (str): output file for results
      type (str): whether to calc dispersion or ideology
      steps (int): number of steps to qsample
      n_jobs (int, optional): sets the number of jobs for parallelization. Defaults to 28.
      pole_1 (int, optional): index of Pole One to calc as base distance. Defaults to 0.
      pole_2 (int, optional): index of Pole Two to calc as base distance. Defaults to 1.

    Raises:
      ValueError: set poles if poles are not set
      ValueError: load data if samples or features are not present
        
    Returns:
      result: pandas.DataFrame containing multiprocessing results
    &#34;&#34;&#34;
    if all(x is not None for x in [self.samples, self.features,
                                self.pL, self.pR]):
        # init vars
        self.num_qsamples = num_qsamples
        self.steps = steps
        if pole_1 != 0 or pole_2 != 1:
            self.__calc_d0(pole_1, pole_2)
        
        if type == &#39;ideology&#39;:
            func_ = self.ideology
            cols=[&#39;ideology&#39;, &#39;dR&#39;, &#39;dL&#39;, &#39;d0&#39;]
        elif type == &#39;dispersion&#39;:
            func_ = self.dispersion
            cols=[&#39;Qsd&#39;, &#39;Qmax&#39;]
        else:
            raise ValueError(&#34;Type must be either dispersion or ideology!&#34;)
        
        result = self.mp_compute(processes,
                                 func_,
                                 cols,
                                 outfile)
    elif self.pL is None or self.pR is None:
        raise ValueError(&#34;set_poles first!&#34;)
    else:
        raise ValueError(&#34;load_data first!&#34;)
    return result</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.compute_polar_indices"><code class="name flex">
<span>def <span class="ident">compute_polar_indices</span></span>(<span>self, num_samples=None, polar_comp=False, POLEFILE=None, steps=5)</span>
</code></dt>
<dd>
<div class="desc"><p>set up polar indices for dissonance func</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>subset of samples to take</dd>
<dt><strong><code>polar_comp</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether or not to set poles</dd>
<dt><strong><code>POLEFILE</code></strong> :&ensp;<code>None</code></dt>
<dd>file containing pole samples and features</dd>
<dt><strong><code>steps</code></strong> :&ensp;<code>int</code></dt>
<dd>number of steps to qsample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_polar_indices(self,
                          num_samples=None,
                          polar_comp=False,
                          POLEFILE=None,
                          steps=5):
    &#39;&#39;&#39;set up polar indices for dissonance func

    Args:
      num_samples (int): subset of samples to take
      polar_comp (bool): whether or not to set poles
      POLEFILE (None): file containing pole samples and features
      steps (int): number of steps to qsample
    &#39;&#39;&#39;
    if all(x is not None for x in [self.samples, self.features, self.poles]):
        if num_samples is not None:
            self.set_nsamples(num_samples)

        if polar_comp:
            self.set_poles(self.qnet, steps, POLEFILE)
        
        # calculate polar indices
        polar_features = pd.concat([self.features, self.poles], axis=0)
        self.polar_indices=np.where(polar_features[self.cols].fillna(&#39;XXXX&#39;).values[0]!=&#39;XXXX&#39;)[0]
    
    elif self.poles is None:
        raise ValueError(&#34;set_poles first!&#34;)
    else:
        raise ValueError(&#34;load_data first!&#34;)</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.dispersion"><code class="name flex">
<span>def <span class="ident">dispersion</span></span>(<span>self, i, return_dict=None)</span>
</code></dt>
<dd>
<div class="desc"><p>qsamples a sample n times and takes distance matrix
to determine max and std of distances between qsamples</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>i</code></strong> :&ensp;<code>int</code></dt>
<dd>index of sample</dd>
<dt><strong><code>return_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary containing multiprocessing results</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[float]</code></dt>
<dd>std and max of the distances btwn qsamples</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dispersion(self,
               i,
               return_dict=None):
    &#34;&#34;&#34;qsamples a sample n times and takes distance matrix 
    to determine max and std of distances between qsamples

    Args:
      i (int): index of sample
      return_dict (dict): dictionary containing multiprocessing results

    Returns:
      list[float]: std and max of the distances btwn qsamples
    &#34;&#34;&#34;
    # qsample sample num_qsample times
    p = self.samples_as_strings[i]
    Qset = [qsample(p, self.qnet, self.steps) for j in np.arange(self.num_qsamples)]
    Qset = np.array(Qset)

    # calculate qdistance matrix for qsampled samples
    matrix = (qdistance_matrix(Qset, Qset, self.qnet, self.qnet))
    Q = matrix.max()
    Qsd = matrix.std()
    
    if return_dict is not None:
        return_dict[i] = [Qsd, Q]
    return [Qsd, Q]</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.dissonance"><code class="name flex">
<span>def <span class="ident">dissonance</span></span>(<span>self, sample_index=0, return_dict=None, MISSING_VAL=0.0, sample=None)</span>
</code></dt>
<dd>
<div class="desc"><p>compute dissonance for a single sample, helper function for all_dissonance</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sample_index</code></strong> :&ensp;<code>int</code></dt>
<dd>index of the sample to compute dissonance. Defaults to 0.</dd>
<dt><strong><code>return_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary containing multiprocessing results</dd>
<dt><strong><code>MISSING_VAL</code></strong> :&ensp;<code>float</code></dt>
<dd>default dissonance value</dd>
<dt><strong><code>sample</code></strong> :&ensp;<code>1D array</code></dt>
<dd>sample to compute dissonance of, instead of using sample index. Defaults to None.</dd>
</dl>
<p>Returns:
diss[self.polar_indices]: ndarray containing dissonance for sample</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dissonance(self,
                sample_index=0,
                return_dict=None,
                MISSING_VAL=0.0,
                sample=None):
    &#39;&#39;&#39;compute dissonance for a single sample, helper function for all_dissonance
    
    Args:
      sample_index (int): index of the sample to compute dissonance. Defaults to 0.
      return_dict (dict): dictionary containing multiprocessing results
      MISSING_VAL (float): default dissonance value
      sample (1D array): sample to compute dissonance of, instead of using sample index. Defaults to None.
      
    Returns: 
      diss[self.polar_indices]: ndarray containing dissonance for sample
    &#39;&#39;&#39;
    if all(x is not None for x in [self.samples, self.features]):
        if sample is None:
            s = self.samples_as_strings[sample_index]
        else:
            s = sample
        if self.polar_indices is None:
            self.polar_indices = range(len(s))

        # init vars and calculate dissonance for sample
        Ds=self.qnet.predict_distributions(s)
        diss=np.ones(len(Ds))*MISSING_VAL
        for i in self.polar_indices:
            if s[i] != &#39;&#39;:
                if s[i] in Ds[i].keys():
                    diss[i]=1-Ds[i][s[i]]/np.max(
                        list(Ds[i].values())) 
                else:
                    diss[i]=1.0
        if return_dict is not None:
            return_dict[sample_index] = diss[self.polar_indices]
        return diss[self.polar_indices]
    else:
        raise ValueError(&#34;load_data first!&#34;)</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.dissonance_matrix"><code class="name flex">
<span>def <span class="ident">dissonance_matrix</span></span>(<span>self, outfile='/example_results/DISSONANCE_matrix.csv', processes=6)</span>
</code></dt>
<dd>
<div class="desc"><p>get the dissonance for all samples</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_file</code></strong> :&ensp;<code>str</code></dt>
<dd>directory and/or file for output</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code></dt>
<dd>max number of processes. Defaults to 6.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>result</code></dt>
<dd>pandas.DataFrame containing dissonances for each sample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dissonance_matrix(self,
                    outfile=&#39;/example_results/DISSONANCE_matrix.csv&#39;,
                    processes=6):
    &#39;&#39;&#39;get the dissonance for all samples

    Args:
      output_file (str): directory and/or file for output
      processes (int): max number of processes. Defaults to 6.

    Returns:
      result: pandas.DataFrame containing dissonances for each sample
    &#39;&#39;&#39;
    # set columns
    if self.polar_indices is not None:
        polar_features = pd.concat([self.features, self.poles], axis=0)
        cols = polar_features[self.cols].dropna(axis=1).columns
    else:
        cols = self.cols
    
    result = self.mp_compute(processes,
                                self.dissonance,
                                cols,
                                outfile)
    return result</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.distance"><code class="name flex">
<span>def <span class="ident">distance</span></span>(<span>self, sample1, sample2, nsteps1=0, nsteps2=0)</span>
</code></dt>
<dd>
<div class="desc"><p>qsamples each sample set num of steps, then takes qdistance</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sample1</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>sample vector 1, must have the same num of features as the qnet</dd>
<dt><strong><code>sample2</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>sample vector 2, must have the same num of features as the qnet</dd>
<dt><strong><code>nsteps1</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of steps to qsample for sample1</dd>
<dt><strong><code>nsteps2</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of steps to qsample for sample2</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>qdistance</code></dt>
<dd>float, distance between two samples</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def distance(self,
            sample1,
            sample2,
            nsteps1=0,
            nsteps2=0):
    &#34;&#34;&#34;qsamples each sample set num of steps, then takes qdistance

    Args:
      sample1 (list[str]): sample vector 1, must have the same num of features as the qnet
      sample2 (list[str]): sample vector 2, must have the same num of features as the qnet
      nsteps1 (int, optional): number of steps to qsample for sample1
      nsteps2 (int, optional): number of steps to qsample for sample2

    Returns:
      qdistance: float, distance between two samples
    &#34;&#34;&#34;
    if self.qnet is None:
        raise ValueError(&#34;load qnet first!&#34;)
    #bp1 = self.getBaseFrequency(sample1)
    #bp2 = self.getBaseFrequency(sample2)
    # qsample samples
    sample1 = qsample(sample1, self.qnet, nsteps1)#, baseline_prob=bp1)
    sample2 = qsample(sample2, self.qnet, nsteps2)#, baseline_prob=bp2)
    return qdistance(sample1, sample2, self.qnet, self.qnet)</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.distfunc_line"><code class="name flex">
<span>def <span class="ident">distfunc_line</span></span>(<span>self, i, return_dict=None)</span>
</code></dt>
<dd>
<div class="desc"><p>compute the distance for a single sample from all other samples</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>i</code></strong> :&ensp;<code>int</code></dt>
<dd>row</dd>
<dt><strong><code>return_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary containing multiprocessing results</dd>
</dl>
<h2 id="return">Return</h2>
<p>line: float, numpy.ndarray</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def distfunc_line(self,
                i,
                return_dict=None):
    &#39;&#39;&#39;compute the distance for a single sample from all other samples

    Args:
      i (int): row
      return_dict (dict): dictionary containing multiprocessing results
    
    Return:
      line: float, numpy.ndarray
    &#39;&#39;&#39;
    if all(x is not None for x in [self.samples, self.features]):
        w = self.samples.index.size
        line = np.zeros(w)
        y = self.samples_as_strings[i]
        for j in range(w):
            # only compute half of the distance matrix
            if j &gt; i:
                x = self.samples_as_strings[j]
                line[j] = self.__distfunc(x, y)
    else:
        raise ValueError(&#34;load_data first!&#34;)
    if return_dict is not None:
        return_dict[i] = line
    return line</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.distfunc_multiples"><code class="name flex">
<span>def <span class="ident">distfunc_multiples</span></span>(<span>self, outfile, processes=6)</span>
</code></dt>
<dd>
<div class="desc"><p>compute distance matrix for all samples in the dataset</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>outfile</code></strong> :&ensp;<code>str</code></dt>
<dd>desired output filename and path</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>result</code></dt>
<dd>pandas.DataFrame containing distance matrix</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def distfunc_multiples(self,
                       outfile,
                       processes=6):
    &#34;&#34;&#34;compute distance matrix for all samples in the dataset

    Args:
      outfile (str): desired output filename and path
      
    Returns:
      result: pandas.DataFrame containing distance matrix
    &#34;&#34;&#34;
    if all(x is not None for x in [self.samples, self.features]):
        cols = [i for i in range(len(self.samples))]
        result = self.mp_compute(processes,
                                    self.distfunc_line,
                                    cols,
                                    outfile)
        # format and save resulting dict, and tranpose symmetrical distance matrix
        result = result.to_numpy()
        result = pd.DataFrame(np.maximum(result, result.transpose()))
        result.to_csv(outfile, index=None, header=None)
    else:
        raise ValueError(&#34;load data first!&#34;)
    
    return result</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.dmat_filewriter"><code class="name flex">
<span>def <span class="ident">dmat_filewriter</span></span>(<span>self, QNETPATH, mpi_path='mpi_tmp/', pyfile='cognet_qdistmatrix.py', MPI_SETUP_FILE='mpi_setup.sh', MPI_RUN_FILE='mpi_run.sh', MPI_LAUNCHER_FILE='../launcher.sh', YEARS='2016', NODES=4, T=12, num_samples=None, OUTFILE='tmp_distmatrix.csv', tmp_samplesfile='tmp_samples_as_strings.csv')</span>
</code></dt>
<dd>
<div class="desc"><p>generate files to compute qdistance matrix using mpi parallelization</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>QNETPATH</code></strong> :&ensp;<code>str</code></dt>
<dd>Qnet filepath</dd>
<dt><strong><code>pyfile</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of generated python file. Defaults to "cognet_qdistmatrix.py".</dd>
<dt><strong><code>MPI_SETUP_FILE</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of mpi setup script. Defaults to "mpi_setup.sh".</dd>
<dt><strong><code>MPI_RUN_FILE</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of mpi run script. Defaults to "mpi_run.sh".</dd>
<dt><strong><code>MPI_LAUNCHER_FILE</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Launcher script filepath. Defaults to "launcher.sh".</dd>
<dt><strong><code>YEARS</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>If looping by year, not currently implemented. Defaults to '2016'.</dd>
<dt><strong><code>NODES</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of nodes to use. Defaults to 4.</dd>
<dt><strong><code>T</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of hours to reserve nodes for. Defaults to 12.</dd>
<dt><strong><code>num_samples</code></strong> :&ensp;<code>[type]</code>, optional</dt>
<dd>How many samples to take. Defaults to None.</dd>
<dt><strong><code>OUTFILE</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>CSV File to write computed qdist matrix. Defaults to 'tmp_distmatrix.csv'.</dd>
<dt><strong><code>tmp_samplesfile</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>CSV File to write samples as strings. Defaults to "tmp_samples_as_strings.csv".</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>load data if qnet, features, or samples are not present]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dmat_filewriter(self,
                    QNETPATH,
                    mpi_path=&#34;mpi_tmp/&#34;,
                    pyfile=&#34;cognet_qdistmatrix.py&#34;,
                    MPI_SETUP_FILE=&#34;mpi_setup.sh&#34;,
                    MPI_RUN_FILE=&#34;mpi_run.sh&#34;,
                    MPI_LAUNCHER_FILE=&#34;../launcher.sh&#34;,
                    YEARS=&#39;2016&#39;,
                    NODES=4,
                    T=12,
                    num_samples=None,
                    OUTFILE=&#39;tmp_distmatrix.csv&#39;,
                    tmp_samplesfile=&#34;tmp_samples_as_strings.csv&#34;):
    &#34;&#34;&#34;generate files to compute qdistance matrix using mpi parallelization

    Args:
      QNETPATH (str): Qnet filepath
      pyfile (str, optional): Name of generated python file. Defaults to &#34;cognet_qdistmatrix.py&#34;.
      MPI_SETUP_FILE (str, optional): Name of mpi setup script. Defaults to &#34;mpi_setup.sh&#34;.
      MPI_RUN_FILE (str, optional): Name of mpi run script. Defaults to &#34;mpi_run.sh&#34;.
      MPI_LAUNCHER_FILE (str, optional): Launcher script filepath. Defaults to &#34;launcher.sh&#34;.
      YEARS (str, optional): If looping by year, not currently implemented. Defaults to &#39;2016&#39;.
      NODES (int, optional): Number of nodes to use. Defaults to 4.
      T (int, optional): Number of hours to reserve nodes for. Defaults to 12.
      num_samples ([type], optional): How many samples to take. Defaults to None.
      OUTFILE (str, optional): CSV File to write computed qdist matrix. Defaults to &#39;tmp_distmatrix.csv&#39;.
      tmp_samplesfile (str, optional): CSV File to write samples as strings. Defaults to &#34;tmp_samples_as_strings.csv&#34;.

    Raises:
        ValueError: load data if qnet, features, or samples are not present]
    &#34;&#34;&#34;
    if all(x is not None for x in [self.samples,self.features,
                                   self.qnet, self.cols]):
        if num_samples is not None:
            self.set_nsamples(num_samples)
        
        # init and make tmp dir 
        tmp_path = mpi_path
        if not os.path.exists(tmp_path):
            os.makedirs(tmp_path)
        
        pd.DataFrame(self.samples_as_strings).to_csv(tmp_path+tmp_samplesfile, header=None, index=None)
        
        w = self.samples.index.size
        
        # writing python file
        with open(tmp_path+pyfile, &#39;w+&#39;) as f:
            f.writelines([&#34;from mpi4py.futures import MPIPoolExecutor\n&#34;,
                          &#34;import numpy as np\n&#34;,
                          &#34;import pandas as pd\n&#34;,
                          &#34;from quasinet.qnet import Qnet, qdistance, load_qnet, qdistance_matrix\n&#34;,
                          &#34;from quasinet.qsampling import qsample, targeted_qsample\n\n&#34;,
                          &#34;qnet=load_qnet(\&#39;{}\&#39;)\n&#34;.format(QNETPATH)])

            f.writelines([&#34;w = {}\n&#34;.format(w),
                          &#34;h = w\n&#34;,
                          &#34;p_all = pd.read_csv(\&#34;{}\&#34;, header=None).values.astype(str)[:]\n\n&#34;.format(tmp_samplesfile)])

            f.writelines([&#34;def distfunc(x,y):\n&#34;,
                          &#34;\td=qdistance(x,y,qnet,qnet)\n&#34;,
                          &#34;\treturn d\n\n&#34;])

            f.writelines([&#34;def dfunc_line(k):\n&#34;,
                          &#34;\tline = np.zeros(w)\n&#34;,
                          &#34;\ty = p_all[k]\n&#34;,
                          &#34;\tfor j in range(w):\n&#34;,
                          &#34;\t\tif j &gt; k:\n&#34;,
                          &#34;\t\t\tx = p_all[j]\n&#34;,
                          &#34;\t\t\tline[j] = distfunc(x, y)\n&#34;,
                          &#34;\treturn line\n\n&#34;])

            f.writelines([&#34;if __name__ == &#39;__main__&#39;:\n&#34;,
                          &#34;\twith MPIPoolExecutor() as executor:\n&#34;,
                          &#34;\t\tresult = executor.map(dfunc_line, range(h))\n&#34;,
                          &#34;\tresult = pd.DataFrame(result)\n&#34;,
                              &#34;\tresult = result.to_numpy()\n&#34;,
                          &#34;\tresult = pd.DataFrame(np.maximum(result, result.transpose()))\n&#34;
                          &#34;\tresult.to_csv(\&#39;{}\&#39;,index=None,header=None)&#34;.format(OUTFILE)])
        
        # writing MPI setup file
        with open(tmp_path+MPI_SETUP_FILE, &#39;w+&#39;) as ms:
            ms.writelines([&#34;#!/bin/bash\n&#34;,
                           &#34;YEAR=$1\n\n&#34;,
                           &#34;if [ $# -gt 1 ] ; then\n&#34;,
                           &#34;\tNODES=$2\n&#34;,
                           &#34;else\n&#34;,
                           &#34;\tNODES=3\n&#34;,
                           &#34;fi\n&#34;,
                           &#34;if [ $# -gt 2 ] ; then\n&#34;,
                           &#34;\tNUM=$3\n&#34;,
                           &#34;else\n&#34;,
                           &#34;\tNUM=&#39;all&#39;\n&#34;,
                           &#34;fi\n&#34;,
                           &#34;if [ $# -gt 3 ] ; then\n&#34;,
                           &#34;\tPROG=$4\n&#34;,
                           &#34;else\n&#34;,
                           &#34;\tPROG=$(tty)\n&#34;,
                           &#34;fi\n\n&#34;,
                           &#34;NUMPROC=`expr 28 \* $NODES`\n&#34;,
                           &#34;echo \&#34;module load midway2\&#34; &gt;&gt; $PROG\n&#34;,
                           &#34;echo \&#34;module unload python\&#34; &gt;&gt; $PROG\n&#34;,
                           &#34;echo \&#34;module unload openmpi\&#34; &gt;&gt; $PROG\n&#34;,
                           &#34;echo \&#34;module load python/anaconda-2020.02\&#34; &gt;&gt; $PROG\n&#34;,
                           &#34;echo \&#34;module load mpi4py\&#34; &gt;&gt; $PROG\n&#34;,
                           &#34;echo \&#34;date; mpiexec -n \&#34;$NUMPROC\&#34; python3 -m mpi4py.futures {}; date\&#34;  &gt;&gt; $PROG\n&#34;.format(pyfile),
                            ])

        # writing MPI run file
        with open(tmp_path+MPI_RUN_FILE, &#39;w+&#39;) as mr:
            mr.writelines([&#34;#!/bin/bash\n&#34;,
                           &#34;YEARS=\&#39;{}\&#39;\n&#34;.format(YEARS),
                           &#34;# nodes requested\n&#34;,
                           &#34;NODES={}\n&#34;.format(NODES),
                           &#34;# time requested\n&#34;,
                           &#34;T={}\n&#34;.format(T),
                           &#34;NUM=\&#39;all\&#39;\n&#34;,
                           &#34;LAUNCH=\&#39;{}\&#39;\n\n&#34;.format(MPI_LAUNCHER_FILE),
                           &#34;for yr in `echo $YEARS`\n&#34;,
                           &#34;do\n&#34;,
                           &#34;\techo $yr\n&#34;,
                           &#34;\t./{} $yr $NODES $NUM tmp_\&#34;$yr\&#34;\n&#34;.format(MPI_SETUP_FILE),
                           &#34;\t$LAUNCH -P tmp_\&#34;$yr\&#34; -F -T $T -N \&#34;$NODES\&#34; -C 28 -p broadwl -J MPI_TMP_\&#34;$yr\&#34; -M 56\n&#34;,
                           &#34;done\n&#34;,
                           &#34;rm tmp_\&#34;$yr\&#34;*\n&#34;])
        os.system(&#34;cp {} {}&#34;.format(MPI_LAUNCHER_FILE,tmp_path+&#39;mpi_launcher.sh&#39;))
    
    else:
        raise ValueError(&#34;load data first!&#34;)</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.embed"><code class="name flex">
<span>def <span class="ident">embed</span></span>(<span>self, infile, name_pref, out_dir, pca_model=False, EMBED_BINARY=None)</span>
</code></dt>
<dd>
<div class="desc"><p>embed data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>infile</code></strong> :&ensp;<code>str</code></dt>
<dd>input file to be embedded</dd>
<dt><strong><code>name_pref</code></strong> :&ensp;<code>str</code></dt>
<dd>preferred name for output file</dd>
<dt><strong><code>out_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>output dir for results</dd>
<dt><strong><code>pca_model</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether or not to generate PCA model</dd>
<dt><strong><code>EMBED_BINARY</code></strong> :&ensp;<code>os.path.abspath</code></dt>
<dd>path to embed binary</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embed(self,
        infile,
        name_pref,
        out_dir,
        pca_model=False,
        EMBED_BINARY=None):
    &#39;&#39;&#39;
    embed data

    Args:
      infile (str): input file to be embedded
      name_pref (str): preferred name for output file
      out_dir (str): output dir for results
      pca_model (bool): whether or not to generate PCA model
      EMBED_BINARY (os.path.abspath): path to embed binary
    &#39;&#39;&#39;
    if all(x is not None for x in [self.year]):
        # init file names 
        yr = self.year
        PREF = name_pref
        FILE = infile
        DATAFILE = out_dir + &#39;data_&#39; +yr
        EFILE = out_dir + PREF + &#39;_E_&#39; +yr
        DFILE = out_dir + PREF + &#39;_D_&#39; +yr
        
        # set embed binary directory
        if EMBED_BINARY is None:
            EMBED = pkgutil.get_data(&#34;cognet.bin&#34;, &#34;__embed__.so&#34;) 
        else:
            EMBED = EMBED_BINARY
        
        # embed data files
        pd.read_csv(FILE, header=None).to_csv(DATAFILE,sep=&#39; &#39;,header=None,index=None)
        STR=EMBED+&#39; -f &#39;+DATAFILE+&#39; -E &#39;+EFILE+&#39; -D &#39;+DFILE
        subprocess.call(STR,shell=True)
        if pca_model:
            embed_to_pca(EFILE, EFILE+&#39;_PCA&#39;)
    elif self.year is None:
        raise ValueError(&#34;load_data first!&#34;)</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.getBaseFrequency"><code class="name flex">
<span>def <span class="ident">getBaseFrequency</span></span>(<span>self, sample)</span>
</code></dt>
<dd>
<div class="desc"><p>get frequency of the variables
helper func for qsampling</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sample</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>vector of sample, must have the same num of features as the qnet</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getBaseFrequency(self, 
                    sample):
    &#39;&#39;&#39;get frequency of the variables
    helper func for qsampling

    Args:
      sample (list[str]): vector of sample, must have the same num of features as the qnet
    &#39;&#39;&#39;
    # if variable is not mutable, set its base frequency to zero 
    MUTABLE=pd.DataFrame(np.zeros(len(self.cols)),index=self.cols).transpose()
         
    for m in self.mutable_vars:
        MUTABLE[m]=1.0
    mutable_x=MUTABLE.values[0]
    base_frequency=mutable_x/mutable_x.sum()
    
    # otherwise, set base frequency weighted by variation weight
    for i in range(len(base_frequency)):
        if base_frequency[i]&gt;0.0:
            base_frequency[i]= self.variation_weight[i]*base_frequency[i]

    return base_frequency/base_frequency.sum()</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.getMaskedSample"><code class="name flex">
<span>def <span class="ident">getMaskedSample</span></span>(<span>self, s, mask_prob=0.5, allow_all_mutable=False)</span>
</code></dt>
<dd>
<div class="desc"><p>inputs a sample and randomly mask elements of the sample</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>s</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>vector of sample, must have the same num of features as the qnet.</dd>
<dt><strong><code>mask_prob</code></strong> :&ensp;<code>float</code></dt>
<dd>float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5</dd>
<dt><strong><code>allow_all_mutable</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether or not all variables are mutable. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>s1,
base_frequency,
MASKrand,
np.where(base_frequency)[0],
np.mean(rnd_match_prob),
np.mean(max_match_prob),
random_sample</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getMaskedSample(self,
                    s,
                    mask_prob=0.5,
                    allow_all_mutable=False):
    &#39;&#39;&#39;inputs a sample and randomly mask elements of the sample

    Args:
      s (list[str]): vector of sample, must have the same num of features as the qnet.
      mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5
      allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.
      
    Returns:
      s1,
      base_frequency,
      MASKrand,
      np.where(base_frequency)[0],
      np.mean(rnd_match_prob),
      np.mean(max_match_prob),
      random_sample
    &#39;&#39;&#39;
    if self.samples is not None:
        # init random mutable variable masking
        s0=s.copy()
        s0=np.array(s0)   
        # double check, because code seems to imply that masking happens in order,
        # i.e. limited to the first 100 features, if there are only 100 mutable features
        MUTABLE=pd.DataFrame(np.zeros(len(self.cols)),index=self.cols).transpose()
        WITHVAL=[x for x in self.cols[np.where(s0)[0]] if x in self.mutable_vars ]
        MASKrand=[x for x in WITHVAL if random.random() &lt; mask_prob ]
        for m in MASKrand:
            MUTABLE[m]=1.0
        
        mutable_x=MUTABLE.values[0]
        base_frequency=mutable_x/mutable_x.sum()

        # if np.isnan(base_frequency).any():
        #     return np.nan,np.nan,np.nan
        #     return self.getMaskedSample(s)

        # mask sample according to masking (base_frequency)
        s1=s.copy()
        for i in range(len(base_frequency)):
            if base_frequency[i]&gt;0.0001:
                s1[i]=&#39;&#39;
            
        # create a random sample to test reconstruction effectiveness
        random_sample=np.copy(s)
        rnd_match_prob=[]        
        max_match_prob=[]        
        D=self.qnet.predict_distributions(s)
        for i in MASKrand:
            random_sample[np.where(
                self.cols==i)[0][0]]=self.__choose_one(
                    self.D_null[np.where(self.cols==i)[0][0]].keys())
            rnd_match_prob=np.append(rnd_match_prob,1/len(
                self.D_null[np.where(self.cols==i)[0][0]].keys()))
            max_match_prob=np.append(
                max_match_prob,np.max(
                    list(D[np.where(
                        self.cols==i)[0][0]].values())))
        
        # calculate base_frequency if all variables are mutable
        if allow_all_mutable:
            WITHVAL=[x for x in self.cols[np.where(s0)[0]]]
            MASKrand=[x for x in WITHVAL if random.random() &lt; mask_prob ]
            for m in MASKrand:
                MUTABLE[m]=1.0
            mutable_x=MUTABLE.values[0]
            base_frequency=mutable_x/mutable_x.sum()
            s1=s.copy()
            for i in range(len(base_frequency)):
                if base_frequency[i]&gt;0.0001:
                    s1[i]=&#39;&#39;

        return s1,base_frequency,MASKrand,np.where(
            base_frequency)[0],np.mean(rnd_match_prob),np.mean(max_match_prob),random_sample
    else:
        raise ValueError(&#34;load_data first!&#34;)</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.ideology"><code class="name flex">
<span>def <span class="ident">ideology</span></span>(<span>self, i, return_dict=None, pole_1=None, pole_2=None)</span>
</code></dt>
<dd>
<div class="desc"><p>return ideology index (left-leaning or right-leaning) for a singular sample</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>i</code></strong> :&ensp;<code>int</code></dt>
<dd>index of sample</dd>
<dt><strong><code>pole_1</code></strong> :&ensp;<code>int</code></dt>
<dd>index of Pole One to calc as base distance. Defaults to 0.</dd>
<dt><strong><code>pole_2</code></strong> :&ensp;<code>int</code></dt>
<dd>index of Pole Two to calc as base distance. Defaults to 1.</dd>
<dt><strong><code>return_dict</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>dict containing results</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[ideology_index, dR, dL, self.d0]</code></dt>
<dd>which way the sample leans,
distance from the right pole,
distance from the left pole,
and distance between poles, respectively</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ideology(self,
            i,
            return_dict=None,
            pole_1=None,
            pole_2=None):
    &#34;&#34;&#34;return ideology index (left-leaning or right-leaning) for a singular sample

    Args:
      i (int): index of sample
      pole_1 (int): index of Pole One to calc as base distance. Defaults to 0.
      pole_2 (int): index of Pole Two to calc as base distance. Defaults to 1.
      return_dict (dict, optional): dict containing results
      
    Returns:
      [ideology_index, dR, dL, self.d0]: which way the sample leans,
                                         distance from the right pole,
                                         distance from the left pole,
                                         and distance between poles, respectively
    &#34;&#34;&#34;
    # calculate base distance between two poles
    if pole_1 is not None or pole_2 is not None:
        self.__calc_d0(pole_1, pole_2)
    
    # calculate distances between sample and the two poles
    p = self.samples_as_strings[i]
    dR = qdistance(self.pR, p, self.qnet, self.qnet)
    dL = qdistance(self.pL, p, self.qnet, self.qnet)
    
    ideology_index = (dR-dL)/self.d0
    if return_dict is not None:
        return_dict[i] = [ideology_index, dR, dL, self.d0]
    return [ideology_index, dR, dL, self.d0]</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>self, year, features_by_year, samples, Qnet)</span>
</code></dt>
<dd>
<div class="desc"><p>load cols, features, samples, and qnet.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>year</code></strong> :&ensp;<code>str</code></dt>
<dd>to identify cols/features.</dd>
<dt><strong><code>features_by_year</code></strong> :&ensp;<code>str</code></dt>
<dd>file containing all features by year of the dataset.</dd>
<dt><strong><code>samples</code></strong> :&ensp;<code>str</code></dt>
<dd>file of samples for that year.</dd>
<dt><strong><code>Qnet</code></strong> :&ensp;<code>str</code></dt>
<dd>Qnet file location.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_data(self,
              year,
              features_by_year,
              samples,
              Qnet):
    &#39;&#39;&#39;load cols, features, samples, and qnet.

    Args:
      year (str): to identify cols/features.
      features_by_year (str): file containing all features by year of the dataset.
      samples (str): file of samples for that year.
      Qnet (str): Qnet file location.
    &#39;&#39;&#39;
    # set attributes from given files and data
    self.qnet = load_qnet(qnet)
    self.year = year
    self.cols = np.array((pd.read_csv(features_by_year,
                        keep_default_na=True, 
                        index_col=0).set_index(
                            &#39;year&#39;)).loc[int(year)].apply(
                                eval).values[0])
    self.features = pd.DataFrame(columns=self.cols)
    self.mutable_vars = [x for x in self.cols]
    #[self.cols].fillna(&#39;&#39;).values.astype(str)[:]

    # read in samples and initialize related attributes
    self.samples=pd.read_csv(samples)
    self.samples = pd.concat([self.samples,self.features], axis=0)
    self.all_samples = self.samples
    self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
    self.s_null=[&#39;&#39;]*len(self.samples_as_strings[0])
    self.D_null=self.qnet.predict_distributions(self.s_null)
    variation_weight = []
    for d in self.D_null:
        v=[]
        for val in d.values():
            v=np.append(v,val)
        variation_weight.append(entropy(v,base=len(v)))
    self.variation_weight = variation_weight</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.load_from_dataformatter"><code class="name flex">
<span>def <span class="ident">load_from_dataformatter</span></span>(<span>self, data_obj, key)</span>
</code></dt>
<dd>
<div class="desc"><p>read in either train or test data, specified by key, from data obj,
and inherit other attributes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data_obj</code></strong> :&ensp;<code>class</code></dt>
<dd>instance of dataformatter class</dd>
<dt><strong><code>key</code></strong> :&ensp;<code>str</code></dt>
<dd>'all', 'train', or 'test', corresponding to sample type</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>featurenames, samples</code></dt>
<dd>formatted arrays</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_from_dataformatter(self, 
                            data_obj,
                            key):
    &#34;&#34;&#34;read in either train or test data, specified by key, from data obj,
    and inherit other attributes.

    Args:
      data_obj (class): instance of dataformatter class
      key (str): &#39;all&#39;, &#39;train&#39;, or &#39;test&#39;, corresponding to sample type
      
    Returns:
      featurenames, samples: formatted arrays
    &#34;&#34;&#34;
    # inherit attributes from dataformatter object
    featurenames, samples = data_obj.format_samples(key)
    if any(x is not None for x in [self.features, self.samples]):
        print(&#34;replacing original features/samples with dataformatter data&#34;)
    self.cols = featurenames
    self.features = pd.DataFrame(columns=self.cols)
    self.samples = pd.DataFrame(samples,columns=self.features)
    self.all_samples = self.samples
    self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
    self.s_null=[&#39;&#39;]*len(self.samples_as_strings[0])
    return featurenames, samples</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.load_from_model"><code class="name flex">
<span>def <span class="ident">load_from_model</span></span>(<span>self, model, data_obj, key, im_vars=None, m_vars=None)</span>
</code></dt>
<dd>
<div class="desc"><p>load parameters from model object</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>Class</code></dt>
<dd>model obj for loading parameters</dd>
<dt><strong><code>data_obj</code></strong> :&ensp;<code>class</code></dt>
<dd>instance of dataformatter class</dd>
<dt><strong><code>key</code></strong> :&ensp;<code>str</code></dt>
<dd>'all', 'train', or 'test', corresponding to sample type</dd>
<dt><strong><code>im_vars</code></strong> :&ensp;<code>list[str]</code>, optional</dt>
<dd>Not implemented yet. Defaults to None.</dd>
<dt><strong><code>m_vars</code></strong> :&ensp;<code>list[str]</code>, optional</dt>
<dd>Not implemented yet. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_from_model(self,
                    model,
                    data_obj,
                    key,
                    im_vars=None,
                    m_vars=None):
    &#34;&#34;&#34;load parameters from model object

    Args:
      model (Class): model obj for loading parameters
      data_obj (class): instance of dataformatter class
      key (str): &#39;all&#39;, &#39;train&#39;, or &#39;test&#39;, corresponding to sample type
      im_vars (list[str], optional): Not implemented yet. Defaults to None.
      m_vars (list[str], optional): Not implemented yet. Defaults to None.
    &#34;&#34;&#34;
    if model is not None:
        # inherit atrributes from model object
        self.qnet = model.myQnet
        featurenames, samples = data_obj.format_samples(key)
        samples = pd.DataFrame(samples)
        self.cols = np.array(featurenames)
        self.features = pd.DataFrame(columns=np.array(featurenames))
        
        # inherit mutable and immutable variables from model obj
        if any(x is not None for x in [model.immutable_vars, model.mutable_vars]):
            if model.immutable_vars is not None:
                self.immutable_vars = model.immutable_vars
                self.mutable_vars = [x for x in self.features if x not in self.immutable_vars]
            elif model.mutable_vars is not None:
                self.mutable_vars = model.mutable_vars
                self.immutable_vars = [x for x in self.features if x not in self.mutable_vars]
        else:
            self.mutable_vars = self.features
        
        # inherit and set class attributes.
        self.samples = pd.DataFrame(samples).replace(&#34;nan&#34;,&#34;&#34;).fillna(&#34;&#34;)
        self.samples.columns = np.array(featurenames)
        self.all_samples = self.samples
        self.samples_as_strings = self.samples.fillna(&#39;&#39;).values.astype(str)[:]
        self.s_null=[&#39;&#39;]*len(self.samples_as_strings[0])
        self.D_null=self.qnet.predict_distributions(self.s_null)
        variation_weight = []
        for d in self.D_null:
            v=[]
            for val in d.values():
                v=np.append(v,val)
            variation_weight.append(entropy(v,base=len(v)))
        variation_weight = np.nan_to_num(variation_weight) # remove nans
        self.variation_weight = variation_weight</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.mp_compute"><code class="name flex">
<span>def <span class="ident">mp_compute</span></span>(<span>self, processes, func, cols, outfile, args=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Compute desired function through multiprocessing and save result to csv.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code></dt>
<dd>number of processes to use.</dd>
<dt><strong><code>func</code></strong> :&ensp;<code>func</code></dt>
<dd>function to compute using multiprocessing</dd>
<dt><strong><code>cols</code></strong> :&ensp;<code>list</code></dt>
<dd>column names of resulting csv</dd>
<dt>outfile (str)): filepath + filename for resulting csv</dt>
<dt><strong><code>args</code></strong> :&ensp;<code>list</code></dt>
<dd>list containing arguments for desired function. Defaults to empty list.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mp_compute(self, 
               processes,
               func, 
               cols,
               outfile, 
               args=[]):
    &#34;&#34;&#34;
    Compute desired function through multiprocessing and save result to csv.

    Args:
      processes (int): number of processes to use.
      func (func): function to compute using multiprocessing
      cols (list): column names of resulting csv
      outfile (str)): filepath + filename for resulting csv
      args (list): list containing arguments for desired function. Defaults to empty list.
    &#34;&#34;&#34;

    # init mp.Manager and result dict
    manager = mp.Manager()
    return_dict = manager.dict()

    # set processes as given, unless class parameter is set
    max_processes = processes
    if self.MAX_PROCESSES != 0:
        max_processes = self.MAX_PROCESSES
        print(&#34;Number of Processes {} has been set using class parameter&#34;.format(self.MAX_PROCESSES))
    num_processes = 0
    process_list = []
    
    # init mp.Processes for each individual sample
    # run once collected processes hit max
    for i in range(len(self.samples)):
        params = tuple([i, return_dict] + args)
        num_processes += 1
        p = mp.Process(target=func,
                    args=params)
        process_list.append(p)
        if num_processes == max_processes:
            [x.start() for x in process_list]
            [x.join() for x in process_list]
            process_list = []
            num_processes = 0
            
    # compute remaining processes
    if num_processes != 0:
        [x.start() for x in process_list]
        [x.join() for x in process_list]
        process_list = []
        num_processes = 0
    
    # format and save resulting dict
    result = pd.DataFrame(return_dict.values(), columns=cols, index=return_dict.keys()).sort_index()
    result.to_csv(outfile, index=None)
    return result</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.polarDistance"><code class="name flex">
<span>def <span class="ident">polarDistance</span></span>(<span>self, i, return_dict=None)</span>
</code></dt>
<dd>
<div class="desc"><p>return the distances from a single sample to the poles</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>i</code></strong> :&ensp;<code>int</code></dt>
<dd>index of sample to take</dd>
<dt><strong><code>return_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary containing multiprocessing results</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>distances</code></dt>
<dd>float, distance from sample to each pole</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def polarDistance(self,
                i,
                return_dict=None):
    &#34;&#34;&#34;return the distances from a single sample to the poles

    Args:
      i (int): index of sample to take
      return_dict (dict): dictionary containing multiprocessing results

    Returns:
      distances: float, distance from sample to each pole
    &#34;&#34;&#34;
    p = self.samples_as_strings[i]
    distances = []
    # calculate from each pole to the sample, and append to array
    for index, row in self.polar_features[self.cols].iterrows():
        row = row.fillna(&#39;&#39;).values.astype(str)[:]
        distances.append(self.distance(p, np.array(row)))
    if return_dict is not None:
        return_dict[i] = distances
    return distances</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.polarDistance_multiple"><code class="name flex">
<span>def <span class="ident">polarDistance_multiple</span></span>(<span>self, outfile, processes=6)</span>
</code></dt>
<dd>
<div class="desc"><p>return the distance from all samples to the poles</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>outfile</code></strong> :&ensp;<code>str</code></dt>
<dd>desired output filename and path</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>result</code></dt>
<dd>pandas.DataFrame containing polar distance results</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def polarDistance_multiple(self,
                           outfile,
                           processes=6):
    &#34;&#34;&#34;return the distance from all samples to the poles

    Args:
      outfile (str): desired output filename and path
      
    Returns:
      result: pandas.DataFrame containing polar distance results
    &#34;&#34;&#34;
    if all(x is not None for x in [self.samples, self.cols,
                                self.polar_features]):
        # get the column names
        pole_names = []
        for index, row in self.polar_features[self.cols].iterrows():
            pole_names.append(index)
        result = self.mp_compute(processes,
                                    self.polarDistance,
                                    pole_names,
                                    outfile)
    else:
        raise ValueError(&#34;load data first!&#34;)
    return result</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.polar_separation"><code class="name flex">
<span>def <span class="ident">polar_separation</span></span>(<span>self, nsteps=0)</span>
</code></dt>
<dd>
<div class="desc"><p>calculates the distance between poles as a qdistance matrix</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nsteps</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self.polar_matrix</code></dt>
<dd>dictionary containing multiprocessing results</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def polar_separation(self,
                    nsteps=0):
    &#34;&#34;&#34;calculates the distance between poles as a qdistance matrix

    Args:
      nsteps (int, optional): [description]. Defaults to 0.
      
    Returns:
      self.polar_matrix: dictionary containing multiprocessing results
    &#34;&#34;&#34;
    # vectorize and qsample poles
    polar_arraydata = self.polar_features[self.cols].values.astype(str)[:]
    samples_ = []
    for vector in polar_arraydata:
        bp = self.getBaseFrequency(vector)
        sample = qsample(vector, self.qnet, nsteps, baseline_prob=bp)
        samples_.append(sample)
    samples_ = np.array(samples_)
    # calculate distance matrix for poles
    self.polar_matrix = qdistance_matrix(samples_, samples_, self.qnet, self.qnet)
    return self.polar_matrix</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.qsampling"><code class="name flex">
<span>def <span class="ident">qsampling</span></span>(<span>self, sample, steps, immutable=False)</span>
</code></dt>
<dd>
<div class="desc"><p>perturb the sample based on thet qnet distributions and number of steps</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sample</code></strong> :&ensp;<code>1d array-like</code></dt>
<dd>sample vector, must have the same num of features as the qnet</dd>
<dt><strong><code>steps</code></strong> :&ensp;<code>int</code></dt>
<dd>number of steps to qsample</dd>
<dt><strong><code>immutable</code></strong> :&ensp;<code>bool</code></dt>
<dd>are there variables that are immutable?</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def qsampling(self,
            sample,
            steps,
            immutable=False):
    &#39;&#39;&#39;perturb the sample based on thet qnet distributions and number of steps

    Args:
      sample (1d array-like): sample vector, must have the same num of features as the qnet
      steps (int): number of steps to qsample
      immutable (bool): are there variables that are immutable?
    &#39;&#39;&#39;
    # immutable, check that mutable variables have been initialized
    if immutable == True:
        if all(x is not None for x in [self.mutable_vars, sample]):
            return qsample(sample,self.qnet,steps,self.getBaseFrequency(self.samples))
        elif self.mutable_vars is None:
            raise ValueError(&#34;set mutable and immutable variables first!&#34;)
    else:
        return qsample(sample,self.qnet,steps)</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.randomMaskReconstruction"><code class="name flex">
<span>def <span class="ident">randomMaskReconstruction</span></span>(<span>self, index=None, return_dict=None, sample=None, index_colname='feature_names', output_dir='recon_results/', file_name='recon_tmp.csv', mask_prob=0.5, allow_all_mutable=False, save_samples=False, save_output=True)</span>
</code></dt>
<dd>
<div class="desc"><p>reconstruct the masked sample by qsampling and comparing to original
set self.mask_prob and self.steps if needed</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>index</code></strong> :&ensp;<code>int</code></dt>
<dd>index of sample to take.</dd>
<dt><strong><code>return_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary containing multiprocessing results. Defaults to None.</dd>
<dt><strong><code>sample</code></strong> :&ensp;<code>list[str]</code>, optional</dt>
<dd>sample vector, must have the same num of features as the qnet. Defaults to None.</dd>
<dt><strong><code>index_colname</code></strong> :&ensp;<code>str</code></dt>
<dd>column name for index. Defaults to "feature_names"</dd>
<dt><strong><code>output_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>directory name for output files. Defaults to "recon_results/".</dd>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>base file name for output files Defaults to "recon_tmp.csv".</dd>
<dt><strong><code>mask_prob</code></strong> :&ensp;<code>float</code></dt>
<dd>float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5</dd>
<dt><strong><code>allow_all_mutable</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether or not all variables are mutable. Defaults to False.</dd>
<dt><strong><code>save_samples</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to include sample vectors in the savefile. Defaults to False.</dd>
<dt><strong><code>save_output</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether or not to save output df to file. Defaults to True.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>Neither sample or index were given</dd>
<dt><code>ValueError</code></dt>
<dd>Both sample and index were given</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>return_values:(1 - (dqestim/dactual))*100,
rmatch_u,
rmatch,
s,
qs,
random_sample,
mask_</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def randomMaskReconstruction(self,
                            index=None,
                            return_dict=None,
                            sample=None,
                            index_colname=&#34;feature_names&#34;,
                            output_dir=&#34;recon_results/&#34;,
                            file_name=&#34;recon_tmp.csv&#34;,
                            mask_prob=0.5,
                            allow_all_mutable=False,
                            save_samples=False,
                            save_output=True):
    &#34;&#34;&#34;reconstruct the masked sample by qsampling and comparing to original
    set self.mask_prob and self.steps if needed

    Args:
      index (int): index of sample to take.
      return_dict (dict): dictionary containing multiprocessing results. Defaults to None.
      sample (list[str], optional): sample vector, must have the same num of features as the qnet. Defaults to None.
      index_colname (str): column name for index. Defaults to &#34;feature_names&#34;
      output_dir (str): directory name for output files. Defaults to &#34;recon_results/&#34;.
      file_name (str): base file name for output files Defaults to &#34;recon_tmp.csv&#34;.
      mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5
      allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.
      save_samples (bool): whether to include sample vectors in the savefile. Defaults to False.
      save_output (bool): whether or not to save output df to file. Defaults to True.

    Raises:
      ValueError: Neither sample or index were given
      ValueError: Both sample and index were given
      
    Returns:
      return_values:(1 - (dqestim/dactual))*100,
                        rmatch_u,
                        rmatch,
                        s,
                        qs,
                        random_sample,
                        mask_
    &#34;&#34;&#34;
    if all(x is None for x in [sample, index]):
        raise ValueError(&#34;Must input either sample or index!&#34;)
    elif all(x is not None for x in [sample, index]):
        raise ValueError(&#34;Must input either sample or index not both!&#34;)
    elif sample is not None:
        s=sample#np.array(pd.DataFrame(sample).fillna(&#39;&#39;).values.astype(str)[:])
    elif index is not None:
        s=self.samples_as_strings[index]
    
    # calculate masked sample and get variables
    s1,bp,mask_,maskindex,rmatch_u,rmatch,random_sample=self.getMaskedSample(s, 
                                                                    mask_prob=mask_prob,
                                                                    allow_all_mutable=allow_all_mutable)
    # if base_frequency is nan, set return_dict to nans
    if np.isnan(bp).any():
        return_dict[index] = np.nan,np.nan,np.nan
        return np.nan,np.nan,np.nan
    
    # make directories
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)

    # qsample sample and calculate distances between original and qsampled 
    qs=qsample(s1,self.qnet,self.steps,bp)
    dqestim=qdistance(s,qs,self.qnet,self.qnet)
    dactual=qdistance(s,s1,self.qnet,self.qnet)
    
    # format and save sample, qsample statistics and values
    cmpf=pd.DataFrame([s,qs,random_sample],
                      columns=self.cols,
                      index=[&#39;sample&#39;,&#39;qsampled&#39;,&#39;random_sample&#39;])[mask_].transpose()
    cmpf.index.name= index_colname
    if save_output:
        file_name = file_name.replace(&#34;tmp&#34;, str(index))
        cmpf.to_csv(output_dir+file_name)
        
    if save_samples:
        return_values= (1 - (dqestim/dactual))*100,rmatch_u,rmatch,s,qs,random_sample,mask_
    else:
        return_values = (1 - (dqestim/dactual))*100,rmatch_u,rmatch,mask_
    
    if return_dict is not None:
        return_dict[index] = return_values
        return return_dict[index]
    return return_values</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.randomMaskReconstruction_multiple"><code class="name flex">
<span>def <span class="ident">randomMaskReconstruction_multiple</span></span>(<span>self, outfile, processes=6, save_samples=False, index_colname='feature_names', output_dir='recon_results/', file_name='recon_tmp.csv', mask_prob=0.5, allow_all_mutable=False)</span>
</code></dt>
<dd>
<div class="desc"><p>runs and saves the results of the predicted masked sample</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_file</code></strong> :&ensp;<code>str</code></dt>
<dd>directory and/or file for output.</dd>
<dt><strong><code>processes</code></strong> :&ensp;<code>int</code></dt>
<dd>max number of processes. Defaults to 6.</dd>
<dt><strong><code>save_samples</code></strong> :&ensp;<code>boolean</code></dt>
<dd>whether or not to save the generated qsamples, random samples, etc. Defaults to False.</dd>
<dt>index_colname="feature_names",</dt>
<dt>output_dir="recon_results/",</dt>
<dt>file_name="recon_tmp.csv",</dt>
<dt><strong><code>mask_prob</code></strong> :&ensp;<code>float</code></dt>
<dd>float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5</dd>
<dt><strong><code>allow_all_mutable</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether or not all variables are mutable. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>result</code></dt>
<dd>pandas.DataFrame containing masking and reconstruction results.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def randomMaskReconstruction_multiple(self,
                                      outfile,
                                      processes=6,
                                      save_samples=False,
                                      index_colname=&#34;feature_names&#34;,
                                      output_dir=&#34;recon_results/&#34;,
                                      file_name=&#34;recon_tmp.csv&#34;,
                                      mask_prob=0.5,
                                      allow_all_mutable=False):
    &#39;&#39;&#39;runs and saves the results of the predicted masked sample

    Args:
      output_file (str): directory and/or file for output.
      processes (int): max number of processes. Defaults to 6.
      save_samples (boolean): whether or not to save the generated qsamples, random samples, etc. Defaults to False.
      index_colname=&#34;feature_names&#34;,
      output_dir=&#34;recon_results/&#34;,
      file_name=&#34;recon_tmp.csv&#34;,
      mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5
      allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.
      
    Returns:
      result: pandas.DataFrame containing masking and reconstruction results.
    &#39;&#39;&#39;
    # set columns for mp_compute
    if save_samples:
        cols = [&#39;rederr&#39;,&#39;r_prob&#39;,&#39;rand_err&#39;,&#39;sample&#39;,&#39;qsampled&#39;,&#39;random_sample&#39;,&#39;mask_&#39;]
    else:
        cols = [&#39;rederr&#39;,&#39;r_prob&#39;,&#39;rand_err&#39;,&#39;mask_&#39;]
    
    # 
    args=[None, index_colname, output_dir,
          file_name, mask_prob, allow_all_mutable]
    
    result = self.mp_compute(processes,
                                self.randomMaskReconstruction,
                                cols,
                                outfile,
                                args=args)
    return result</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.random_sample"><code class="name flex">
<span>def <span class="ident">random_sample</span></span>(<span>self, df=None, n=1)</span>
</code></dt>
<dd>
<div class="desc"><p>compute a random sample from the underlying distributions of the dataset, by column.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>Desired data to take random sample of. Defaults to None, in which case qnet samples are used.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>number of random samples to take. Defaults to 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>return_df (pd.DataFrame): Random sample drawn from underlying distribution of each column.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_sample(self,
                  df=None,
                  n=1):
    &#39;&#39;&#39;compute a random sample from the underlying distributions of the dataset, by column.
    
    
    Args:
      df (pandas.DataFrame): Desired data to take random sample of. Defaults to None, in which case qnet samples are used.
      n (int): number of random samples to take. Defaults to 1.
      
    Returns:
      return_df (pd.DataFrame): Random sample drawn from underlying distribution of each column.
    &#39;&#39;&#39;
    # check if a new dataset was inputted
    if df is None:
        samples_ = self.samples
    else:
        samples_ = df

    # take random sample from each of the columns based on their distribution
    return_df = pd.DataFrame()
    for col in samples_.columns:
        return_df[col] = samples_[col].sample(n=n, replace=True).values
        
    return return_df</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.set_immutable_vars"><code class="name flex">
<span>def <span class="ident">set_immutable_vars</span></span>(<span>self, IMMUTABLE_FILE)</span>
</code></dt>
<dd>
<div class="desc"><p>set vars to immutable and mutable,
can prob combine this with the load_data func: only set the immutable vars if necessary</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>IMMUTABLE_FILE</code></strong> :&ensp;<code>str</code></dt>
<dd>file containing the immutable features/vars</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_immutable_vars(self,
                    IMMUTABLE_FILE):
    &#39;&#39;&#39;set vars to immutable and mutable, 
    can prob combine this with the load_data func: only set the immutable vars if necessary

    Args:
      IMMUTABLE_FILE (str): file containing the immutable features/vars
    &#39;&#39;&#39;
    # set mutable and immutable variable attributes 
    if self.cols is None:
        raise ValueError(&#34;load_data first!&#34;)
    self.immutable_vars = pd.read_csv(IMMUTABLE_FILE,index_col=0).transpose()
    self.mutable_vars = None
    self.mutable_vars = [x for x in self.cols
                        if x.upper() not in self.immutable_vars.columns]</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.set_nsamples"><code class="name flex">
<span>def <span class="ident">set_nsamples</span></span>(<span>self, num_samples, random=False)</span>
</code></dt>
<dd>
<div class="desc"><p>select a subset of the samples</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>Set num of samples to subset, default to None, resets to all samples</dd>
<dt><strong><code>random</code></strong> :&ensp;<code>bool</code></dt>
<dd>take random sample if true, ordered sample if false</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_nsamples(self,
                num_samples,
                random=False):
    &#39;&#39;&#39;select a subset of the samples

    Args:
      num_samples (int): Set num of samples to subset, default to None, resets to all samples
      random (bool): take random sample if true, ordered sample if false
    &#39;&#39;&#39;
    # each time function is called, reset samples to use_all_samples
    # this allows us to call nsamples numerous times 
    self.samples = self.all_samples
    if self.samples is not None:
        # if a greater number of sample is selected than available, raise error
        if all(x is not None for x in [num_samples, self.samples]):
            if num_samples &gt; len(self.samples.index):
                string = &#39;The number of selected samples ({}) &#39; + \
                    &#39;is greater than the number of samples ({})!&#39;
                string = string.format(num_samples, len(self.samples.index))
                raise ValueError(string)

            # if the same number of samples is selected as available, print warning
            if num_samples == len(self.samples.index):
                string = &#39;The number of selected samples ({}) &#39; + \
                    &#39;is equal to the number of samples ({})!&#39;
                string = string.format(num_samples, len(self.samples.index))
                print(string)
                
            # if random is true, return random sample, otherwise return an ordered slice
            if random:
                self.samples = self.samples.sample(num_samples)
            else:
                self.samples = self.samples.iloc[:num_samples]
            self.nsamples = num_samples
            self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
            
        elif self.samples is None:
            raise ValueError(&#34;load_data first!&#34;)</code></pre>
</details>
</dd>
<dt id="cognet.cognet.cognet.set_poles"><code class="name flex">
<span>def <span class="ident">set_poles</span></span>(<span>self, POLEFILE, pole_1, pole_2, steps=0, mutable=False, VERBOSE=False, restrict=True, nsamples=None, random=False)</span>
</code></dt>
<dd>
<div class="desc"><p>set the poles and samples such that the samples contain features in poles</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>steps</code></strong> :&ensp;<code>int</code></dt>
<dd>number of steps to qsample</dd>
<dt><strong><code>POLEFILE</code></strong> :&ensp;<code>str</code></dt>
<dd>file containing poles samples and features</dd>
<dt><strong><code>pole_1</code></strong> :&ensp;<code>str</code></dt>
<dd>column name for first pole</dd>
<dt><strong><code>pole_2</code></strong> :&ensp;<code>str</code></dt>
<dd>column name for second pole</dd>
<dt><strong><code>mutable</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether or not to set poles as the only mutable_vars</dd>
<dt><strong><code>VERBOSE</code></strong> :&ensp;<code>bool</code></dt>
<dd>boolean flag prints number of pole features not found in sample features if True</dd>
<dt><strong><code>restrict</code></strong> :&ensp;<code>bool</code></dt>
<dd>boolean flag restricts the sample features to polar features if True</dd>
<dt><strong><code>random</code></strong> :&ensp;<code>bool</code></dt>
<dd>boolean flag takes random sample of all_samples</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_poles(self,
              POLEFILE,
              pole_1,
              pole_2,
              steps=0,
              mutable=False,
              VERBOSE=False,
              restrict=True,
              nsamples = None,
              random=False):
    &#39;&#39;&#39;set the poles and samples such that the samples contain features in poles

    Args:
      steps (int): number of steps to qsample
      POLEFILE (str): file containing poles samples and features
      pole_1 (str): column name for first pole
      pole_2 (str): column name for second pole
      mutable (bool): Whether or not to set poles as the only mutable_vars
      VERBOSE (bool): boolean flag prints number of pole features not found in sample features if True
      restrict (bool): boolean flag restricts the sample features to polar features if True
      random (bool): boolean flag takes random sample of all_samples
    &#39;&#39;&#39;
    invalid_count = 0
    if all(x is not None for x in [self.samples, self.qnet]):
        # read and set poles
        poles = pd.read_csv(POLEFILE, index_col=0)
        self.poles=poles.transpose()
        self.polar_features = pd.concat([self.features, self.poles], axis=0).fillna(&#39;&#39;)
        poles_dict = {}
        for column in poles:
            p_ = self.polar_features.loc[column][self.cols].fillna(&#39;&#39;).values.astype(str)[:]
            # qsample poles to qnet
            poles_dict[column] = self.qsampling(p_,steps)
        self.poles_dict = poles_dict
        self.pL = self.poles_dict[pole_1]
        self.pR = self.poles_dict[pole_2]
        self.d0 = qdistance(self.pL, self.pR, self.qnet, self.qnet)
        
        # restrict sample columns to polar columns
        if restrict:
            cols = [x for x in self.poles.columns if x in self.samples.columns]
            self.samples=self.samples[cols]
            self.restricted = True
            self.samples = pd.concat([self.features,self.samples], axis=0).replace(&#34;nan&#34;,&#34;&#34;).fillna(&#39;&#39;)
            self.samples_as_strings = self.samples[self.cols].fillna(&#39;&#39;).values.astype(str)[:]
            
        # if restrict==False, unrestrict it and set original
        else:
            self.restricted = False
            self.samples = self.all_samples
            if self.nsamples is not None:
                self.set_nsamples(nsamples, random)
        
        # identify pole features that were excluded due to sample features restriction
        if VERBOSE:
            for x in self.poles.columns:
                if x not in self.samples.columns:
                    invalid_count += 1
                    #self.samples[x]=&#39;&#39;
        
        if mutable:
            self.mutable_vars=[x for x in self.cols if x in self.poles.columns]
    elif self.samples is None:
        raise ValueError(&#34;load_data first!&#34;)

    if VERBOSE:
        print(&#34;{} pole features not found in sample features&#34;.format(invalid_count))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<img src="logozed_nowhite.png" alt="drawing" style="width:400px;"/>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="cognet" href="index.html">cognet</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="cognet.cognet.cognet" href="#cognet.cognet.cognet">cognet</a></code></h4>
<ul class="">
<li><code><a title="cognet.cognet.cognet.compute_DLI_samples" href="#cognet.cognet.cognet.compute_DLI_samples">compute_DLI_samples</a></code></li>
<li><code><a title="cognet.cognet.cognet.compute_polar_indices" href="#cognet.cognet.cognet.compute_polar_indices">compute_polar_indices</a></code></li>
<li><code><a title="cognet.cognet.cognet.dispersion" href="#cognet.cognet.cognet.dispersion">dispersion</a></code></li>
<li><code><a title="cognet.cognet.cognet.dissonance" href="#cognet.cognet.cognet.dissonance">dissonance</a></code></li>
<li><code><a title="cognet.cognet.cognet.dissonance_matrix" href="#cognet.cognet.cognet.dissonance_matrix">dissonance_matrix</a></code></li>
<li><code><a title="cognet.cognet.cognet.distance" href="#cognet.cognet.cognet.distance">distance</a></code></li>
<li><code><a title="cognet.cognet.cognet.distfunc_line" href="#cognet.cognet.cognet.distfunc_line">distfunc_line</a></code></li>
<li><code><a title="cognet.cognet.cognet.distfunc_multiples" href="#cognet.cognet.cognet.distfunc_multiples">distfunc_multiples</a></code></li>
<li><code><a title="cognet.cognet.cognet.dmat_filewriter" href="#cognet.cognet.cognet.dmat_filewriter">dmat_filewriter</a></code></li>
<li><code><a title="cognet.cognet.cognet.embed" href="#cognet.cognet.cognet.embed">embed</a></code></li>
<li><code><a title="cognet.cognet.cognet.getBaseFrequency" href="#cognet.cognet.cognet.getBaseFrequency">getBaseFrequency</a></code></li>
<li><code><a title="cognet.cognet.cognet.getMaskedSample" href="#cognet.cognet.cognet.getMaskedSample">getMaskedSample</a></code></li>
<li><code><a title="cognet.cognet.cognet.ideology" href="#cognet.cognet.cognet.ideology">ideology</a></code></li>
<li><code><a title="cognet.cognet.cognet.load_data" href="#cognet.cognet.cognet.load_data">load_data</a></code></li>
<li><code><a title="cognet.cognet.cognet.load_from_dataformatter" href="#cognet.cognet.cognet.load_from_dataformatter">load_from_dataformatter</a></code></li>
<li><code><a title="cognet.cognet.cognet.load_from_model" href="#cognet.cognet.cognet.load_from_model">load_from_model</a></code></li>
<li><code><a title="cognet.cognet.cognet.mp_compute" href="#cognet.cognet.cognet.mp_compute">mp_compute</a></code></li>
<li><code><a title="cognet.cognet.cognet.polarDistance" href="#cognet.cognet.cognet.polarDistance">polarDistance</a></code></li>
<li><code><a title="cognet.cognet.cognet.polarDistance_multiple" href="#cognet.cognet.cognet.polarDistance_multiple">polarDistance_multiple</a></code></li>
<li><code><a title="cognet.cognet.cognet.polar_separation" href="#cognet.cognet.cognet.polar_separation">polar_separation</a></code></li>
<li><code><a title="cognet.cognet.cognet.qsampling" href="#cognet.cognet.cognet.qsampling">qsampling</a></code></li>
<li><code><a title="cognet.cognet.cognet.randomMaskReconstruction" href="#cognet.cognet.cognet.randomMaskReconstruction">randomMaskReconstruction</a></code></li>
<li><code><a title="cognet.cognet.cognet.randomMaskReconstruction_multiple" href="#cognet.cognet.cognet.randomMaskReconstruction_multiple">randomMaskReconstruction_multiple</a></code></li>
<li><code><a title="cognet.cognet.cognet.random_sample" href="#cognet.cognet.cognet.random_sample">random_sample</a></code></li>
<li><code><a title="cognet.cognet.cognet.set_immutable_vars" href="#cognet.cognet.cognet.set_immutable_vars">set_immutable_vars</a></code></li>
<li><code><a title="cognet.cognet.cognet.set_nsamples" href="#cognet.cognet.cognet.set_nsamples">set_nsamples</a></code></li>
<li><code><a title="cognet.cognet.cognet.set_poles" href="#cognet.cognet.cognet.set_poles">set_poles</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
Author: Lynn Zheng, Jin Li and Ishanu Chattopadhyay <a href="https://zed.uchicago.edu"> Zero Knowledge Discovery, University of Chicago</a>. Email: ishanu@uchicago.edu
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>
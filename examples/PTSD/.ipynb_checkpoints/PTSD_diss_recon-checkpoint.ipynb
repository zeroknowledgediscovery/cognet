{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0b54cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from quasinet.qnet import qdistance, save_qnet\n",
    "from cognet.cognet import cognet as cg\n",
    "from cognet.dataFormatter import dataFormatter\n",
    "from cognet.model import model \n",
    "#import cognet.util\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d585f7be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## read in data, configure to Qnet specifications and fit Qnet model\n",
    "data_obj=dataFormatter(samples=\"data/PTSD_cognet_test_processed.csv\")\n",
    "features,samples = data_obj.Qnet_formatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a7a27b-194e-44fd-bc6d-30eda87ec0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = False\n",
    "model_obj = model()\n",
    "if fit:\n",
    "    model_obj.fit(data_obj=data_obj, njobs=2)\n",
    "    # model_obj.save(\"examples_results/PTSD_cognet_test.joblib\")\n",
    "    save_qnet(model_obj.myQnet, \"results/PTSD_cognet_test.joblib\", low_mem=False)\n",
    "else:\n",
    "    model_obj.load(\"results/PTSD_cognet_test.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece14390-1eda-4f18-8ddb-e6c63d3846a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from quasinet.qnet import Qnet, qdistance, load_qnet, qdistance_matrix\n",
    "from quasinet.qsampling import qsample, targeted_qsample\n",
    "#from mpi4py.futures import MPIPoolExecutor\n",
    "import sys\n",
    "import subprocess\n",
    "from scipy.stats import entropy\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from cognet.util import embed_to_pca\n",
    "import pkgutil\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class cognet:\n",
    "    \"\"\"Aggregate related Qnet functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init\n",
    "        \"\"\"\n",
    "        self.year = None\n",
    "        self.n_jobs = 28\n",
    "        self.qnet = None\n",
    "        self.steps = 120\n",
    "        self.num_qsamples = None\n",
    "        self.all_samples = None\n",
    "        self.samples = None\n",
    "        self.samples_as_strings = None\n",
    "        self.features = None\n",
    "        self.cols = None\n",
    "        self.immutable_vars = None\n",
    "        self.mutable_vars = None\n",
    "        self.poles = None\n",
    "        self.polar_features = None\n",
    "        self.polar_indices = None\n",
    "        self.poles_dict = {}\n",
    "        self.d0 = None\n",
    "        self.s_null = None\n",
    "        self.D_null = None\n",
    "        self.mask_prob = 0.5\n",
    "        self.variation_weight = None\n",
    "        self.polar_matrix = None\n",
    "        self.nsamples = None\n",
    "        self.restricted = False\n",
    "        self.MAX_PROCESSES = 0\n",
    "    \n",
    "    def load_from_model(self,\n",
    "                        model,\n",
    "                        data_obj,\n",
    "                        key,\n",
    "                        im_vars=None,\n",
    "                        m_vars=None):\n",
    "        \"\"\"load parameters from model object\n",
    "\n",
    "        Args:\n",
    "          model (Class): model obj for loading parameters\n",
    "          data_obj (class): instance of dataformatter class\n",
    "          key (str): 'all', 'train', or 'test', corresponding to sample type\n",
    "          im_vars (list[str], optional): Not implemented yet. Defaults to None.\n",
    "          m_vars (list[str], optional): Not implemented yet. Defaults to None.\n",
    "        \"\"\"\n",
    "        if model is not None:\n",
    "            # inherit atrributes from model object\n",
    "            self.qnet = model.myQnet\n",
    "            featurenames, samples = data_obj.format_samples(key)\n",
    "            samples = pd.DataFrame(samples)\n",
    "            self.cols = np.array(featurenames)\n",
    "            self.features = pd.DataFrame(columns=np.array(featurenames))\n",
    "            \n",
    "            # inherit mutable and immutable variables from model obj\n",
    "            if any(x is not None for x in [model.immutable_vars, model.mutable_vars]):\n",
    "                if model.immutable_vars is not None:\n",
    "                    self.immutable_vars = model.immutable_vars\n",
    "                    self.mutable_vars = [x for x in self.features if x not in self.immutable_vars]\n",
    "                elif model.mutable_vars is not None:\n",
    "                    self.mutable_vars = model.mutable_vars\n",
    "                    self.immutable_vars = [x for x in self.features if x not in self.mutable_vars]\n",
    "            else:\n",
    "                self.mutable_vars = self.features\n",
    "            \n",
    "            # inherit and set class attributes.\n",
    "            self.samples = pd.DataFrame(samples).replace(\"nan\",\"\").fillna(\"\")\n",
    "            self.samples.columns = np.array(featurenames)\n",
    "            self.all_samples = self.samples\n",
    "            self.samples_as_strings = self.samples.fillna('').values.astype(str)[:]\n",
    "            self.s_null=['']*len(self.samples_as_strings[0])\n",
    "            self.D_null=self.qnet.predict_distributions(self.s_null)\n",
    "            variation_weight = []\n",
    "            for d in self.D_null:\n",
    "                v=[]\n",
    "                for val in d.values():\n",
    "                    v=np.append(v,val)\n",
    "                variation_weight.append(entropy(v,base=len(v)))\n",
    "            variation_weight = np.nan_to_num(variation_weight) # remove nans\n",
    "            self.variation_weight = variation_weight\n",
    "    \n",
    "    def load_from_dataformatter(self, \n",
    "                                data_obj,\n",
    "                                key):\n",
    "        \"\"\"read in either train or test data, specified by key, from data obj,\n",
    "        and inherit other attributes.\n",
    "\n",
    "        Args:\n",
    "          data_obj (class): instance of dataformatter class\n",
    "          key (str): 'all', 'train', or 'test', corresponding to sample type\n",
    "          \n",
    "        Returns:\n",
    "          featurenames, samples: formatted arrays\n",
    "        \"\"\"\n",
    "        # inherit attributes from dataformatter object\n",
    "        featurenames, samples = data_obj.format_samples(key)\n",
    "        if any(x is not None for x in [self.features, self.samples]):\n",
    "            print(\"replacing original features/samples with dataformatter data\")\n",
    "        self.cols = featurenames\n",
    "        self.features = pd.DataFrame(columns=self.cols)\n",
    "        self.samples = pd.DataFrame(samples,columns=self.features)\n",
    "        self.all_samples = self.samples\n",
    "        self.samples_as_strings = self.samples[self.cols].fillna('').values.astype(str)[:]\n",
    "        self.s_null=['']*len(self.samples_as_strings[0])\n",
    "        return featurenames, samples\n",
    "\n",
    "    def load_data(self,\n",
    "                  year,\n",
    "                  features_by_year,\n",
    "                  samples,\n",
    "                  Qnet):\n",
    "        '''load cols, features, samples, and qnet.\n",
    "\n",
    "        Args:\n",
    "          year (str): to identify cols/features.\n",
    "          features_by_year (str): file containing all features by year of the dataset.\n",
    "          samples (str): file of samples for that year.\n",
    "          Qnet (str): Qnet file location.\n",
    "        '''\n",
    "        # set attributes from given files and data\n",
    "        self.qnet = load_qnet(qnet)\n",
    "        self.year = year\n",
    "        self.cols = np.array((pd.read_csv(features_by_year,\n",
    "                            keep_default_na=True, \n",
    "                            index_col=0).set_index(\n",
    "                                'year')).loc[int(year)].apply(\n",
    "                                    eval).values[0])\n",
    "        self.features = pd.DataFrame(columns=self.cols)\n",
    "        self.mutable_vars = [x for x in self.cols]\n",
    "        #[self.cols].fillna('').values.astype(str)[:]\n",
    "\n",
    "        # read in samples and initialize related attributes\n",
    "        self.samples=pd.read_csv(samples)\n",
    "        self.samples = pd.concat([self.samples,self.features], axis=0)\n",
    "        self.all_samples = self.samples\n",
    "        self.samples_as_strings = self.samples[self.cols].fillna('').values.astype(str)[:]\n",
    "        self.s_null=['']*len(self.samples_as_strings[0])\n",
    "        self.D_null=self.qnet.predict_distributions(self.s_null)\n",
    "        variation_weight = []\n",
    "        for d in self.D_null:\n",
    "            v=[]\n",
    "            for val in d.values():\n",
    "                v=np.append(v,val)\n",
    "            variation_weight.append(entropy(v,base=len(v)))\n",
    "        self.variation_weight = variation_weight\n",
    "\n",
    "    def set_immutable_vars(self,\n",
    "                        IMMUTABLE_FILE):\n",
    "        '''set vars to immutable and mutable, \n",
    "        can prob combine this with the load_data func: only set the immutable vars if necessary\n",
    "\n",
    "        Args:\n",
    "          IMMUTABLE_FILE (str): file containing the immutable features/vars\n",
    "        '''\n",
    "        # set mutable and immutable variable attributes \n",
    "        if self.cols is None:\n",
    "            raise ValueError(\"load_data first!\")\n",
    "        self.immutable_vars = pd.read_csv(IMMUTABLE_FILE,index_col=0).transpose()\n",
    "        self.mutable_vars = None\n",
    "        self.mutable_vars = [x for x in self.cols\n",
    "                            if x.upper() not in self.immutable_vars.columns]\n",
    "    \n",
    "    def set_nsamples(self,\n",
    "                    num_samples,\n",
    "                    random=False):\n",
    "        '''select a subset of the samples\n",
    "\n",
    "        Args:\n",
    "          num_samples (int): Set num of samples to subset, default to None, resets to all samples\n",
    "          random (bool): take random sample if true, ordered sample if false\n",
    "        '''\n",
    "        # each time function is called, reset samples to use_all_samples\n",
    "        # this allows us to call nsamples numerous times \n",
    "        self.samples = self.all_samples\n",
    "        if self.samples is not None:\n",
    "            # if a greater number of sample is selected than available, raise error\n",
    "            if all(x is not None for x in [num_samples, self.samples]):\n",
    "                if num_samples > len(self.samples.index):\n",
    "                    string = 'The number of selected samples ({}) ' + \\\n",
    "                        'is greater than the number of samples ({})!'\n",
    "                    string = string.format(num_samples, len(self.samples.index))\n",
    "                    raise ValueError(string)\n",
    "\n",
    "                # if the same number of samples is selected as available, print warning\n",
    "                if num_samples == len(self.samples.index):\n",
    "                    string = 'The number of selected samples ({}) ' + \\\n",
    "                        'is equal to the number of samples ({})!'\n",
    "                    string = string.format(num_samples, len(self.samples.index))\n",
    "                    print(string)\n",
    "                    \n",
    "                # if random is true, return random sample, otherwise return an ordered slice\n",
    "                if random:\n",
    "                    self.samples = self.samples.sample(num_samples)\n",
    "                else:\n",
    "                    self.samples = self.samples.iloc[:num_samples]\n",
    "                self.nsamples = num_samples\n",
    "                self.samples_as_strings = self.samples[self.cols].fillna('').values.astype(str)[:]\n",
    "                \n",
    "            elif self.samples is None:\n",
    "                raise ValueError(\"load_data first!\")\n",
    "\n",
    "    def __variation_weight(self,\n",
    "                        index):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        d_=self.D_null[index]\n",
    "        v=[]\n",
    "        for val in d_.values():\n",
    "            v=np.append(v,val)\n",
    "        return entropy(v,base=len(v))\n",
    "    \n",
    "    def getBaseFrequency(self, \n",
    "                        sample):\n",
    "        '''get frequency of the variables\n",
    "        helper func for qsampling\n",
    "\n",
    "        Args:\n",
    "          sample (list[str]): vector of sample, must have the same num of features as the qnet\n",
    "        '''\n",
    "        # if variable is not mutable, set its base frequency to zero \n",
    "        MUTABLE=pd.DataFrame(np.zeros(len(self.cols)),index=self.cols).transpose()\n",
    "             \n",
    "        for m in self.mutable_vars:\n",
    "            MUTABLE[m]=1.0\n",
    "        mutable_x=MUTABLE.values[0]\n",
    "        base_frequency=mutable_x/mutable_x.sum()\n",
    "        \n",
    "        # otherwise, set base frequency weighted by variation weight\n",
    "        for i in range(len(base_frequency)):\n",
    "            if base_frequency[i]>0.0:\n",
    "                base_frequency[i]= self.variation_weight[i]*base_frequency[i]\n",
    "\n",
    "        return base_frequency/base_frequency.sum()\n",
    "    \n",
    "    def qsampling(self,\n",
    "                sample,\n",
    "                steps,\n",
    "                immutable=False):\n",
    "        '''perturb the sample based on thet qnet distributions and number of steps\n",
    "\n",
    "        Args:\n",
    "          sample (1d array-like): sample vector, must have the same num of features as the qnet\n",
    "          steps (int): number of steps to qsample\n",
    "          immutable (bool): are there variables that are immutable?\n",
    "        '''\n",
    "        # immutable, check that mutable variables have been initialized\n",
    "        if immutable == True:\n",
    "            if all(x is not None for x in [self.mutable_vars, sample]):\n",
    "                return qsample(sample,self.qnet,steps,self.getBaseFrequency(self.samples))\n",
    "            elif self.mutable_vars is None:\n",
    "                raise ValueError(\"set mutable and immutable variables first!\")\n",
    "        else:\n",
    "            return qsample(sample,self.qnet,steps)\n",
    "\n",
    "    def random_sample(self,\n",
    "                      df=None,\n",
    "                      n=1):\n",
    "        '''compute a random sample from the underlying distributions of the dataset, by column.\n",
    "        \n",
    "        \n",
    "        Args:\n",
    "          df (pandas.DataFrame): Desired data to take random sample of. Defaults to None, in which case qnet samples are used.\n",
    "          n (int): number of random samples to take. Defaults to 1.\n",
    "          \n",
    "        Returns:\n",
    "          return_df (pd.DataFrame): Random sample drawn from underlying distribution of each column.\n",
    "        '''\n",
    "        # check if a new dataset was inputted\n",
    "        if df is None:\n",
    "            samples_ = self.samples\n",
    "        else:\n",
    "            samples_ = df\n",
    "\n",
    "        # take random sample from each of the columns based on their distribution\n",
    "        return_df = pd.DataFrame()\n",
    "        for col in samples_.columns:\n",
    "            return_df[col] = samples_[col].sample(n=n, replace=True)\n",
    "            \n",
    "        return return_df\n",
    "    \n",
    "    def set_poles(self,\n",
    "                  POLEFILE,\n",
    "                  pole_1,\n",
    "                  pole_2,\n",
    "                  steps=0,\n",
    "                  mutable=False,\n",
    "                  VERBOSE=False,\n",
    "                  restrict=True,\n",
    "                  nsamples = None,\n",
    "                  random=False):\n",
    "        '''set the poles and samples such that the samples contain features in poles\n",
    "\n",
    "        Args:\n",
    "          steps (int): number of steps to qsample\n",
    "          POLEFILE (str): file containing poles samples and features\n",
    "          pole_1 (str): column name for first pole\n",
    "          pole_2 (str): column name for second pole\n",
    "          mutable (bool): Whether or not to set poles as the only mutable_vars\n",
    "          VERBOSE (bool): boolean flag prints number of pole features not found in sample features if True\n",
    "          restrict (bool): boolean flag restricts the sample features to polar features if True\n",
    "          random (bool): boolean flag takes random sample of all_samples\n",
    "        '''\n",
    "        invalid_count = 0\n",
    "        if all(x is not None for x in [self.samples, self.qnet]):\n",
    "            # read and set poles\n",
    "            poles = pd.read_csv(POLEFILE, index_col=0)\n",
    "            self.poles=poles.transpose()\n",
    "            self.polar_features = pd.concat([self.features, self.poles], axis=0).fillna('')\n",
    "            poles_dict = {}\n",
    "            for column in poles:\n",
    "                p_ = self.polar_features.loc[column][self.cols].fillna('').values.astype(str)[:]\n",
    "                # qsample poles to qnet\n",
    "                poles_dict[column] = self.qsampling(p_,steps)\n",
    "            self.poles_dict = poles_dict\n",
    "            self.pL = self.poles_dict[pole_1]\n",
    "            self.pR = self.poles_dict[pole_2]\n",
    "            self.d0 = qdistance(self.pL, self.pR, self.qnet, self.qnet)\n",
    "            \n",
    "            # restrict sample columns to polar columns\n",
    "            if restrict:\n",
    "                cols = [x for x in self.poles.columns if x in self.samples.columns]\n",
    "                self.samples=self.samples[cols]\n",
    "                self.restricted = True\n",
    "                self.samples = pd.concat([self.features,self.samples], axis=0).replace(\"nan\",\"\").fillna('')\n",
    "                self.samples_as_strings = self.samples[self.cols].fillna('').values.astype(str)[:]\n",
    "                \n",
    "            # if restrict==False, unrestrict it and set original\n",
    "            else:\n",
    "                self.restricted = False\n",
    "                self.samples = self.all_samples\n",
    "                if self.nsamples is not None:\n",
    "                    self.set_nsamples(nsamples, random)\n",
    "            \n",
    "            # identify pole features that were excluded due to sample features restriction\n",
    "            if VERBOSE:\n",
    "                for x in self.poles.columns:\n",
    "                    if x not in self.samples.columns:\n",
    "                        invalid_count += 1\n",
    "                        #self.samples[x]=''\n",
    "            \n",
    "            if mutable:\n",
    "                self.mutable_vars=[x for x in self.cols if x in self.poles.columns]\n",
    "        elif self.samples is None:\n",
    "            raise ValueError(\"load_data first!\")\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(\"{} pole features not found in sample features\".format(invalid_count))\n",
    "\n",
    "    def mp_compute(self, \n",
    "                   processes,\n",
    "                   func, \n",
    "                   cols,\n",
    "                   outfile, \n",
    "                   args=[]):\n",
    "        \"\"\"\n",
    "        Compute desired function through multiprocessing and save result to csv.\n",
    "\n",
    "        Args:\n",
    "          processes (int): number of processes to use.\n",
    "          func (func): function to compute using multiprocessing\n",
    "          cols (list): column names of resulting csv\n",
    "          outfile (str)): filepath + filename for resulting csv\n",
    "          args (list): list containing arguments for desired function. Defaults to empty list.\n",
    "        \"\"\"\n",
    "\n",
    "        # init mp.Manager and result dict\n",
    "        manager = mp.Manager()\n",
    "        return_dict = manager.dict()\n",
    "\n",
    "        # set processes as given, unless class parameter is set\n",
    "        max_processes = processes\n",
    "        if self.MAX_PROCESSES != 0:\n",
    "            max_processes = self.MAX_PROCESSES\n",
    "            print(\"Number of Processes {} has been set using class parameter\".format(self.MAX_PROCESSES))\n",
    "        num_processes = 0\n",
    "        process_list = []\n",
    "        \n",
    "        # init mp.Processes for each individual sample\n",
    "        # run once collected processes hit max\n",
    "        for i in range(len(self.samples)):\n",
    "            params = tuple([i, return_dict] + args)\n",
    "            num_processes += 1\n",
    "            p = mp.Process(target=func,\n",
    "                        args=params)\n",
    "            process_list.append(p)\n",
    "            if num_processes == max_processes:\n",
    "                [x.start() for x in process_list]\n",
    "                [x.join() for x in process_list]\n",
    "                process_list = []\n",
    "                num_processes = 0\n",
    "                \n",
    "        # compute remaining processes\n",
    "        if num_processes != 0:\n",
    "            [x.start() for x in process_list]\n",
    "            [x.join() for x in process_list]\n",
    "            process_list = []\n",
    "            num_processes = 0\n",
    "        \n",
    "        # format and save resulting dict\n",
    "        result = pd.DataFrame(return_dict.values(), columns=cols, index=return_dict.keys()).sort_index()\n",
    "        result.to_csv(outfile, index=None)\n",
    "        return result\n",
    "    \n",
    "    def distance(self,\n",
    "                sample1,\n",
    "                sample2,\n",
    "                nsteps1=0,\n",
    "                nsteps2=0):\n",
    "        \"\"\"qsamples each sample set num of steps, then takes qdistance\n",
    "\n",
    "        Args:\n",
    "          sample1 (list[str]): sample vector 1, must have the same num of features as the qnet\n",
    "          sample2 (list[str]): sample vector 2, must have the same num of features as the qnet\n",
    "          nsteps1 (int, optional): number of steps to qsample for sample1\n",
    "          nsteps2 (int, optional): number of steps to qsample for sample2\n",
    "\n",
    "        Returns:\n",
    "          qdistance: float, distance between two samples\n",
    "        \"\"\"\n",
    "        if self.qnet is None:\n",
    "            raise ValueError(\"load qnet first!\")\n",
    "        #bp1 = self.getBaseFrequency(sample1)\n",
    "        #bp2 = self.getBaseFrequency(sample2)\n",
    "        # qsample samples\n",
    "        sample1 = qsample(sample1, self.qnet, nsteps1)#, baseline_prob=bp1)\n",
    "        sample2 = qsample(sample2, self.qnet, nsteps2)#, baseline_prob=bp2)\n",
    "        return qdistance(sample1, sample2, self.qnet, self.qnet)\n",
    "    \n",
    "    def __distfunc(self, \n",
    "                   x, \n",
    "                   y):\n",
    "        '''Compute distance between two samples\n",
    "\n",
    "        Args:\n",
    "          x (list[str]): first sample\n",
    "          y (list[str]): second sample\n",
    "          \n",
    "        Returns:\n",
    "         d: qdistance\n",
    "        '''\n",
    "        d=qdistance(x,y,self.qnet,self.qnet)\n",
    "        return d\n",
    "    \n",
    "    def distfunc_line(self,\n",
    "                    i,\n",
    "                    return_dict=None):\n",
    "        '''compute the distance for a single sample from all other samples\n",
    "\n",
    "        Args:\n",
    "          i (int): row\n",
    "          return_dict (dict): dictionary containing multiprocessing results\n",
    "        \n",
    "        Return:\n",
    "          line: float, numpy.ndarray\n",
    "        '''\n",
    "        if all(x is not None for x in [self.samples, self.features]):\n",
    "            w = self.samples.index.size\n",
    "            line = np.zeros(w)\n",
    "            y = self.samples_as_strings[i]\n",
    "            for j in range(w):\n",
    "                # only compute half of the distance matrix\n",
    "                if j > i:\n",
    "                    x = self.samples_as_strings[j]\n",
    "                    line[j] = self.__distfunc(x, y)\n",
    "        else:\n",
    "            raise ValueError(\"load_data first!\")\n",
    "        if return_dict is not None:\n",
    "            return_dict[i] = line\n",
    "        return line\n",
    "    \n",
    "    def distfunc_multiples(self,\n",
    "                           outfile,\n",
    "                           processes=6):\n",
    "        \"\"\"compute distance matrix for all samples in the dataset\n",
    "\n",
    "        Args:\n",
    "          outfile (str): desired output filename and path\n",
    "          \n",
    "        Returns:\n",
    "          result: pandas.DataFrame containing distance matrix\n",
    "        \"\"\"\n",
    "        if all(x is not None for x in [self.samples, self.features]):\n",
    "            cols = [i for i in range(len(self.samples))]\n",
    "            result = self.mp_compute(processes,\n",
    "                                        self.distfunc_line,\n",
    "                                        cols,\n",
    "                                        outfile)\n",
    "            # format and save resulting dict, and tranpose symmetrical distance matrix\n",
    "            result = result.to_numpy()\n",
    "            result = pd.DataFrame(np.maximum(result, result.transpose()))\n",
    "            result.to_csv(outfile, index=None, header=None)\n",
    "        else:\n",
    "            raise ValueError(\"load data first!\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def polarDistance(self,\n",
    "                    i,\n",
    "                    return_dict=None):\n",
    "        \"\"\"return the distances from a single sample to the poles\n",
    "\n",
    "        Args:\n",
    "          i (int): index of sample to take\n",
    "          return_dict (dict): dictionary containing multiprocessing results\n",
    "\n",
    "        Returns:\n",
    "          distances: float, distance from sample to each pole\n",
    "        \"\"\"\n",
    "        p = self.samples_as_strings[i]\n",
    "        distances = []\n",
    "        # calculate from each pole to the sample, and append to array\n",
    "        for index, row in self.polar_features[self.cols].iterrows():\n",
    "            row = row.fillna('').values.astype(str)[:]\n",
    "            distances.append(self.distance(p, np.array(row)))\n",
    "        if return_dict is not None:\n",
    "            return_dict[i] = distances\n",
    "        return distances\n",
    "            \n",
    "    def polarDistance_multiple(self,\n",
    "                               outfile,\n",
    "                               processes=6):\n",
    "        \"\"\"return the distance from all samples to the poles\n",
    "\n",
    "        Args:\n",
    "          outfile (str): desired output filename and path\n",
    "          \n",
    "        Returns:\n",
    "          result: pandas.DataFrame containing polar distance results\n",
    "        \"\"\"\n",
    "        if all(x is not None for x in [self.samples, self.cols,\n",
    "                                    self.polar_features]):\n",
    "            # get the column names\n",
    "            pole_names = []\n",
    "            for index, row in self.polar_features[self.cols].iterrows():\n",
    "                pole_names.append(index)\n",
    "            result = self.mp_compute(processes,\n",
    "                                        self.polarDistance,\n",
    "                                        pole_names,\n",
    "                                        outfile)\n",
    "        else:\n",
    "            raise ValueError(\"load data first!\")\n",
    "        return result\n",
    "    \n",
    "    def polar_separation(self,\n",
    "                        nsteps=0):\n",
    "        \"\"\"calculates the distance between poles as a qdistance matrix\n",
    "\n",
    "        Args:\n",
    "          nsteps (int, optional): [description]. Defaults to 0.\n",
    "          \n",
    "        Returns:\n",
    "          self.polar_matrix: dictionary containing multiprocessing results\n",
    "        \"\"\"\n",
    "        # vectorize and qsample poles\n",
    "        polar_arraydata = self.polar_features[self.cols].values.astype(str)[:]\n",
    "        samples_ = []\n",
    "        for vector in polar_arraydata:\n",
    "            bp = self.getBaseFrequency(vector)\n",
    "            sample = qsample(vector, self.qnet, nsteps, baseline_prob=bp)\n",
    "            samples_.append(sample)\n",
    "        samples_ = np.array(samples_)\n",
    "        # calculate distance matrix for poles\n",
    "        self.polar_matrix = qdistance_matrix(samples_, samples_, self.qnet, self.qnet)\n",
    "        return self.polar_matrix\n",
    "        \n",
    "    def embed(self,\n",
    "            infile,\n",
    "            name_pref,\n",
    "            out_dir,\n",
    "            pca_model=False,\n",
    "            EMBED_BINARY=None):\n",
    "        '''\n",
    "        embed data\n",
    "\n",
    "        Args:\n",
    "          infile (str): input file to be embedded\n",
    "          name_pref (str): preferred name for output file\n",
    "          out_dir (str): output dir for results\n",
    "          pca_model (bool): whether or not to generate PCA model\n",
    "          EMBED_BINARY (os.path.abspath): path to embed binary\n",
    "        '''\n",
    "        if all(x is not None for x in [self.year]):\n",
    "            # init file names \n",
    "            yr = self.year\n",
    "            PREF = name_pref\n",
    "            FILE = infile\n",
    "            DATAFILE = out_dir + 'data_' +yr\n",
    "            EFILE = out_dir + PREF + '_E_' +yr\n",
    "            DFILE = out_dir + PREF + '_D_' +yr\n",
    "            \n",
    "            # set embed binary directory\n",
    "            if EMBED_BINARY is None:\n",
    "                EMBED = pkgutil.get_data(\"cognet.bin\", \"__embed__.so\") \n",
    "            else:\n",
    "                EMBED = EMBED_BINARY\n",
    "            \n",
    "            # embed data files\n",
    "            pd.read_csv(FILE, header=None).to_csv(DATAFILE,sep=' ',header=None,index=None)\n",
    "            STR=EMBED+' -f '+DATAFILE+' -E '+EFILE+' -D '+DFILE\n",
    "            subprocess.call(STR,shell=True)\n",
    "            if pca_model:\n",
    "                embed_to_pca(EFILE, EFILE+'_PCA')\n",
    "        elif self.year is None:\n",
    "            raise ValueError(\"load_data first!\")\n",
    "    \n",
    "    def __calc_d0(self,\n",
    "                pole_1,\n",
    "                pole_2):\n",
    "        \"\"\"calculate distance between two poles\n",
    "\n",
    "        Args:\n",
    "          pole_1 (list[str]): a polar vector, must have same number of features as qnet\n",
    "          pole_2 (list[str]): a polar vector, must have same number of features as qnet\n",
    "        \"\"\"\n",
    "        self.pL = self.poles_dict[pole_1]\n",
    "        self.pR = self.poles_dict[pole_2]\n",
    "        self.d0 = qdistance(self.pL, self.pR, self.qnet, self.qnet)\n",
    "        \n",
    "    def ideology(self,\n",
    "                i,\n",
    "                return_dict=None,\n",
    "                pole_1=None,\n",
    "                pole_2=None):\n",
    "        \"\"\"return ideology index (left-leaning or right-leaning) for a singular sample\n",
    "\n",
    "        Args:\n",
    "          i (int): index of sample\n",
    "          pole_1 (int): index of Pole One to calc as base distance. Defaults to 0.\n",
    "          pole_2 (int): index of Pole Two to calc as base distance. Defaults to 1.\n",
    "          return_dict (dict, optional): dict containing results\n",
    "          \n",
    "        Returns:\n",
    "          [ideology_index, dR, dL, self.d0]: which way the sample leans,\n",
    "                                             distance from the right pole,\n",
    "                                             distance from the left pole,\n",
    "                                             and distance between poles, respectively\n",
    "        \"\"\"\n",
    "        # calculate base distance between two poles\n",
    "        if pole_1 is not None or pole_2 is not None:\n",
    "            self.__calc_d0(pole_1, pole_2)\n",
    "        \n",
    "        # calculate distances between sample and the two poles\n",
    "        p = self.samples_as_strings[i]\n",
    "        dR = qdistance(self.pR, p, self.qnet, self.qnet)\n",
    "        dL = qdistance(self.pL, p, self.qnet, self.qnet)\n",
    "        \n",
    "        ideology_index = (dR-dL)/self.d0\n",
    "        if return_dict is not None:\n",
    "            return_dict[i] = [ideology_index, dR, dL, self.d0]\n",
    "        return [ideology_index, dR, dL, self.d0]\n",
    "\n",
    "    def dispersion(self,\n",
    "                   i,\n",
    "                   return_dict=None):\n",
    "        \"\"\"qsamples a sample n times and takes distance matrix \n",
    "        to determine max and std of distances between qsamples\n",
    "\n",
    "        Args:\n",
    "          i (int): index of sample\n",
    "          return_dict (dict): dictionary containing multiprocessing results\n",
    "\n",
    "        Returns:\n",
    "          list[float]: std and max of the distances btwn qsamples\n",
    "        \"\"\"\n",
    "        # qsample sample num_qsample times\n",
    "        p = self.samples_as_strings[i]\n",
    "        Qset = [qsample(p, self.qnet, self.steps) for j in np.arange(self.num_qsamples)]\n",
    "        Qset = np.array(Qset)\n",
    "\n",
    "        # calculate qdistance matrix for qsampled samples\n",
    "        matrix = (qdistance_matrix(Qset, Qset, self.qnet, self.qnet))\n",
    "        Q = matrix.max()\n",
    "        Qsd = matrix.std()\n",
    "        \n",
    "        if return_dict is not None:\n",
    "            return_dict[i] = [Qsd, Q]\n",
    "        return [Qsd, Q]\n",
    "    \n",
    "    def compute_DLI_samples(self,\n",
    "                        type,\n",
    "                        outfile,\n",
    "                        num_qsamples=40,\n",
    "                        steps=120,\n",
    "                        n_jobs=28,\n",
    "                        pole_1=0,\n",
    "                        pole_2=1,\n",
    "                        processes=6):\n",
    "        \"\"\"compute and save ideology index or dispersion for all samples\n",
    "\n",
    "        Args:\n",
    "          num_qsamples (int): number of qsamples to compute\n",
    "          outfile (str): output file for results\n",
    "          type (str): whether to calc dispersion or ideology\n",
    "          steps (int): number of steps to qsample\n",
    "          n_jobs (int, optional): sets the number of jobs for parallelization. Defaults to 28.\n",
    "          pole_1 (int, optional): index of Pole One to calc as base distance. Defaults to 0.\n",
    "          pole_2 (int, optional): index of Pole Two to calc as base distance. Defaults to 1.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: set poles if poles are not set\n",
    "          ValueError: load data if samples or features are not present\n",
    "            \n",
    "        Returns:\n",
    "          result: pandas.DataFrame containing multiprocessing results\n",
    "        \"\"\"\n",
    "        if all(x is not None for x in [self.samples, self.features,\n",
    "                                    self.pL, self.pR]):\n",
    "            # init vars\n",
    "            self.num_qsamples = num_qsamples\n",
    "            self.steps = steps\n",
    "            if pole_1 != 0 or pole_2 != 1:\n",
    "                self.__calc_d0(pole_1, pole_2)\n",
    "            \n",
    "            if type == 'ideology':\n",
    "                func_ = self.ideology\n",
    "                cols=['ideology', 'dR', 'dL', 'd0']\n",
    "            elif type == 'dispersion':\n",
    "                func_ = self.dispersion\n",
    "                cols=['Qsd', 'Qmax']\n",
    "            else:\n",
    "                raise ValueError(\"Type must be either dispersion or ideology!\")\n",
    "            \n",
    "            result = self.mp_compute(processes,\n",
    "                                     func_,\n",
    "                                     cols,\n",
    "                                     outfile)\n",
    "        elif self.pL is None or self.pR is None:\n",
    "            raise ValueError(\"set_poles first!\")\n",
    "        else:\n",
    "            raise ValueError(\"load_data first!\")\n",
    "        return result\n",
    "\n",
    "    def compute_polar_indices(self,\n",
    "                              num_samples=None,\n",
    "                              polar_comp=False,\n",
    "                              POLEFILE=None,\n",
    "                              steps=5):\n",
    "        '''set up polar indices for dissonance func\n",
    "\n",
    "        Args:\n",
    "          num_samples (int): subset of samples to take\n",
    "          polar_comp (bool): whether or not to set poles\n",
    "          POLEFILE (None): file containing pole samples and features\n",
    "          steps (int): number of steps to qsample\n",
    "        '''\n",
    "        if all(x is not None for x in [self.samples, self.features, self.poles]):\n",
    "            if num_samples is not None:\n",
    "                self.set_nsamples(num_samples)\n",
    "\n",
    "            if polar_comp:\n",
    "                self.set_poles(self.qnet, steps, POLEFILE)\n",
    "            \n",
    "            # calculate polar indices\n",
    "            polar_features = pd.concat([self.features, self.poles], axis=0)\n",
    "            self.polar_indices=np.where(polar_features[self.cols].fillna('XXXX').values[0]!='XXXX')[0]\n",
    "        \n",
    "        elif self.poles is None:\n",
    "            raise ValueError(\"set_poles first!\")\n",
    "        else:\n",
    "            raise ValueError(\"load_data first!\")\n",
    "\n",
    "    def dissonance(self,\n",
    "                sample_index,\n",
    "                return_dict=None,\n",
    "                MISSING_VAL=0.0):\n",
    "        '''compute dissonance for a single sample, helper function for all_dissonance\n",
    "        \n",
    "        Args:\n",
    "          sample_index (int): index of the sample to compute dissonance\n",
    "          return_dict (dict): dictionary containing multiprocessing results\n",
    "          MISSING_VAL (float): default dissonance value\n",
    "          \n",
    "        Returns: \n",
    "          diss[self.polar_indices]: ndarray containing dissonance for sample\n",
    "        '''\n",
    "        if all(x is not None for x in [self.samples, self.features]):\n",
    "            s = self.samples_as_strings[sample_index]\n",
    "            if self.polar_indices is None:\n",
    "                self.polar_indices = range(len(s))\n",
    "\n",
    "            # init vars and calculate dissonance for sample\n",
    "            Ds=self.qnet.predict_distributions(s)\n",
    "            diss=np.ones(len(Ds))*MISSING_VAL\n",
    "            for i in self.polar_indices:\n",
    "                if s[i] != '':\n",
    "                    if s[i] in Ds[i].keys():\n",
    "                        diss[i]=1-Ds[i][s[i]]/np.max(\n",
    "                            list(Ds[i].values())) \n",
    "                    else:\n",
    "                        diss[i]=1.0\n",
    "            if return_dict is not None:\n",
    "                return_dict[sample_index] = diss[self.polar_indices]\n",
    "            return diss[self.polar_indices]\n",
    "        else:\n",
    "            raise ValueError(\"load_data first!\")\n",
    "    \n",
    "    def dissonance_matrix(self,\n",
    "                        outfile='/example_results/DISSONANCE_matrix.csv',\n",
    "                        processes=6):\n",
    "        '''get the dissonance for all samples\n",
    "\n",
    "        Args:\n",
    "          output_file (str): directory and/or file for output\n",
    "          processes (int): max number of processes. Defaults to 6.\n",
    "\n",
    "        Returns:\n",
    "          result: pandas.DataFrame containing dissonances for each sample\n",
    "        '''\n",
    "        # set columns\n",
    "        if self.polar_indices is not None:\n",
    "            polar_features = pd.concat([self.features, self.poles], axis=0)\n",
    "            cols = polar_features[self.cols].dropna(axis=1).columns\n",
    "        else:\n",
    "            cols = self.cols\n",
    "        \n",
    "        result = self.mp_compute(processes,\n",
    "                                    self.dissonance,\n",
    "                                    cols,\n",
    "                                    outfile)\n",
    "        return result\n",
    "    \n",
    "    def __choose_one(self,\n",
    "                X):\n",
    "        '''returns a random element of X\n",
    "\n",
    "        Args:\n",
    "          X (1D array-like): vector from which random element is to be chosen\n",
    "        \n",
    "        Returns:\n",
    "          X: random element of sample\n",
    "          None: if X has len 0\n",
    "        '''\n",
    "        X=list(X)\n",
    "        if len(X)>0:\n",
    "            return X[np.random.randint(len(X))]\n",
    "        return None\n",
    "\n",
    "    def getMaskedSample(self,\n",
    "                        s,\n",
    "                        mask_prob=0.5,\n",
    "                        allow_all_mutable=False):\n",
    "        '''inputs a sample and randomly mask elements of the sample\n",
    "\n",
    "        Args:\n",
    "          s (list[str]): vector of sample, must have the same num of features as the qnet.\n",
    "          mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5\n",
    "          allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.\n",
    "          \n",
    "        Returns:\n",
    "          s1,\n",
    "          base_frequency,\n",
    "          MASKrand,\n",
    "          np.where(base_frequency)[0],\n",
    "          np.mean(rnd_match_prob),\n",
    "          np.mean(max_match_prob),\n",
    "          random_sample\n",
    "        '''\n",
    "        if self.samples is not None:\n",
    "            # init random mutable variable masking\n",
    "            s0=s.copy()\n",
    "            s0=np.array(s0)   \n",
    "            # double check, because code seems to imply that masking happens in order,\n",
    "            # i.e. limited to the first 100 features, if there are only 100 mutable features\n",
    "            MUTABLE=pd.DataFrame(np.zeros(len(self.cols)),index=self.cols).transpose()\n",
    "            WITHVAL=[x for x in self.cols[np.where(s0)[0]] if x in self.mutable_vars ]\n",
    "            MASKrand=[x for x in WITHVAL if random.random() < mask_prob ]\n",
    "            for m in MASKrand:\n",
    "                MUTABLE[m]=1.0\n",
    "            \n",
    "            mutable_x=MUTABLE.values[0]\n",
    "            base_frequency=mutable_x/mutable_x.sum()\n",
    "\n",
    "            # if np.isnan(base_frequency).any():\n",
    "            #     return np.nan,np.nan,np.nan\n",
    "            #     return self.getMaskedSample(s)\n",
    "\n",
    "            # mask sample according to masking (base_frequency)\n",
    "            s1=s.copy()\n",
    "            for i in range(len(base_frequency)):\n",
    "                if base_frequency[i]>0.0001:\n",
    "                    s1[i]=''\n",
    "                \n",
    "            # create a random sample to test reconstruction effectiveness\n",
    "            random_sample=np.copy(s)\n",
    "            rnd_match_prob=[]        \n",
    "            max_match_prob=[]        \n",
    "            D=self.qnet.predict_distributions(s)\n",
    "            for i in MASKrand:\n",
    "                random_sample[np.where(\n",
    "                    self.cols==i)[0][0]]=self.__choose_one(\n",
    "                        self.D_null[np.where(self.cols==i)[0][0]].keys())\n",
    "                rnd_match_prob=np.append(rnd_match_prob,1/len(\n",
    "                    self.D_null[np.where(self.cols==i)[0][0]].keys()))\n",
    "                max_match_prob=np.append(\n",
    "                    max_match_prob,np.max(\n",
    "                        list(D[np.where(\n",
    "                            self.cols==i)[0][0]].values())))\n",
    "            \n",
    "            # calculate base_frequency if all variables are mutable\n",
    "            if allow_all_mutable:\n",
    "                WITHVAL=[x for x in self.cols[np.where(s0)[0]]]\n",
    "                MASKrand=[x for x in WITHVAL if random.random() < mask_prob ]\n",
    "                for m in MASKrand:\n",
    "                    MUTABLE[m]=1.0\n",
    "                mutable_x=MUTABLE.values[0]\n",
    "                base_frequency=mutable_x/mutable_x.sum()\n",
    "                s1=s.copy()\n",
    "                for i in range(len(base_frequency)):\n",
    "                    if base_frequency[i]>0.0001:\n",
    "                        s1[i]=''\n",
    "\n",
    "            return s1,base_frequency,MASKrand,np.where(\n",
    "                base_frequency)[0],np.mean(rnd_match_prob),np.mean(max_match_prob),random_sample\n",
    "        else:\n",
    "            raise ValueError(\"load_data first!\")\n",
    "\n",
    "    def randomMaskReconstruction(self,\n",
    "                                index,\n",
    "                                return_dict=None,\n",
    "                                sample=None,\n",
    "                                index_colname=\"feature_names\",\n",
    "                                output_dir=\"recon_results/\",\n",
    "                                file_name=\"recon_tmp.csv\",\n",
    "                                mask_prob=0.5,\n",
    "                                allow_all_mutable=False,\n",
    "                                save_samples=False):\n",
    "        \"\"\"reconstruct the masked sample by qsampling and comparing to original\n",
    "        set self.mask_prob and self.steps if needed\n",
    "\n",
    "        Args:\n",
    "          index (int): index of sample to take.\n",
    "          return_dict (dict): dictionary containing multiprocessing results. Defaults to None.\n",
    "          sample (list[str], optional): sample vector, must have the same num of features as the qnet. Defaults to None.\n",
    "          index_colname (str): column name for index. Defaults to \"feature_names\"\n",
    "          output_dir (str): directory name for output files. Defaults to \"recon_results/\".\n",
    "          file_name (str): base file name for output files Defaults to \"recon_tmp.csv\".\n",
    "          mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5\n",
    "          allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: Neither sample or index were given\n",
    "          ValueError: Both sample and index were given\n",
    "          \n",
    "        Returns:\n",
    "          return_dict[index]:(1 - (dqestim/dactual))*100,\n",
    "                            rmatch_u,\n",
    "                            rmatch,\n",
    "                            s,\n",
    "                            qs,\n",
    "                            random_sample,\n",
    "                            mask_\n",
    "        \"\"\"\n",
    "        if all(x is None for x in [sample, index]):\n",
    "            raise ValueError(\"Must input either sample or index!\")\n",
    "        elif all(x is not None for x in [sample, index]):\n",
    "            raise ValueError(\"Must input either sample or index not both!\")\n",
    "        elif sample is not None:\n",
    "            s=np.array(pd.DataFrame(sample).fillna('').values.astype(str)[:])\n",
    "        elif index is not None:\n",
    "            s=self.samples_as_strings[index]\n",
    "        \n",
    "        # calculate masked sample and get variables\n",
    "        s1,bp,mask_,maskindex,rmatch_u,rmatch,random_sample=self.getMaskedSample(s, \n",
    "                                                                        mask_prob=mask_prob,\n",
    "                                                                        allow_all_mutable=allow_all_mutable)\n",
    "        # if base_frequency is nan, set return_dict to nans\n",
    "        if np.isnan(bp).any():\n",
    "            return_dict[index] = np.nan,np.nan,np.nan\n",
    "            return np.nan,np.nan,np.nan\n",
    "        \n",
    "        # make directories\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "\n",
    "        # qsample sample and calculate distances between original and qsampled \n",
    "        qs=qsample(s1,self.qnet,self.steps,bp)\n",
    "        dqestim=qdistance(s,qs,self.qnet,self.qnet)\n",
    "        dactual=qdistance(s,s1,self.qnet,self.qnet)\n",
    "        \n",
    "        # format and save sample and qsample statistics and values\n",
    "        cmpf=pd.DataFrame([s,qs,random_sample],\n",
    "                          columns=self.cols,\n",
    "                          index=['sample','qsampled','random_sample'])[mask_].transpose()\n",
    "        cmpf.index.name= index_colname\n",
    "        file_name = file_name.replace(\"tmp\", str(index))\n",
    "        cmpf.to_csv(output_dir+file_name)\n",
    "        if return_dict is not None:\n",
    "            if save_samples:\n",
    "                return_dict[index] = (1 - (dqestim/dactual))*100,rmatch_u,rmatch,s,qs,random_sample,mask_\n",
    "            else:\n",
    "                return_dict[index] = (1 - (dqestim/dactual))*100,rmatch_u,rmatch,mask_\n",
    "        return return_dict[index]\n",
    "\n",
    "    def randomMaskReconstruction_multiple(self,\n",
    "                                          outfile,\n",
    "                                          processes=6,\n",
    "                                          save_samples=False,\n",
    "                                          index_colname=\"feature_names\",\n",
    "                                          output_dir=\"recon_results/\",\n",
    "                                          file_name=\"recon_tmp.csv\",\n",
    "                                          mask_prob=0.5,\n",
    "                                          allow_all_mutable=False):\n",
    "        '''runs and saves the results of the predicted masked sample\n",
    "\n",
    "        Args:\n",
    "          output_file (str): directory and/or file for output.\n",
    "          processes (int): max number of processes. Defaults to 6.\n",
    "          save_samples (boolean): whether or not to save the generated qsamples, random samples, etc. Defaults to False.\n",
    "          index_colname=\"feature_names\",\n",
    "          output_dir=\"recon_results/\",\n",
    "          file_name=\"recon_tmp.csv\",\n",
    "          mask_prob (float): float btwn 0 and 1, prob to mask element of sample. Defaults to 0.5\n",
    "          allow_all_mutable (bool): whether or not all variables are mutable. Defaults to False.\n",
    "          \n",
    "        Returns:\n",
    "          result: pandas.DataFrame containing masking and reconstruction results.\n",
    "        '''\n",
    "        # set columns for mp_compute\n",
    "        if save_samples:\n",
    "            cols = ['rederr','r_prob','rand_err','sample','qsampled','random_sample','mask_']\n",
    "        else:\n",
    "            cols = ['rederr','r_prob','rand_err','mask_']\n",
    "        \n",
    "        # \n",
    "        args=[None, index_colname, output_dir,\n",
    "              file_name, mask_prob, allow_all_mutable]\n",
    "        \n",
    "        result = self.mp_compute(processes,\n",
    "                                    self.randomMaskReconstruction,\n",
    "                                    cols,\n",
    "                                    outfile,\n",
    "                                    args=args)\n",
    "        return result\n",
    "    \n",
    "    def dmat_filewriter(self,\n",
    "                        QNETPATH,\n",
    "                        mpi_path=\"mpi_tmp/\",\n",
    "                        pyfile=\"cognet_qdistmatrix.py\",\n",
    "                        MPI_SETUP_FILE=\"mpi_setup.sh\",\n",
    "                        MPI_RUN_FILE=\"mpi_run.sh\",\n",
    "                        MPI_LAUNCHER_FILE=\"../launcher.sh\",\n",
    "                        YEARS='2016',\n",
    "                        NODES=4,\n",
    "                        T=12,\n",
    "                        num_samples=None,\n",
    "                        OUTFILE='tmp_distmatrix.csv',\n",
    "                        tmp_samplesfile=\"tmp_samples_as_strings.csv\"):\n",
    "        \"\"\"generate files to compute qdistance matrix using mpi parallelization\n",
    "\n",
    "        Args:\n",
    "          QNETPATH (str): Qnet filepath\n",
    "          pyfile (str, optional): Name of generated python file. Defaults to \"cognet_qdistmatrix.py\".\n",
    "          MPI_SETUP_FILE (str, optional): Name of mpi setup script. Defaults to \"mpi_setup.sh\".\n",
    "          MPI_RUN_FILE (str, optional): Name of mpi run script. Defaults to \"mpi_run.sh\".\n",
    "          MPI_LAUNCHER_FILE (str, optional): Launcher script filepath. Defaults to \"launcher.sh\".\n",
    "          YEARS (str, optional): If looping by year, not currently implemented. Defaults to '2016'.\n",
    "          NODES (int, optional): Number of nodes to use. Defaults to 4.\n",
    "          T (int, optional): Number of hours to reserve nodes for. Defaults to 12.\n",
    "          num_samples ([type], optional): How many samples to take. Defaults to None.\n",
    "          OUTFILE (str, optional): CSV File to write computed qdist matrix. Defaults to 'tmp_distmatrix.csv'.\n",
    "          tmp_samplesfile (str, optional): CSV File to write samples as strings. Defaults to \"tmp_samples_as_strings.csv\".\n",
    "\n",
    "        Raises:\n",
    "            ValueError: load data if qnet, features, or samples are not present]\n",
    "        \"\"\"\n",
    "        if all(x is not None for x in [self.samples,self.features,\n",
    "                                       self.qnet, self.cols]):\n",
    "            if num_samples is not None:\n",
    "                self.set_nsamples(num_samples)\n",
    "            \n",
    "            # init and make tmp dir \n",
    "            tmp_path = mpi_path\n",
    "            if not os.path.exists(tmp_path):\n",
    "                os.makedirs(tmp_path)\n",
    "            \n",
    "            pd.DataFrame(self.samples_as_strings).to_csv(tmp_path+tmp_samplesfile, header=None, index=None)\n",
    "            \n",
    "            w = self.samples.index.size\n",
    "            \n",
    "            # writing python file\n",
    "            with open(tmp_path+pyfile, 'w+') as f:\n",
    "                f.writelines([\"from mpi4py.futures import MPIPoolExecutor\\n\",\n",
    "                              \"import numpy as np\\n\",\n",
    "                              \"import pandas as pd\\n\",\n",
    "                              \"from quasinet.qnet import Qnet, qdistance, load_qnet, qdistance_matrix\\n\",\n",
    "                              \"from quasinet.qsampling import qsample, targeted_qsample\\n\\n\",\n",
    "                              \"qnet=load_qnet(\\'{}\\')\\n\".format(QNETPATH)])\n",
    "\n",
    "                f.writelines([\"w = {}\\n\".format(w),\n",
    "                              \"h = w\\n\",\n",
    "                              \"p_all = pd.read_csv(\\\"{}\\\", header=None).values.astype(str)[:]\\n\\n\".format(tmp_samplesfile)])\n",
    "\n",
    "                f.writelines([\"def distfunc(x,y):\\n\",\n",
    "                              \"\\td=qdistance(x,y,qnet,qnet)\\n\",\n",
    "                              \"\\treturn d\\n\\n\"])\n",
    "\n",
    "                f.writelines([\"def dfunc_line(k):\\n\",\n",
    "                              \"\\tline = np.zeros(w)\\n\",\n",
    "                              \"\\ty = p_all[k]\\n\",\n",
    "                              \"\\tfor j in range(w):\\n\",\n",
    "                              \"\\t\\tif j > k:\\n\",\n",
    "                              \"\\t\\t\\tx = p_all[j]\\n\",\n",
    "                              \"\\t\\t\\tline[j] = distfunc(x, y)\\n\",\n",
    "                              \"\\treturn line\\n\\n\"])\n",
    "\n",
    "                f.writelines([\"if __name__ == '__main__':\\n\",\n",
    "                              \"\\twith MPIPoolExecutor() as executor:\\n\",\n",
    "                              \"\\t\\tresult = executor.map(dfunc_line, range(h))\\n\",\n",
    "                              \"\\tresult = pd.DataFrame(result)\\n\",\n",
    "\t                          \"\\tresult = result.to_numpy()\\n\",\n",
    "                              \"\\tresult = pd.DataFrame(np.maximum(result, result.transpose()))\\n\"\n",
    "                              \"\\tresult.to_csv(\\'{}\\',index=None,header=None)\".format(OUTFILE)])\n",
    "            \n",
    "            # writing MPI setup file\n",
    "            with open(tmp_path+MPI_SETUP_FILE, 'w+') as ms:\n",
    "                ms.writelines([\"#!/bin/bash\\n\",\n",
    "                               \"YEAR=$1\\n\\n\",\n",
    "                               \"if [ $# -gt 1 ] ; then\\n\",\n",
    "                               \"\\tNODES=$2\\n\",\n",
    "                               \"else\\n\",\n",
    "                               \"\\tNODES=3\\n\",\n",
    "                               \"fi\\n\",\n",
    "                               \"if [ $# -gt 2 ] ; then\\n\",\n",
    "                               \"\\tNUM=$3\\n\",\n",
    "                               \"else\\n\",\n",
    "                               \"\\tNUM='all'\\n\",\n",
    "                               \"fi\\n\",\n",
    "                               \"if [ $# -gt 3 ] ; then\\n\",\n",
    "                               \"\\tPROG=$4\\n\",\n",
    "                               \"else\\n\",\n",
    "                               \"\\tPROG=$(tty)\\n\",\n",
    "                               \"fi\\n\\n\",\n",
    "                               \"NUMPROC=`expr 28 \\* $NODES`\\n\",\n",
    "                               \"echo \\\"module load midway2\\\" >> $PROG\\n\",\n",
    "                               \"echo \\\"module unload python\\\" >> $PROG\\n\",\n",
    "                               \"echo \\\"module unload openmpi\\\" >> $PROG\\n\",\n",
    "                               \"echo \\\"module load python/anaconda-2020.02\\\" >> $PROG\\n\",\n",
    "                               \"echo \\\"module load mpi4py\\\" >> $PROG\\n\",\n",
    "                               \"echo \\\"date; mpiexec -n \\\"$NUMPROC\\\" python3 -m mpi4py.futures {}; date\\\"  >> $PROG\\n\".format(pyfile),\n",
    "                                ])\n",
    "\n",
    "            # writing MPI run file\n",
    "            with open(tmp_path+MPI_RUN_FILE, 'w+') as mr:\n",
    "                mr.writelines([\"#!/bin/bash\\n\",\n",
    "                               \"YEARS=\\'{}\\'\\n\".format(YEARS),\n",
    "                               \"# nodes requested\\n\",\n",
    "                               \"NODES={}\\n\".format(NODES),\n",
    "                               \"# time requested\\n\",\n",
    "                               \"T={}\\n\".format(T),\n",
    "                               \"NUM=\\'all\\'\\n\",\n",
    "                               \"LAUNCH=\\'{}\\'\\n\\n\".format(MPI_LAUNCHER_FILE),\n",
    "                               \"for yr in `echo $YEARS`\\n\",\n",
    "                               \"do\\n\",\n",
    "                               \"\\techo $yr\\n\",\n",
    "                               \"\\t./{} $yr $NODES $NUM tmp_\\\"$yr\\\"\\n\".format(MPI_SETUP_FILE),\n",
    "                               \"\\t$LAUNCH -P tmp_\\\"$yr\\\" -F -T $T -N \\\"$NODES\\\" -C 28 -p broadwl -J MPI_TMP_\\\"$yr\\\" -M 56\\n\",\n",
    "                               \"done\\n\",\n",
    "                               \"rm tmp_\\\"$yr\\\"*\\n\"])\n",
    "            os.system(\"cp {} {}\".format(MPI_LAUNCHER_FILE,tmp_path+'mpi_launcher.sh'))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"load data first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09ab5308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptsd1</th>\n",
       "      <th>ptsd2</th>\n",
       "      <th>ptsd3</th>\n",
       "      <th>ptsd4</th>\n",
       "      <th>ptsd5</th>\n",
       "      <th>ptsd6</th>\n",
       "      <th>ptsd7</th>\n",
       "      <th>ptsd8</th>\n",
       "      <th>ptsd9</th>\n",
       "      <th>ptsd10</th>\n",
       "      <th>...</th>\n",
       "      <th>ptsd202</th>\n",
       "      <th>ptsd203</th>\n",
       "      <th>ptsd204</th>\n",
       "      <th>ptsd205</th>\n",
       "      <th>ptsd206</th>\n",
       "      <th>ptsd207</th>\n",
       "      <th>ptsd208</th>\n",
       "      <th>ptsd209</th>\n",
       "      <th>ptsd210</th>\n",
       "      <th>ptsd211</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows  211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ptsd1 ptsd2 ptsd3 ptsd4 ptsd5 ptsd6 ptsd7 ptsd8 ptsd9 ptsd10  ... ptsd202  \\\n",
       "0       4     2     5     4     2     3     2     3     3      2  ...       2   \n",
       "1       3     2     3     3     1     3     2     2     2      1  ...       3   \n",
       "2       2     2     2     4     1     3     3     2     2      4  ...       4   \n",
       "3       2     2     5     1     3     2     1     2     2      1  ...       1   \n",
       "4       4     1     1     1     2     2     1     1     1      2  ...       2   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...     ...   \n",
       "299     4     2     3     4     1     1     1     1     3      2  ...       2   \n",
       "300     5     2     2     4     4     3     2     5     4      4  ...       5   \n",
       "301     5     1     2     1     1     3     5     5     3      4  ...       4   \n",
       "302     2     1     3     3     1     2     2     2     1      1  ...       1   \n",
       "303     5     2     1     1     1     1     1     1     1      1  ...       2   \n",
       "\n",
       "    ptsd203 ptsd204 ptsd205 ptsd206 ptsd207 ptsd208 ptsd209 ptsd210 ptsd211  \n",
       "0         4       4       3       2       3       2       2       4       2  \n",
       "1         4       4       3       3       3       4       3       4       5  \n",
       "2         2       2       4       4       2       3       2       2       5  \n",
       "3         2       3       3       3       2       1       1       2       2  \n",
       "4         2       2       2       1       1       2       2       2       3  \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "299       2       2       2       2       2       2       2       2       3  \n",
       "300       5       5       5       4       5       5       4       4       5  \n",
       "301       3       3       2       3       1       3       2       5       5  \n",
       "302       1       1       1       1       2       2       2       2       2  \n",
       "303       2       1       2       1       1       2       2       1       4  \n",
       "\n",
       "[304 rows x 211 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Qnet, write mpi files for distance matrix\n",
    "# run qdistance matrix with \"./mpi_run.sh\" command\n",
    "cognet_obj = cognet()\n",
    "cognet_obj.load_from_model(model_obj, data_obj, 'all')\n",
    "cognet_obj.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "411ebbf1-8a47-468b-b307-96b1efb66eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cognet_obj.dmat_filewriter(\"results/PTSD_cognet_test.joblib\",\n",
    "                           #mpi_path=\"mpi_tmp\",\n",
    "                           pyfile=\"PTSD_cognet.py\",\n",
    "                           NODES=4,T=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d0e9a49-343f-4def-97fb-6fefcd749a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction results       rederr    r_prob  rand_err  \\\n",
      "0  38.607344  0.216667  0.379358   \n",
      "1  47.639666  0.215714  0.404629   \n",
      "2  34.483355  0.220098  0.381088   \n",
      "3  39.161146  0.222549  0.455628   \n",
      "4  41.798428  0.219027  0.452333   \n",
      "5  27.094722  0.218421  0.397192   \n",
      "6  16.701176  0.217672  0.466267   \n",
      "7  26.137256  0.216832  0.453159   \n",
      "8  43.133913  0.215789  0.387582   \n",
      "9  31.968260  0.217347  0.379329   \n",
      "\n",
      "                                               mask_  \n",
      "0  [ptsd3, ptsd6, ptsd7, ptsd10, ptsd12, ptsd13, ...  \n",
      "1  [ptsd2, ptsd3, ptsd7, ptsd8, ptsd10, ptsd13, p...  \n",
      "2  [ptsd3, ptsd4, ptsd7, ptsd10, ptsd11, ptsd12, ...  \n",
      "3  [ptsd2, ptsd6, ptsd7, ptsd8, ptsd10, ptsd14, p...  \n",
      "4  [ptsd1, ptsd2, ptsd3, ptsd5, ptsd10, ptsd11, p...  \n",
      "5  [ptsd1, ptsd2, ptsd3, ptsd7, ptsd14, ptsd17, p...  \n",
      "6  [ptsd3, ptsd4, ptsd9, ptsd10, ptsd14, ptsd16, ...  \n",
      "7  [ptsd6, ptsd9, ptsd10, ptsd11, ptsd12, ptsd14,...  \n",
      "8  [ptsd1, ptsd3, ptsd4, ptsd7, ptsd13, ptsd15, p...  \n",
      "9  [ptsd5, ptsd7, ptsd9, ptsd10, ptsd12, ptsd17, ...  \n"
     ]
    }
   ],
   "source": [
    "# random mask and reconstruction\n",
    "cognet_obj.set_nsamples(10)\n",
    "recon_df = cognet_obj.randomMaskReconstruction_multiple('results/PTSD_randomMaskRecon_test.csv')\n",
    "print(\"reconstruction results\", recon_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ac6c71-ee22-4fad-9ff0-ce72e5ece76c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09268731566380246"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cognet_dist = qdistance(np.array(cognet_obj.samples.iloc[3]), np.array(cognet_obj.samples.iloc[4]), model_obj.myQnet, model_obj.myQnet)\n",
    "qdistance(samples[3],samples[4], model_obj.myQnet, model_obj.myQnet)\n",
    "#samples.shape\n",
    "#len(model_obj.myQnet.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1340a8d-f530-410e-b4c9-291738da612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2' '2' '5' '1' '3' '2' '1' '2' '2' '1' '2' '3' '3' '3' '3' '1' '3' '2'\n",
      " '2' '1' '1' '1' '1' '3' '3' '2' '3' '4' '3' '3' '1' '2' '4' '3' '3' '3'\n",
      " '3' '1' '3' '2' '3' '4' '2' '3' '3' '1' '1' '3' '3' '3' '1' '3' '4' '1'\n",
      " '1' '2' '2' '1' '1' '4' '3' '1' '1' '1' '2' '1' '1' '2' '2' '1' '1' '2'\n",
      " '2' '2' '2' '2' '2' '1' '2' '2' '1' '3' '1' '3' '3' '2' '2' '1' '2' '2'\n",
      " '1' '1' '3' '3' '2' '1' '1' '4' '1' '2' '1' '1' '4' '2' '2' '1' '1' '4'\n",
      " '3' '2' '1' '1' '3' '4' '2' '2' '1' '1' '1' '2' '2' '1' '3' '3' '2' '2'\n",
      " '1' '1' '1' '2' '1' '2' '2' '1' '1' '1' '1' '2' '1' '1' '1' '1' '1' '2'\n",
      " '1' '1' '2' '3' '2' '1' '1' '1' '2' '2' '2' '1' '1' '2' '1' '1' '1' '1'\n",
      " '1' '1' '2' '1' '1' '1' '1' '1' '2' '1' '1' '1' '2' '1' '1' '1' '1' '1'\n",
      " '1' '1' '1' '1' '2' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1'\n",
      " '2' '1' '1' '1' '2' '3' '3' '3' '2' '1' '1' '2' '2']\n",
      "[2 2 5 1 3 2 1 2 2 1 2 3 3 3 3 1 3 2 2 1 1 1 1 3 3 2 3 4 3 3 1 2 4 3 3 3 3\n",
      " 1 3 2 3 4 2 3 3 1 1 3 3 3 1 3 4 1 1 2 2 1 1 4 3 1 1 1 2 1 1 2 2 1 1 2 2 2\n",
      " 2 2 2 1 2 2 1 3 1 3 3 2 2 1 2 2 1 1 3 3 2 1 1 4 1 2 1 1 4 2 2 1 1 4 3 2 1\n",
      " 1 3 4 2 2 1 1 1 2 2 1 3 3 2 2 1 1 1 2 1 2 2 1 1 1 1 2 1 1 1 1 1 2 1 1 2 3\n",
      " 2 1 1 1 2 2 2 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 2\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 3 3 3 2 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(samples[3])\n",
    "print(np.array(data_obj.samples.iloc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734edb52-d4e8-4cc3-b171-8714648ea28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj.samples.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a34d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed distance matrix, make sure to generate distance matrix first\n",
    "cognet_obj.embed(\"mpi_tmp/distmatrix.csv\", \"PTSD\", \"mpi_tmp/\",EMBED_BINARY='cognet/cognet/bin/__embed__.so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e04b2205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptsd1</th>\n",
       "      <th>ptsd2</th>\n",
       "      <th>ptsd3</th>\n",
       "      <th>ptsd4</th>\n",
       "      <th>ptsd5</th>\n",
       "      <th>ptsd6</th>\n",
       "      <th>ptsd7</th>\n",
       "      <th>ptsd8</th>\n",
       "      <th>ptsd9</th>\n",
       "      <th>ptsd10</th>\n",
       "      <th>...</th>\n",
       "      <th>ptsd202</th>\n",
       "      <th>ptsd203</th>\n",
       "      <th>ptsd204</th>\n",
       "      <th>ptsd205</th>\n",
       "      <th>ptsd206</th>\n",
       "      <th>ptsd207</th>\n",
       "      <th>ptsd208</th>\n",
       "      <th>ptsd209</th>\n",
       "      <th>ptsd210</th>\n",
       "      <th>ptsd211</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.312509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266935</td>\n",
       "      <td>0.264993</td>\n",
       "      <td>0.778503</td>\n",
       "      <td>0.051811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272496</td>\n",
       "      <td>0.049030</td>\n",
       "      <td>0.147404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.369530</td>\n",
       "      <td>0.259742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274517</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.429788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058872</td>\n",
       "      <td>0.025926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435161</td>\n",
       "      <td>0.246107</td>\n",
       "      <td>0.263992</td>\n",
       "      <td>0.362627</td>\n",
       "      <td>0.425756</td>\n",
       "      <td>0.455718</td>\n",
       "      <td>0.194502</td>\n",
       "      <td>0.251415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.306131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065765</td>\n",
       "      <td>0.254303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031635</td>\n",
       "      <td>0.354011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244961</td>\n",
       "      <td>0.355941</td>\n",
       "      <td>0.249957</td>\n",
       "      <td>0.300921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.192401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514859</td>\n",
       "      <td>0.474972</td>\n",
       "      <td>0.893145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278177</td>\n",
       "      <td>0.099964</td>\n",
       "      <td>0.331973</td>\n",
       "      <td>0.285513</td>\n",
       "      <td>0.274473</td>\n",
       "      <td>0.519960</td>\n",
       "      <td>0.091940</td>\n",
       "      <td>0.232926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.415880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.831320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347092</td>\n",
       "      <td>0.375952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.171105</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882757</td>\n",
       "      <td>0.643175</td>\n",
       "      <td>0.124499</td>\n",
       "      <td>0.376191</td>\n",
       "      <td>0.360915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194873</td>\n",
       "      <td>0.336675</td>\n",
       "      <td>0.599461</td>\n",
       "      <td>0.512339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.199602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238838</td>\n",
       "      <td>0.427842</td>\n",
       "      <td>0.840383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704204</td>\n",
       "      <td>0.637969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.395141</td>\n",
       "      <td>0.124113</td>\n",
       "      <td>0.153013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.873649</td>\n",
       "      <td>0.361911</td>\n",
       "      <td>0.565607</td>\n",
       "      <td>0.202331</td>\n",
       "      <td>0.481121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445971</td>\n",
       "      <td>0.271715</td>\n",
       "      <td>0.494140</td>\n",
       "      <td>0.238315</td>\n",
       "      <td>0.247135</td>\n",
       "      <td>0.236357</td>\n",
       "      <td>0.284215</td>\n",
       "      <td>0.440002</td>\n",
       "      <td>0.135582</td>\n",
       "      <td>0.072131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.568948</td>\n",
       "      <td>0.064654</td>\n",
       "      <td>0.299622</td>\n",
       "      <td>0.249607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040533</td>\n",
       "      <td>0.562342</td>\n",
       "      <td>0.043875</td>\n",
       "      <td>0.329156</td>\n",
       "      <td>0.117683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458955</td>\n",
       "      <td>0.233770</td>\n",
       "      <td>0.429294</td>\n",
       "      <td>0.643744</td>\n",
       "      <td>0.392149</td>\n",
       "      <td>0.659080</td>\n",
       "      <td>0.390238</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.521111</td>\n",
       "      <td>0.470570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.786939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.768844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183991</td>\n",
       "      <td>0.110213</td>\n",
       "      <td>0.607079</td>\n",
       "      <td>0.319974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ptsd1     ptsd2     ptsd3     ptsd4     ptsd5     ptsd6     ptsd7  \\\n",
       "0   0.312509  0.000000  0.266935  0.264993  0.778503  0.051811  0.000000   \n",
       "1   0.429788  0.000000  0.166296  0.000000  0.000000  0.086513  0.000000   \n",
       "2   0.306131  0.000000  0.000000  0.252755  0.000000  0.065765  0.254303   \n",
       "3   0.192401  0.000000  0.514859  0.474972  0.893145  0.000000  0.519839   \n",
       "4   0.415880  0.000000  0.254754  0.000000  0.831320  0.000000  0.347092   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.003012  0.171105  0.025767  0.000000  0.882757  0.643175  0.124499   \n",
       "96  0.199602  0.000000  0.238838  0.427842  0.840383  0.000000  0.362284   \n",
       "97  0.395141  0.124113  0.153013  0.000000  0.873649  0.361911  0.565607   \n",
       "98  0.568948  0.064654  0.299622  0.249607  0.000000  0.040533  0.562342   \n",
       "99  0.521111  0.470570  0.000000  0.000000  0.000000  0.786939  0.000000   \n",
       "\n",
       "       ptsd8     ptsd9    ptsd10  ...   ptsd202   ptsd203   ptsd204   ptsd205  \\\n",
       "0   0.272496  0.049030  0.147404  ...  0.000000  0.000000  0.369530  0.259742   \n",
       "1   0.000000  0.058872  0.025926  ...  0.556886  0.000000  0.435161  0.246107   \n",
       "2   0.000000  0.031635  0.354011  ...  0.702195  0.000000  0.000000  0.244961   \n",
       "3   0.000000  0.000000  0.000000  ...  0.278177  0.099964  0.331973  0.285513   \n",
       "4   0.375952  0.000000  0.576713  ...  0.000000  0.092742  0.000000  0.000000   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.376191  0.360915  0.000000  ...  0.194873  0.336675  0.599461  0.512339   \n",
       "96  0.000000  0.000000  0.342790  ...  0.000000  0.000000  0.704204  0.637969   \n",
       "97  0.202331  0.481121  0.000000  ...  0.445971  0.271715  0.494140  0.238315   \n",
       "98  0.043875  0.329156  0.117683  ...  0.452940  0.000000  0.458955  0.233770   \n",
       "99  0.000000  0.000000  0.768844  ...  0.183991  0.110213  0.607079  0.319974   \n",
       "\n",
       "     ptsd206   ptsd207   ptsd208   ptsd209   ptsd210   ptsd211  \n",
       "0   0.000000  0.309774  0.000000  0.000000  0.274517  0.000000  \n",
       "1   0.263992  0.362627  0.425756  0.455718  0.194502  0.251415  \n",
       "2   0.355941  0.249957  0.300921  0.000000  0.000000  0.234866  \n",
       "3   0.274473  0.519960  0.091940  0.232926  0.000000  0.130753  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.037378  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "96  0.000000  0.000000  0.000000  0.454954  0.000000  0.020877  \n",
       "97  0.247135  0.236357  0.284215  0.440002  0.135582  0.072131  \n",
       "98  0.429294  0.643744  0.392149  0.659080  0.390238  0.000000  \n",
       "99  0.000000  0.000000  0.030852  0.000000  0.000000  0.037257  \n",
       "\n",
       "[100 rows x 211 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute dissonance matrix\n",
    "cognet_obj.set_nsamples(304)\n",
    "return_dict = cognet_obj.dissonance_matrix(outfile=\"mpi_tmp/PTSD_dissonance_matrix.csv\", processes=2)\n",
    "return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70956bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5321586a20>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "D=pd.DataFrame(return_dict.copy())\n",
    "D.mean(axis=1).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = pd.read_csv(\"PTSD_cognet_test.csv\")\n",
    "# samples = samples.drop(['record_id', 'PTSDDx'], axis=1)\n",
    "# samples.to_csv(\"PTSD_cognet_test_processed.csv\", index=False)\n",
    "# samples = pd.read_csv(\"PTSD_cognet_test_processed.csv\")\n",
    "# samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466602a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
